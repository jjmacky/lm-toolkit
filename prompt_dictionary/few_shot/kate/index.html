
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../../zero_shot/whiteboard_of_thought/">
      
      
        <link rel="next" href="../../additional_resources/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.27">
    
    
      
        <title>KATE (kNN-Augmented in-conText Example selection) - The Language Model Toolkit</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.6543a935.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../css/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#kate-knn-augmented-in-context-example-selection" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="The Language Model Toolkit" class="md-header__button md-logo" aria-label="The Language Model Toolkit" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            The Language Model Toolkit
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              KATE (kNN-Augmented in-conText Example selection)
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/jjmacky/lm-toolkit" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    jjmacky/lm-toolkit
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../" class="md-tabs__link">
          
  
  Prompt Dictionary

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../synthetic_data/basic_tutorial_1/airbnb_rewriting/" class="md-tabs__link">
          
  
  Synthetic data

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../podcast_episodes/ai_n_stuff_episodes/" class="md-tabs__link">
        
  
    
  
  Podcast episodes

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../support_me/" class="md-tabs__link">
        
  
    
  
  Support my work

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="The Language Model Toolkit" class="md-nav__button md-logo" aria-label="The Language Model Toolkit" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    The Language Model Toolkit
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/jjmacky/lm-toolkit" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    jjmacky/lm-toolkit
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" checked>
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Prompt Dictionary
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Prompt Dictionary
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Welcome to the Prompt Dictionary!
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_2" >
        
          
          <label class="md-nav__link" for="__nav_1_2" id="__nav_1_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Zero-shot prompting strategies
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_2">
            <span class="md-nav__icon md-icon"></span>
            Zero-shot prompting strategies
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zero_shot/overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview of zero-shot prompting
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zero_shot/assumption_consideration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Assumption Consideration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zero_shot/emotion_prompting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Emotion Prompting
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zero_shot/plain_vanilla/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Plain Vanilla
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zero_shot/rephrase_and_respond/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Rephrase and Respond (RaR)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zero_shot/rereading/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Rereading (Re2)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zero_shot/role_prompting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Role Prompting
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zero_shot/self_ask/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Self-ask
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zero_shot/simtom/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SimToM
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zero_shot/solo_performance_prompting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Solo Performance Prompting (SPP)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zero_shot/style_prompting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Style Prompting
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zero_shot/system_2_attention/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    System 2 Attention (S2A)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../zero_shot/whiteboard_of_thought/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Whiteboard-of-Thought (WoT)
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_3" checked>
        
          
          <label class="md-nav__link" for="__nav_1_3" id="__nav_1_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Few-shot prompting strategies
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_1_3">
            <span class="md-nav__icon md-icon"></span>
            Few-shot prompting strategies
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    KATE (kNN-Augmented in-conText Example selection)
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    KATE (kNN-Augmented in-conText Example selection)
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#kate-knn-augmented-in-context-example-selection" class="md-nav__link">
    <span class="md-ellipsis">
      KATE (kNN-Augmented in-conText Example selection)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="KATE (kNN-Augmented in-conText Example selection)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-to-use-it" class="md-nav__link">
    <span class="md-ellipsis">
      How to use it
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#when-to-use-it" class="md-nav__link">
    <span class="md-ellipsis">
      When to use it
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-to-know" class="md-nav__link">
    <span class="md-ellipsis">
      What to know
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Best practices
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-to-watch-out-for" class="md-nav__link">
    <span class="md-ellipsis">
      What to watch out for
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#citation" class="md-nav__link">
    <span class="md-ellipsis">
      Citation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#application-example-pii-masking-with-llama3-8b" class="md-nav__link">
    <span class="md-ellipsis">
      Application example - PII masking with Llama3 (8B)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Application example - PII masking with Llama3 (8B)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#methodology" class="md-nav__link">
    <span class="md-ellipsis">
      Methodology
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#results" class="md-nav__link">
    <span class="md-ellipsis">
      Results
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#code" class="md-nav__link">
    <span class="md-ellipsis">
      Code
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Code">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#load-dataset-from-hugging-face" class="md-nav__link">
    <span class="md-ellipsis">
      Load dataset from Hugging Face
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load-custom-functions" class="md-nav__link">
    <span class="md-ellipsis">
      Load custom functions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-and-embed-data" class="md-nav__link">
    <span class="md-ellipsis">
      Create and embed data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#perform-simulated-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Perform simulated inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#examine-alternative-inference-approaches" class="md-nav__link">
    <span class="md-ellipsis">
      Examine alternative inference approaches
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../additional_resources/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Additional prompting resrouces
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Synthetic data
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Synthetic data
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../synthetic_data/basic_tutorial_1/airbnb_rewriting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Basic synthetic data creation tutorial
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../podcast_episodes/ai_n_stuff_episodes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Podcast episodes
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../support_me/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Support my work
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#kate-knn-augmented-in-context-example-selection" class="md-nav__link">
    <span class="md-ellipsis">
      KATE (kNN-Augmented in-conText Example selection)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="KATE (kNN-Augmented in-conText Example selection)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-to-use-it" class="md-nav__link">
    <span class="md-ellipsis">
      How to use it
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#when-to-use-it" class="md-nav__link">
    <span class="md-ellipsis">
      When to use it
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-to-know" class="md-nav__link">
    <span class="md-ellipsis">
      What to know
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Best practices
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-to-watch-out-for" class="md-nav__link">
    <span class="md-ellipsis">
      What to watch out for
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#citation" class="md-nav__link">
    <span class="md-ellipsis">
      Citation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#application-example-pii-masking-with-llama3-8b" class="md-nav__link">
    <span class="md-ellipsis">
      Application example - PII masking with Llama3 (8B)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Application example - PII masking with Llama3 (8B)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#methodology" class="md-nav__link">
    <span class="md-ellipsis">
      Methodology
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#results" class="md-nav__link">
    <span class="md-ellipsis">
      Results
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#code" class="md-nav__link">
    <span class="md-ellipsis">
      Code
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Code">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#load-dataset-from-hugging-face" class="md-nav__link">
    <span class="md-ellipsis">
      Load dataset from Hugging Face
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load-custom-functions" class="md-nav__link">
    <span class="md-ellipsis">
      Load custom functions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-and-embed-data" class="md-nav__link">
    <span class="md-ellipsis">
      Create and embed data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#perform-simulated-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Perform simulated inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#examine-alternative-inference-approaches" class="md-nav__link">
    <span class="md-ellipsis">
      Examine alternative inference approaches
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>KATE (kNN-Augmented in-conText Example selection)</h1>

<!-- ---
tags:
  - zero-shot
  - knn
  - exemplars
  - encoder
  - single-turn
--- -->

<h2 id="kate-knn-augmented-in-context-example-selection">KATE (kNN-Augmented in-conText Example selection)</h2>
<p>KATE is a technique <a href="#citation">introduced in early 2021</a> that aims to improve the few-shot learning capabilities of large language models, at the time demonstrated with GPT-3, by selecting semantically similar in-context examples at inference for a given user input.</p>
<p>KATE can be considered an early, specialized form of Retrieval-Augmented Generation (RAG), but it differs from more recent RAG techniques in its focus and application. While KATE aims to improve task performance through retrieval of similar examples, modern RAG techniques are more focused on increasing factual accuracy, reducing hallucinations, and integrating enterprise knowledge bases into standard language model workflows.</p>
<p>KATE is most effective when the retrieved examples significantly enhance the model's ability to generate appropriate responses. It's less suitable for tasks requiring factual accuracy (where newer RAG methods might be better) or those where simpler techniques (like standard kNN or basic prompting) are sufficient.</p>
<h3 id="how-to-use-it">How to use it</h3>
<p>Implementing KATE requires a more advanced application approach than simple prompting methods. It involves two main steps, preprocessing a dataset of examples, and comparing those examples to user input during inference. </p>
<ol>
<li>Preprocessing (done once):<ol>
<li>Choose a pre-trained sentence encoder (e.g., RoBERTa-large)</li>
<li>Encode all examples in a relevant dataset using this encoder (for example the SST-2 dataset if the application task is sentiment analysis)</li>
<li>Store these encodings for efficient retrieval</li>
</ol>
</li>
<li>Model inference<ol>
<li>For each user request:<ol>
<li>Encode the main question or instruction to be asked of the langauge model using the same sentence encoder</li>
<li>Calculate the distance between the encoded question/instruction and each entry in the embedded example set from the preprocessing step</li>
<li>Retrieve the k nearest neighbors from the embedded example set relative to the question/instruction</li>
<li>Use these retrieved examples as the in-context examples for the language model</li>
</ol>
</li>
<li>Prompt construction:<ol>
<li>Format the retrieved examples and the question/instruction into a prompt</li>
<li>The prompt format typically follows: [Retrieved examples] + [question/instruction]</li>
</ol>
</li>
<li>Model API call:<ol>
<li>Send the constructed prompt to the language model</li>
<li>The model generates its response based on this augmented context</li>
</ol>
</li>
<li>Post-processing:<ol>
<li>Process the model's output as needed for your specific application</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="when-to-use-it">When to use it</h3>
<div class="admonition tip">
<p class="admonition-title">When to use KATE</p>
<p><strong>Consider using KATE when the following conditions hold:</strong></p>
<ol>
<li>
<p>Model characteristics:</p>
<ul>
<li>You're working with a language model that benefits significantly from in-context examples.</li>
<li>In many applications the model is likely to be relatively small or medium-sized, as larger models might not see substantial improvements.</li>
</ul>
</li>
<li>
<p>Data requirements:</p>
<ul>
<li>You have access to a relevant, high-quality dataset of examples (typically thousands to tens of thousands) for embedding.</li>
<li>Your dataset is not primarily factual (for factual retrieval, newer RAG methods are often more appropriate).</li>
</ul>
</li>
<li>
<p>Task constraints:</p>
<ul>
<li>The task requires adherence to specific rules or patterns, and examples can effectively demonstrate these rules, serving as a flexible alternative to writing explicit, complex operational logic. See <a href="#application-example-pii-masking-with-llama3-8b">Application example</a> below for an implementation of this scenario.</li>
<li>Fine-tuning is not feasible or desirable (e.g., to avoid overfitting on small datasets).</li>
<li>You need more nuanced outputs than standard kNN classification can provide.</li>
</ul>
</li>
<li>
<p>Computational resources:</p>
<ul>
<li>You have sufficient GPU power to compute embeddings and distances without introducing significant latency during preprocessing or inference.</li>
<li>Your embedded dataset is small enough (typically &lt;100,000 examples) to allow for quick nearest neighbor searches.</li>
</ul>
</li>
<li>
<p>Application focus:</p>
<ul>
<li>You're working on tasks that benefit from dynamic, context-sensitive responses rather than static, pre-defined answers.</li>
</ul>
</li>
</ol>
<p><strong>Potential Applications:</strong></p>
<ol>
<li>
<p>Text PII masking:</p>
<ul>
<li>Example scenario: Masking PII data in unstructured text using a pre-defined set of tags.</li>
<li>Why KATE: Can use similar masked PII text to infer proper masking tags without building complex business logic into this system.</li>
<li>See <a href="#application-example-pii-masking-with-llama3-8b">Application example</a> below for an implementation of this scenario.</li>
</ul>
</li>
<li>
<p>Nuanced sentiment analysis:</p>
<ul>
<li>Example scenario: Analyzing customer feedback in a niche industry with unique terminology and sentiment expressions.</li>
<li>Why KATE: Can provide relevant industry-specific examples to guide the model's interpretation, potentially outperforming generic sentiment classifiers.</li>
</ul>
</li>
<li>
<p>Conceptual tutoring systems:</p>
<ul>
<li>Example scenario: Creating an AI tutor for advanced physics concepts.</li>
<li>Why KATE: Can retrieve and present relevant problem-solution pairs to help explain complex concepts, potentially offering more tailored explanations than a static knowledge base.</li>
</ul>
</li>
<li>
<p>Rapid prototyping in new domains:</p>
<ul>
<li>Example scenario: Quickly developing a proof-of-concept for analyzing literary styles across different authors.</li>
<li>Why KATE: Allows for fast implementation using a small, curated dataset of literary examples, potentially demonstrating the feasibility of AI-assisted literary analysis without extensive model training.</li>
</ul>
</li>
</ol>
</div>
<h3 id="what-to-know">What to know</h3>
<p>KATE was originally developed and tested on GPT-3, demonstrating its effectiveness in enhancing the model's few-shot learning capabilities across various natural language understanding and generation tasks. The method is a form of Retrieval-Augmented Generation (RAG) applied specifically to in-context example selection, which allowed GPT-3 to leverage relevant information from a large corpus of examples without fine-tuning.</p>
<p>The authors presented the image below contrasting KATE, which uses k-nearest neighbors search during inference to find relevant examples, with a less tailored approach that uses random examples.</p>
<p><img alt="An image of the KATE workflow" src="../../../images/few_shot/kate.png" /></p>
<p>The paper's experiments showed that KATE consistently outperformed random example selection across different tasks, including sentiment analysis, table-to-text generation, and open-domain question answering. Particularly notable were the improvements on complex tasks like table-to-text generation (41.9% improvement on the ToTTo dataset) and open-domain question answering (45.5% improvement on the Natural Questions dataset).</p>
<p>An important finding was that the choice of sentence encoder for retrieving similar examples significantly impacts KATE's performance. The researchers found that encoders fine-tuned on task-related datasets often yielded better results. This suggests that tailoring the retrieval mechanism to the specific task can further enhance the method's effectiveness.</p>
<p>While KATE showed promise in 2021, it's important to note that the field of natural language processing is rapidly evolving. More advanced RAG methods and few-shot learning techniques have been developed since KATE was introduced, potentially offering better performance or efficiency for certain applications. Additionally, as language models continue to grow in size and capability, the relative benefits of techniques like KATE may change. Therefore, it's crucial to benchmark KATE against current state-of-the-art methods when considering its use in practical applications.</p>
<h3 id="best-practices">Best practices</h3>
<div class="admonition tip">
<p class="admonition-title">Best practices for using KATE</p>
<ul>
<li>Use a pre-trained sentence encoder like RoBERTa for initial implementation.</li>
<li>If there is a sentence encoder that has been fine-tuned on a similar task or dataset, use that as it can improve performance. Experiment with fine-tuning the encoder on task-specific datasets.</li>
<li>Use the largest dataset you can for a fixed inference latency threshold (increasing the search space of relevant examples improves performance by helping to select more relevant in-context examples).</li>
<li>Using around 5 in-context examples is a good rule of thumb to balance performance and inference cost/latency (k=5).</li>
<li>When constructing the in-context examples for the prompt, the order of the examples likely doesn't matter. Try experimenting with both the default (closest examples first) and reversed (closest examples nearst the question/instruction) orders.</li>
</ul>
</div>
<h3 id="what-to-watch-out-for">What to watch out for</h3>
<div class="admonition warning">
<p class="admonition-title">What to watch out for with KATE</p>
<ul>
<li>As models and retrieval methods continue to advance KATE may have more limited use cases, try standard prompting methods before implementing KATE.</li>
<li>KATE requires significant additional infrastructure compared to standard prompting (including an embedded dataset and additional computational resources for inference).</li>
<li>Performance gains may vary across different models and tasks.</li>
<li>The choice of sentence encoder can significantly impact results, use an encoder fine-tuned on a similar task or dataset if possible.</li>
<li>More recent developments in RAG and few-shot learning techniques may offer better performance or efficiency, especially when factual information is the priority.</li>
</ul>
</div>
<h3 id="citation">Citation</h3>
<p>Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. 2021. What Makes Good In-Context Examples for GPT-3. <a href="https://arxiv.org/abs/2101.06804">arXiv preprint arXiv:2101.06804</a>. DOI: 10.48550/arXiv.2101.06804.</p>
<h3 id="application-example-pii-masking-with-llama3-8b">Application example - PII masking with Llama3 (8B)</h3>
<p>Let's implement KATE in a realistic setting. We'll consider the task of masking personally identifiable information (PII) without fine-tuning a model. At inference, given a sentence to mask, we'll use KATE to look up relevant examples to guide the model on which masks to use. </p>
<p>For this example, we're using the pii-masking-300k dataset, which can be found <a href="https://huggingface.co/datasets/ai4privacy/pii-masking-300k?row=0">here</a>. This synthetic dataset contains various text entries including personal statements, forms and ID cards, government records, and other types of text likely to contain PII. To follow the format of pii-masking-300k, we must use a predefined set of masking tags (i.e. we cannot simply delete the data or replace it with 'xxxxxxxx'), which makes the task a good candidate for KATE.</p>
<p>The Python notebook for the code below can be found <a href="https://github.com/jjmacky/lm-toolkit/blob/main/docs/prompt_dictionary/few_shot/code/kate/kate_demo.ipynb">here</a>.</p>
<h4 id="methodology">Methodology</h4>
<p>Our approach follows the KATE methodology:</p>
<ol>
<li>Embed the training dataset from pii-masking-300k (about 178,000 total pairs of masked and unmasked sentences).</li>
<li>To simulate inference, select a random sentence from the pii-masking-300k validation dataset.</li>
<li>Embed this sentence and calculate its k nearest neighbors in the embedded training dataset.</li>
<li>Build a prompt with the resulting examples and the random sentence to be masked.</li>
<li>Serve this prompt to the model (Llama3 8B in this case).</li>
</ol>
<h4 id="results">Results</h4>
<p>Before diving into the code, let's examine the results to motivate the implementation.
This was the random entry from the validation set used to simulate KATE inference:
<div class="highlight"><pre><span></span><code><span class="nt">Immunization Certification</span><span class="p">:</span>
<span class="w">  </span><span class="nt">individuals</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Princess</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">hlfiadjwhwi16966</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">+22-252 178-7818</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">United Kingdom</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">636</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Fleming Way</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Swindon</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ENG</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">SN1 2NN</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Townhouse 90</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">05:59</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Morag</span>
<span class="w">  </span><span class="nt">background</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">23/09/1972</span>
</code></pre></div></p>
<p>Here is the ground truth masked entry from the dataset:
<div class="highlight"><pre><span></span><code><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">Immunization_Certification</span><span class="p">:</span>
<span class="w">    </span><span class="nt">individuals</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">TITLE</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">USERNAME</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">TEL</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">COUNTRY</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">BUILDING</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">STREET</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">CITY</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">STATE</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">POSTCODE</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">SECADDRESS</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">TIME</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">LASTNAME1</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">background</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">DATE</span><span class="p p-Indicator">]</span>
</code></pre></div></p>
<p>After providing Llama3 (8B) with the top 5 nearest neighbors as examples along with the random entry above, this is how the model masked PII:
<div class="highlight"><pre><span></span><code><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">Immunization_Certification</span><span class="p">:</span>
<span class="w">  </span><span class="nt">individuals</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">TITLE</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">BOD</span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># Wrong, should be [USERNAME]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">TEL</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">COUNTRY</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">BUILDING</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">STREET</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">CITY</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">STATE</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">POSTCODE</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">SECADDRESS</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">TIME</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">LASTNAME1</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">background</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">DATE</span><span class="p p-Indicator">]</span>
</code></pre></div>
The model got a single tag wrong, which is not perfect, but still pretty good compared to the other methods we'll see below. The strong performance is due to the fact that the k neighbors identified during inference are similar to the sample, allowing the model to infer the masked tags for the unseen sample. Here is the unmasked and masked nearest neighbor for the inference sample.</p>
<p>Unmasked:
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8207886065</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">974312500</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">+132 289 676-9075</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">United States</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">332</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Rochelle Street</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">New York</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NY</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10464</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Flat 298</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">~`4teF</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Langmeier</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">COMMENTS_C</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Conduct</span><span class="nv"> </span><span class="s">IP</span><span class="nv"> </span><span class="s">audit,</span><span class="nv"> </span><span class="s">update</span><span class="nv"> </span><span class="s">trademark</span><span class="nv"> </span><span class="s">portfolio,</span><span class="nv"> </span><span class="s">support</span><span class="nv"> </span><span class="s">patent</span><span class="nv"> </span><span class="s">applications</span><span class="nv"> </span><span class="s">process.&quot;</span>
<span class="nt">background</span><span class="p">:</span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">6:30 AM</span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">31st October 2027</span>
</code></pre></div></p>
<p>Masked:
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">IDCARD</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">PASSPORT</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">TEL</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">COUNTRY</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">BUILDING</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">STREET</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">CITY</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">STATE</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">POSTCODE</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">SECADDRESS</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">PASS</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">LASTNAME1</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">COMMENTS_C</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Conduct</span><span class="nv"> </span><span class="s">IP</span><span class="nv"> </span><span class="s">audit,</span><span class="nv"> </span><span class="s">update</span><span class="nv"> </span><span class="s">trademark</span><span class="nv"> </span><span class="s">portfolio,</span><span class="nv"> </span><span class="s">support</span><span class="nv"> </span><span class="s">patent</span><span class="nv"> </span><span class="s">applications</span><span class="nv"> </span><span class="s">process.&quot;</span>
<span class="nt">background</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">[</span><span class="nv">TIME</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">[</span><span class="nv">DATE</span><span class="p p-Indicator">]</span>
</code></pre></div></p>
<p>Let's compare KATE with other methods. Below is the result if we just try asking Llama3 (8B) to mask the entry using the simple prompt:</p>
<blockquote>
<p>"Please mask the personally identifiable information in this text: {random entry}"</p>
</blockquote>
<p>This is also a good check for data contamination. If the model had memorized the PII dataset, it might achieve strong zero-shot masking performance without much direction or examples. However, it does not. Every field returned by the model is incorrect.</p>
<div class="highlight"><pre><span></span><code><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">Immunization_Certification</span><span class="p">:</span>
<span class="w">  </span><span class="nt">individuals</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Princess</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">XXXXXXXXXXXXXXXXXX</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">XXXXXXXX</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">United Kingdom</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">XXXX</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Fleming Way</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Swindon</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ENG</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">SN1 2NN</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Townhouse 90</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">05:59</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Morag</span>
<span class="w">  </span><span class="nt">background</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">XXXX/09/1972</span>
</code></pre></div>
<p>We can also test providing the model with the available set of masks and judge performance. Here the model does much better, but still gets many fields incorrect.</p>
<blockquote>
<p>"You have the following list of masks available: [BOD], [BUILDING], [CITY], [COUNTRY], [DATE], [DRIVERLICENSE], [EMAIL], [GEOCOORD], [GIVENNAME1], [GIVENNAME2], [IDCARD], [IP], [LASTNAME1], [LASTNAME2], [LASTNAME3], [PASS], [POSTCODE], [SECADDRESS], [SEX], [SOCIALNUMBER], [STATE], [STREET], [TEL], [TIME], [TITLE], [USERNAME].</p>
<p>Please mask the personally identifiable information in this sentence using the appropriate mask: {random entry}"</p>
</blockquote>
<div class="highlight"><pre><span></span><code><span class="nt">Immunization_Certification</span><span class="p">:</span>
<span class="w">  </span><span class="nt">individuals</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Princess</span><span class="w"> </span><span class="c1"># Wrong, should be [TITLE]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">SOCIALNUMBER</span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># Wrong, should be [USERNAME]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">+[TEL]</span><span class="w"> </span><span class="c1"># Wrong (shouldn&#39;t include &#39;+&#39;)</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">COUNTRY</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">DATE</span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># Wrong, [DATE] is not present</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">STREET</span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># Missing [BUILDING], [CITY], and [STATE]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">POSTCODE</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">SECADDRESS</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">TIME</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Morag</span><span class="w"> </span><span class="c1"># Wrong, should be [LASTNAME1]</span>
<span class="w">  </span><span class="nt">background</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">DATE</span><span class="p p-Indicator">]</span>
</code></pre></div>
<p>We can also follow the approach of Liu et al. in their paper by selecting random examples rather than the k nearest. Here, again this results in a response with many incorrectly masked fields.
<div class="highlight"><pre><span></span><code><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">Immunization_Certification</span><span class="p">:</span>
<span class="w">  </span><span class="nt">individuals</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">NAME1</span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># Wrong, should be [TITLE]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">USERNAME</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">PHONE_NUMBER</span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># Wrong, should be [TEL]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">COUNTRY</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">NUMBER</span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># Wrong, should be [BUILDING]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">STREET_ADDRESS</span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># Wrong, should be [STREET]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">CITY</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">STATE</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">POSTAL_CODE</span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># Wrong, should be [POSTCODE]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">ADDRESS_LINE2</span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># Wrong, should be [SECADDRESS]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">TIME</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">NAME2</span><span class="p p-Indicator">]</span><span class="w"> </span><span class="c1"># Wrong, should be [LASTNAME1]</span>
<span class="w">  </span><span class="nt">background</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">DATE</span><span class="p p-Indicator">]</span>
</code></pre></div></p>
<p>This comparison demonstrates that KATE's approach of selecting relevant examples based on similarity has the potential to outperform both zero-shot and random example selection methods for novel use cases not originally considered in the KATE paper. However, a more comprehensive evaluation across multiple examples would be necessary to draw broader conclusions about its overall effectiveness.</p>
<h4 id="code">Code</h4>
<h5 id="load-dataset-from-hugging-face">Load dataset from Hugging Face</h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;ai4privacy/pii-masking-300k&quot;</span><span class="p">)</span>
</code></pre></div>
<h5 id="load-custom-functions">Load custom functions</h5>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">RobertaTokenizer</span><span class="p">,</span> <span class="n">RobertaModel</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">ollama</span>

<span class="c1"># Initialize RoBERTa tokenizer and model</span>
<span class="c1"># We&#39;ll use RoBERTa to follow the approach of the paper.</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">RobertaTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;roberta-base&#39;</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RobertaModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;roberta-base&#39;</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Function to generate embeddings for a given text using RoBERTa</span>
<span class="k">def</span> <span class="nf">get_roberta_embedding</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Average pooling of last hidden state</span>

<span class="c1"># Embed all entries in the dataset, showing progress with tqdm</span>
<span class="k">def</span> <span class="nf">embed_data</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Embedding entries&quot;</span><span class="p">):</span>
        <span class="n">embedding</span> <span class="o">=</span> <span class="n">get_roberta_embedding</span><span class="p">(</span><span class="n">entry</span><span class="p">[</span><span class="s2">&quot;original&quot;</span><span class="p">])</span> <span class="c1"># Embed the original sentence to be masked for later comparison during inference</span>
        <span class="n">entry</span><span class="p">[</span><span class="s2">&quot;embedding&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="c1"># Calculate similarity between a new text and the dataset, returning top k similar indices</span>
<span class="k">def</span> <span class="nf">calculate_similarity</span><span class="p">(</span><span class="n">new_text_embedding</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">similarity_scores</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">([</span><span class="n">new_text_embedding</span><span class="p">],</span> <span class="p">[</span><span class="n">d</span><span class="p">[</span><span class="s2">&quot;embedding&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">top_k_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argpartition</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">,</span> <span class="o">-</span><span class="n">k</span><span class="p">)[</span><span class="o">-</span><span class="n">k</span><span class="p">:]</span> <span class="c1"># Only do a partial sort the entire dataset for efficiency</span>
    <span class="n">top_k_indices</span> <span class="o">=</span> <span class="n">top_k_indices</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">similarity_scores</span><span class="p">[</span><span class="n">top_k_indices</span><span class="p">])][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># Quickly sort the top indices</span>
    <span class="k">return</span> <span class="n">top_k_indices</span>

<span class="c1"># Generate example masked sentences from the most similar entries</span>
<span class="k">def</span> <span class="nf">get_example_masked_sentences</span><span class="p">(</span><span class="n">top_k_indices</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="n">example_masked_sentences</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Original sentence:</span><span class="si">{</span><span class="n">data</span><span class="p">[</span><span class="n">index</span><span class="p">][</span><span class="s1">&#39;original&#39;</span><span class="p">]</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">Masked sentence:</span><span class="si">{</span><span class="n">data</span><span class="p">[</span><span class="n">index</span><span class="p">][</span><span class="s1">&#39;masked&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">top_k_indices</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">example_masked_sentences</span>

<span class="c1"># Build a prompt for the language model using examples and the sentence to mask</span>
<span class="k">def</span> <span class="nf">build_model_prompt</span><span class="p">(</span><span class="n">example_masked_sentences</span><span class="p">,</span> <span class="n">sentence_to_mask</span><span class="p">):</span>
    <span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        Please mask the PII in the given sentence by following the examples:</span>

<span class="s2">        </span><span class="si">{examples}</span>

<span class="s2">        Here is the sentence to mask. Respond only with the masked sentence and no additional explanation or commentary:</span>
<span class="s2">        Sentence to mask: </span><span class="si">{sentence}</span>
<span class="s2">        Masked sentence:</span>
<span class="s2">        &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">examples</span><span class="o">=</span><span class="n">example_masked_sentences</span><span class="p">,</span>
        <span class="n">sentence</span><span class="o">=</span><span class="n">sentence_to_mask</span>
    <span class="p">)</span>

<span class="c1"># Call the language model to perform PII masking during inference</span>
<span class="k">def</span> <span class="nf">call_model</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">llm_model</span><span class="p">):</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>
    <span class="p">]</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">ollama</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">llm_model</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div>
<h5 id="create-and-embed-data">Create and embed data</h5>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">])),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Creating dataset:&quot;</span><span class="p">):</span>
    <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;original&quot;</span><span class="p">:</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">][</span><span class="n">index</span><span class="p">][</span><span class="s1">&#39;source_text&#39;</span><span class="p">],</span> <span class="s2">&quot;masked&quot;</span><span class="p">:</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">][</span><span class="n">index</span><span class="p">][</span><span class="s1">&#39;target_text&#39;</span><span class="p">]})</span>

<span class="n">embedded_data</span> <span class="o">=</span> <span class="n">embed_data</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div>
<h5 id="perform-simulated-inference">Perform simulated inference</h5>
<div class="highlight"><pre><span></span><code><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">14</span><span class="p">)</span> <span class="c1"># Set random seed</span>

<span class="c1"># Pick random sentence from validation set to use as a test of the approach</span>
<span class="n">rand_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">]),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">test_sentence</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">][</span><span class="n">rand_index</span><span class="p">][</span><span class="s1">&#39;source_text&#39;</span><span class="p">]</span>
<span class="n">target_sentence</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">][</span><span class="n">rand_index</span><span class="p">][</span><span class="s1">&#39;target_text&#39;</span><span class="p">]</span>

<span class="c1"># Get similar sentence for our test sentence in our embedded data</span>
<span class="n">embedded_test_sentence</span> <span class="o">=</span> <span class="n">get_roberta_embedding</span><span class="p">(</span><span class="n">test_sentence</span><span class="p">)</span>
<span class="n">top_k_indices</span> <span class="o">=</span> <span class="n">calculate_similarity</span><span class="p">(</span><span class="n">embedded_test_sentence</span><span class="p">,</span> <span class="n">embedded_data</span><span class="p">)</span>

<span class="c1"># Build the model prompt</span>
<span class="n">example_masked_sentences</span> <span class="o">=</span> <span class="n">get_example_masked_sentences</span><span class="p">(</span><span class="n">top_k_indices</span><span class="p">,</span> <span class="n">embedded_data</span><span class="p">)</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">build_model_prompt</span><span class="p">(</span><span class="n">example_masked_sentences</span><span class="p">,</span> <span class="n">test_sentence</span><span class="p">)</span>

<span class="c1"># Get the masked sentence</span>
<span class="n">masked_sentence</span> <span class="o">=</span> <span class="n">call_model</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="s2">&quot;llama3&quot;</span><span class="p">)</span>

<span class="c1"># Compare the masked sentence from our model with the ground truth from the data</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Masked sentence from model: </span><span class="si">{</span><span class="n">masked_sentence</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Masked ground-truth sentence from data: </span><span class="si">{</span><span class="n">target_sentence</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h5 id="examine-alternative-inference-approaches">Examine alternative inference approaches</h5>
<p>First, we can just try asking the model to mask the sentence. This is also a good check to see if there is data contamination. If the model memorized the PII dataset it might achieve strong zero-shot masking without much direction.
<div class="highlight"><pre><span></span><code><span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Please mask the personally identifiable information in this sentence: </span><span class="si">{</span><span class="n">test_sentence</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>
<span class="p">]</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">ollama</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3&quot;</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></p>
<p>Alternatively we can be more sophisticated and give the model the list of masks available. This approach still produces many incorrect masks.
<div class="highlight"><pre><span></span><code><span class="n">masks</span> <span class="o">=</span> <span class="s2">&quot;[BOD], [BUILDING], [CITY], [COUNTRY], [DATE], [DRIVERLICENSE], [EMAIL], [GEOCOORD], [GIVENNAME1], [GIVENNAME2], [IDCARD], [IP], [LASTNAME1], [LASTNAME2], [LASTNAME3], [PASS], [POSTCODE], [SECADDRESS], [SEX], [SOCIALNUMBER], [STATE], [STREET], [TEL], [TIME], [TITLE], [USERNAME]&quot;</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;You have the following list of masks available: </span><span class="si">{</span><span class="n">masks</span><span class="si">}</span><span class="s2">.</span><span class="se">\n\n</span><span class="s2"> Please mask the personally identifiable information in this sentence using the appropriate mask: </span><span class="si">{</span><span class="n">test_sentence</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>
<span class="p">]</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">ollama</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3&quot;</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></p>
<p>We can also test if KATE outperforms using random examples chosen from the dataset rather than the k nearest neighbors.
<div class="highlight"><pre><span></span><code><span class="n">k</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">random_top_k_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)),</span> <span class="n">k</span><span class="p">)</span>
<span class="n">example_masked_sentences</span> <span class="o">=</span> <span class="n">get_example_masked_sentences</span><span class="p">(</span><span class="n">random_top_k_indices</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">build_model_prompt</span><span class="p">(</span><span class="n">example_masked_sentences</span><span class="p">,</span> <span class="n">test_sentence</span><span class="p">)</span>
<span class="n">masked_sentence</span> <span class="o">=</span> <span class="n">call_model</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="s2">&quot;llama3&quot;</span><span class="p">)</span>

<span class="c1"># Compare the masked sentence from our model with the ground truth from the data</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Masked sentence from model: </span><span class="si">{</span><span class="n">masked_sentence</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Masked ground-truth sentence from data: </span><span class="si">{</span><span class="n">target_sentence</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024 <a href="https://github.com/jjmacky"  target="_blank" rel="noopener">James McCammon</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["content.code.copy", "navigation.tabs", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.subtitle"], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.ad660dcc.min.js"></script>
      
        <script src="../../../javascript/open_links_in_new_tab.js"></script>
      
    
  </body>
</html>