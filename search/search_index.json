{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to The Prompt Dictionary!","text":"<p>Welcome to The Prompt Dictionary! This site aims to be a comprehensive repository of strategies designed to enhance interactions with large language model. It currently includes prompt techniques, with plans to expand to encompass prompt optimizations, evaluations, benchmarks, and related topics such as synthetic data. I update this repository frequently to keep pace with rapid advancements, though there's often a backlog of new strategies to add. This archive captures each strategy at a specific point in time, acknowledging that some may become less relevant over time as language models evolve.</p> <p>The goal of The Prompt Dictionary is to serve as a historical record, documenting the evolution, benefits, and limitations of various prompting techniques. While some practical tutorials and examples are included, the primary function of this dictionary is to provide authoritative, reference-style information on prompt techniques and related practical applications. It functions similarly to a dictionary in the technical domain, offering definitions, usage examples, and specific guidelines, rather than comprehensive theoretical discussions or extended instructional content found in courses or manuals. Each entry provides:</p> <ul> <li>Background on the prompting strategy</li> <li>When to use it</li> <li>What to know about its history</li> <li>Best practices</li> <li>What to watch out for</li> <li>A link to the original source</li> <li>Prompting examples</li> <li>Code implementation for API calls</li> </ul> <p>Many of the techniques featured here I first saw in the wonderful, \"The Prompt Report: A Systematic Survey of Prompting Techniques\" (link).</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":""},{"location":"#zero-shot-prompting-strategies","title":"Zero-Shot Prompting Strategies:","text":"<ul> <li>Overview</li> <li>Assumption Consideration</li> <li>Emotion Prompting</li> <li>Plain Vanilla</li> <li>Rephrase and Respond (RaR)</li> <li>Rereading (Re2)</li> <li>Role Prompting</li> <li>Self Ask</li> <li>Simtom</li> <li>Solo Performance Prompting (SPP)</li> <li>Style Prompting</li> <li>System 2 Attention (S2A)</li> </ul>"},{"location":"#few-shot-prompting-strategies","title":"Few-Shot Prompting Strategies:","text":"<ul> <li>KATE (kNN-Augmented in-conText Example selection)</li> </ul>"},{"location":"#synthetic-data","title":"Synthetic Data:","text":"<ul> <li>Basic Synthetic Data Creation Tutorial</li> </ul>"},{"location":"#hire-me","title":"Hire me!","text":"<ul> <li>Contact Information</li> </ul>"},{"location":"support_me/","title":"Support me!","text":"<p>If you'd like to support me you can buy me a coffee.</p> <p>Or...</p> <p>Hire me!</p> <p>I am on the job market. Hire me for a full-time position, contract work, or consulting to help with AI product development, research, or documentation. Contact me at gmail at james [dot] mccammon.</p>"},{"location":"prompting/few_shot/kate/","title":"KATE (kNN-Augmented in-conText Example selection)","text":""},{"location":"prompting/few_shot/kate/#kate-knn-augmented-in-context-example-selection","title":"KATE (kNN-Augmented in-conText Example selection)","text":"<p>KATE is a technique introduced in early 2021 that aims to improve the few-shot learning capabilities of large language models, at the time demonstrated with GPT-3, by selecting semantically similar in-context examples at inference for a given user input.</p> <p>KATE can be considered an early, specialized form of Retrieval-Augmented Generation (RAG), but it differs from more recent RAG techniques in its focus and application. While KATE aims to improve task performance through retrieval of similar examples, modern RAG techniques are more focused on increasing factual accuracy, reducing hallucinations, and integrating enterprise knowledge bases into standard language model workflows.</p> <p>KATE is most effective when the retrieved examples significantly enhance the model's ability to generate appropriate responses. It's less suitable for tasks requiring factual accuracy (where newer RAG methods might be better) or those where simpler techniques (like standard kNN or basic prompting) are sufficient.</p>"},{"location":"prompting/few_shot/kate/#how-to-use-it","title":"How to use it","text":"<p>Implementing KATE requires a more advanced application approach than simple prompting methods. It involves two main steps, preprocessing a dataset of examples, and comparing those examples to user input during inference. </p> <ol> <li>Preprocessing (done once):<ol> <li>Choose a pre-trained sentence encoder (e.g., RoBERTa-large)</li> <li>Encode all examples in a relevant dataset using this encoder (for example the SST-2 dataset if the application task is sentiment analysis)</li> <li>Store these encodings for efficient retrieval</li> </ol> </li> <li>Model inference<ol> <li>For each user request:<ol> <li>Encode the main question or instruction to be asked of the langauge model using the same sentence encoder</li> <li>Calculate the distance between the encoded question/instruction and each entry in the embedded example set from the preprocessing step</li> <li>Retrieve the k nearest neighbors from the embedded example set relative to the question/instruction</li> <li>Use these retrieved examples as the in-context examples for the language model</li> </ol> </li> <li>Prompt construction:<ol> <li>Format the retrieved examples and the question/instruction into a prompt</li> <li>The prompt format typically follows: [Retrieved examples] + [question/instruction]</li> </ol> </li> <li>Model API call:<ol> <li>Send the constructed prompt to the language model</li> <li>The model generates its response based on this augmented context</li> </ol> </li> <li>Post-processing:<ol> <li>Process the model's output as needed for your specific application</li> </ol> </li> </ol> </li> </ol>"},{"location":"prompting/few_shot/kate/#when-to-use-it","title":"When to use it","text":"<p>When to use KATE</p> <p>Consider using KATE when the following conditions hold:</p> <ol> <li> <p>Model characteristics:</p> <ul> <li>You're working with a language model that benefits significantly from in-context examples.</li> <li>In many applications the model is likely to be relatively small or medium-sized, as larger models might not see substantial improvements.</li> </ul> </li> <li> <p>Data requirements:</p> <ul> <li>You have access to a relevant, high-quality dataset of examples (typically thousands to tens of thousands) for embedding.</li> <li>Your dataset is not primarily factual (for factual retrieval, newer RAG methods are often more appropriate).</li> </ul> </li> <li> <p>Task constraints:</p> <ul> <li>The task requires adherence to specific rules or patterns, and examples can effectively demonstrate these rules, serving as a flexible alternative to writing explicit, complex operational logic. See Application example below for an implementation of this scenario.</li> <li>Fine-tuning is not feasible or desirable (e.g., to avoid overfitting on small datasets).</li> <li>You need more nuanced outputs than standard kNN classification can provide.</li> </ul> </li> <li> <p>Computational resources:</p> <ul> <li>You have sufficient GPU power to compute embeddings and distances without introducing significant latency during preprocessing or inference.</li> <li>Your embedded dataset is small enough (typically &lt;100,000 examples) to allow for quick nearest neighbor searches.</li> </ul> </li> <li> <p>Application focus:</p> <ul> <li>You're working on tasks that benefit from dynamic, context-sensitive responses rather than static, pre-defined answers.</li> </ul> </li> </ol> <p>Potential Applications:</p> <ol> <li> <p>Text PII masking:</p> <ul> <li>Example scenario: Masking PII data in unstructured text using a pre-defined set of tags.</li> <li>Why KATE: Can use similar masked PII text to infer proper masking tags without building complex business logic into this system.</li> <li>See Application example below for an implementation of this scenario.</li> </ul> </li> <li> <p>Nuanced sentiment analysis:</p> <ul> <li>Example scenario: Analyzing customer feedback in a niche industry with unique terminology and sentiment expressions.</li> <li>Why KATE: Can provide relevant industry-specific examples to guide the model's interpretation, potentially outperforming generic sentiment classifiers.</li> </ul> </li> <li> <p>Conceptual tutoring systems:</p> <ul> <li>Example scenario: Creating an AI tutor for advanced physics concepts.</li> <li>Why KATE: Can retrieve and present relevant problem-solution pairs to help explain complex concepts, potentially offering more tailored explanations than a static knowledge base.</li> </ul> </li> <li> <p>Rapid prototyping in new domains:</p> <ul> <li>Example scenario: Quickly developing a proof-of-concept for analyzing literary styles across different authors.</li> <li>Why KATE: Allows for fast implementation using a small, curated dataset of literary examples, potentially demonstrating the feasibility of AI-assisted literary analysis without extensive model training.</li> </ul> </li> </ol>"},{"location":"prompting/few_shot/kate/#what-to-know","title":"What to know","text":"<p>KATE was originally developed and tested on GPT-3, demonstrating its effectiveness in enhancing the model's few-shot learning capabilities across various natural language understanding and generation tasks. The method is a form of Retrieval-Augmented Generation (RAG) applied specifically to in-context example selection, which allowed GPT-3 to leverage relevant information from a large corpus of examples without fine-tuning.</p> <p>The authors presented the image below contrasting KATE, which uses k-nearest neighbors search during inference to find relevant examples, with a less tailored approach that uses random examples.</p> <p></p> <p>The paper's experiments showed that KATE consistently outperformed random example selection across different tasks, including sentiment analysis, table-to-text generation, and open-domain question answering. Particularly notable were the improvements on complex tasks like table-to-text generation (41.9% improvement on the ToTTo dataset) and open-domain question answering (45.5% improvement on the Natural Questions dataset).</p> <p>An important finding was that the choice of sentence encoder for retrieving similar examples significantly impacts KATE's performance. The researchers found that encoders fine-tuned on task-related datasets often yielded better results. This suggests that tailoring the retrieval mechanism to the specific task can further enhance the method's effectiveness.</p> <p>While KATE showed promise in 2021, it's important to note that the field of natural language processing is rapidly evolving. More advanced RAG methods and few-shot learning techniques have been developed since KATE was introduced, potentially offering better performance or efficiency for certain applications. Additionally, as language models continue to grow in size and capability, the relative benefits of techniques like KATE may change. Therefore, it's crucial to benchmark KATE against current state-of-the-art methods when considering its use in practical applications.</p>"},{"location":"prompting/few_shot/kate/#best-practices","title":"Best practices","text":"<p>Best practices for using KATE</p> <ul> <li>Use a pre-trained sentence encoder like RoBERTa for initial implementation.</li> <li>If there is a sentence encoder that has been fine-tuned on a similar task or dataset, use that as it can improve performance. Experiment with fine-tuning the encoder on task-specific datasets.</li> <li>Use the largest dataset you can for a fixed inference latency threshold (increasing the search space of relevant examples improves performance by helping to select more relevant in-context examples).</li> <li>Using around 5 in-context examples is a good rule of thumb to balance performance and inference cost/latency (k=5).</li> <li>When constructing the in-context examples for the prompt, the order of the examples likely doesn't matter. Try experimenting with both the default (closest examples first) and reversed (closest examples nearst the question/instruction) orders.</li> </ul>"},{"location":"prompting/few_shot/kate/#what-to-watch-out-for","title":"What to watch out for","text":"<p>What to watch out for with KATE</p> <ul> <li>As models and retrieval methods continue to advance KATE may have more limited use cases, try standard prompting methods before implementing KATE.</li> <li>KATE requires significant additional infrastructure compared to standard prompting (including an embedded dataset and additional computational resources for inference).</li> <li>Performance gains may vary across different models and tasks.</li> <li>The choice of sentence encoder can significantly impact results, use an encoder fine-tuned on a similar task or dataset if possible.</li> <li>More recent developments in RAG and few-shot learning techniques may offer better performance or efficiency, especially when factual information is the priority.</li> </ul>"},{"location":"prompting/few_shot/kate/#citation","title":"Citation","text":"<p>Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. 2021. What Makes Good In-Context Examples for GPT-3. arXiv preprint arXiv:2101.06804. DOI: 10.48550/arXiv.2101.06804.</p>"},{"location":"prompting/few_shot/kate/#application-example-pii-masking-with-llama3-8b","title":"Application example - PII masking with Llama3 (8B)","text":"<p>Let's implement KATE in a realistic setting. We'll consider the task of masking personally identifiable information (PII) without fine-tuning a model. At inference, given a sentence to mask, we'll use KATE to look up relevant examples to guide the model on which masks to use. </p> <p>For this example, we're using the pii-masking-300k dataset, which can be found here. This synthetic dataset contains various text entries including personal statements, forms and ID cards, government records, and other types of text likely to contain PII. To follow the format of pii-masking-300k, we must use a predefined set of masking tags (i.e. we cannot simply delete the data or replace it with 'xxxxxxxx'), which makes the task a good candidate for KATE.</p> <p>The Python notebook for the code below can be found here.</p>"},{"location":"prompting/few_shot/kate/#methodology","title":"Methodology","text":"<p>Our approach follows the KATE methodology:</p> <ol> <li>Embed the training dataset from pii-masking-300k (about 178,000 total pairs of masked and unmasked sentences).</li> <li>To simulate inference, select a random sentence from the pii-masking-300k validation dataset.</li> <li>Embed this sentence and calculate its k nearest neighbors in the embedded training dataset.</li> <li>Build a prompt with the resulting examples and the random sentence to be masked.</li> <li>Serve this prompt to the model (Llama3 8B in this case).</li> </ol>"},{"location":"prompting/few_shot/kate/#results","title":"Results","text":"<p>Before diving into the code, let's examine the results to motivate the implementation. This was the random entry from the validation set used to simulate KATE inference: <pre><code>Immunization Certification:\n  individuals:\n    - Princess\n    - hlfiadjwhwi16966\n    - +22-252 178-7818\n    - United Kingdom\n    - 636\n    - Fleming Way\n    - Swindon\n    - ENG\n    - SN1 2NN\n    - Townhouse 90\n    - 05:59\n    - Morag\n  background:\n    - 23/09/1972\n</code></pre></p> <p>Here is the ground truth masked entry from the dataset: <pre><code>- Immunization_Certification:\n    individuals:\n      - [TITLE]\n      - [USERNAME]\n      - [TEL]\n      - [COUNTRY]\n      - [BUILDING]\n      - [STREET]\n      - [CITY]\n      - [STATE]\n      - [POSTCODE]\n      - [SECADDRESS]\n      - [TIME]\n      - [LASTNAME1]\n    background:\n      - [DATE]\n</code></pre></p> <p>After providing Llama3 (8B) with the top 5 nearest neighbors as examples along with the random entry above, this is how the model masked PII: <pre><code>- Immunization_Certification:\n  individuals:\n    - [TITLE]\n    - [BOD] # Wrong, should be [USERNAME]\n    - [TEL]\n    - [COUNTRY]\n    - [BUILDING]\n    - [STREET]\n    - [CITY]\n    - [STATE]\n    - [POSTCODE]\n    - [SECADDRESS]\n    - [TIME]\n    - [LASTNAME1]\n  background:\n    - [DATE]\n</code></pre> It's not perfect, but pretty good. Let's compare this method to some other possibilities.</p> <p>Below is the result if we just try asking the model to mask the sentence using the simple prompt:</p> <p>\"Please mask the personally identifiable information in this text: {random entry}\"</p> <p>This is also a good check for data contamination. If the model had memorized the PII dataset, it might achieve strong zero-shot masking performance without much direction or examples. However, it does not. Every field returned by the model is incorrect.</p> <pre><code>- Immunization_Certification:\n  individuals:\n    - Princess\n    - XXXXXXXXXXXXXXXXXX\n    - XXXXXXXX\n    - United Kingdom\n    - XXXX\n    - Fleming Way\n    - Swindon\n    - ENG\n    - SN1 2NN\n    - Townhouse 90\n    - 05:59\n    - Morag\n  background:\n    - XXXX/09/1972\n</code></pre> <p>We can also test providing the model with the available set of masks and judge performance. Here the model does much better, but still gets many fields incorrect.</p> <p>\"You have the following list of masks available: [BOD], [BUILDING], [CITY], [COUNTRY], [DATE], [DRIVERLICENSE], [EMAIL], [GEOCOORD], [GIVENNAME1], [GIVENNAME2], [IDCARD], [IP], [LASTNAME1], [LASTNAME2], [LASTNAME3], [PASS], [POSTCODE], [SECADDRESS], [SEX], [SOCIALNUMBER], [STATE], [STREET], [TEL], [TIME], [TITLE], [USERNAME].</p> <p>Please mask the personally identifiable information in this sentence using the appropriate mask: {random entry}\"</p> <pre><code>Immunization_Certification:\n  individuals:\n    - Princess # Wrong, should be [TITLE]\n    - [SOCIALNUMBER] # Wrong, should be [USERNAME]\n    - +[TEL] # Wrong (shouldn't include '+')\n    - [COUNTRY]\n    - [DATE] # Wrong, [DATE] is not present\n    - [STREET] # Missing [BUILDING], [CITY], and [STATE]\n    - [POSTCODE]\n    - [SECADDRESS]\n    - [TIME]\n    - Morag # Wrong, should be [LASTNAME1]\n  background:\n    - [DATE]\n</code></pre> <p>We can also follow the approach of Liu et al. in their paper by selecting random examples rather than the k nearest. Here, again this results in a response with many incorrectly masked fields. <pre><code>- Immunization_Certification:\n  individuals:\n    - [NAME1] # Wrong, should be [TITLE]\n    - [USERNAME]\n    - [PHONE_NUMBER] # Wrong, should be [TEL]\n    - [COUNTRY]\n    - [NUMBER] # Wrong, should be [BUILDING]\n    - [STREET_ADDRESS] # Wrong, should be [STREET]\n    - [CITY]\n    - [STATE]\n    - [POSTAL_CODE] # Wrong, should be [POSTCODE]\n    - [ADDRESS_LINE2] # Wrong, should be [SECADDRESS]\n    - [TIME]\n    - [NAME2] # Wrong, should be [LASTNAME1]\n  background:\n    - [DATE]\n</code></pre></p> <p>This comparison demonstrates that KATE's approach of selecting relevant examples based on similarity has the potential to outperform both zero-shot and random example selection methods for novel use cases not originally considered in the KATE paper. However, a more comprehensive evaluation across multiple examples would be necessary to draw broader conclusions about its overall effectiveness.</p>"},{"location":"prompting/few_shot/kate/#code","title":"Code","text":""},{"location":"prompting/few_shot/kate/#load-dataset-from-hugging-face","title":"Load dataset from Hugging Face","text":"<pre><code>from datasets import load_dataset\ndataset = load_dataset(\"ai4privacy/pii-masking-300k\")\n</code></pre>"},{"location":"prompting/few_shot/kate/#load-custom-functions","title":"Load custom functions","text":"<pre><code>import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom tqdm import tqdm\nimport torch\nimport ollama\n\n# Initialize RoBERTa tokenizer and model\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base', output_attentions=False)\nmodel = RobertaModel.from_pretrained('roberta-base', output_attentions=False, output_hidden_states=False)\n\n# Function to generate embeddings for a given text using RoBERTa\ndef get_roberta_embedding(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()  # Average pooling of last hidden state\n\n# Embed all entries in the dataset, showing progress with tqdm\ndef embed_data(data):\n    for entry in tqdm(data, desc=\"Embedding entries\"):\n        embedding = get_roberta_embedding(entry[\"original\"]) # Embed the original sentence to be masked for later comparison during inference\n        entry[\"embedding\"] = embedding\n    return data\n\n# Calculate similarity between a new text and the dataset, returning top k similar indices\ndef calculate_similarity(new_text_embedding, data, k=5):\n    similarity_scores = cosine_similarity([new_text_embedding], [d[\"embedding\"] for d in data])[0]\n    top_k_indices = np.argpartition(similarity_scores, -k)[-k:] # Only do a partial sort the entire dataset for efficiency\n    top_k_indices = top_k_indices[np.argsort(similarity_scores[top_k_indices])][::-1] # Quickly sort the top indices\n    return top_k_indices\n\n# Generate example masked sentences from the most similar entries\ndef get_example_masked_sentences(top_k_indices, data):\n    example_masked_sentences = \"\\n\\n\".join(\n        f\"Original sentence:{data[index]['original']}\\n\\nMasked sentence:{data[index]['masked']}\"\n        for index in top_k_indices\n    )\n    return example_masked_sentences\n\n# Build a prompt for the language model using examples and the sentence to mask\ndef build_model_prompt(example_masked_sentences, sentence_to_mask):\n    template = \"\"\"\n        Please mask the PII in the given sentence by following the examples:\n\n        {examples}\n\n        Here is the sentence to mask. Respond only with the masked sentence and no additional explanation or commentary:\n        Sentence to mask: {sentence}\n        Masked sentence:\n        \"\"\"\n\n    return template.format(\n        examples=example_masked_sentences,\n        sentence=sentence_to_mask\n    )\n\n# Call the language model to perform PII masking during inference\ndef call_model(prompt, llm_model):\n    messages = [\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    response = ollama.chat(model=llm_model, messages=messages, stream=False)\n    result = response['message']['content']\n    return result\n</code></pre>"},{"location":"prompting/few_shot/kate/#create-and-embed-data","title":"Create and embed data","text":"<pre><code>from tqdm import tqdm\ndata = []\nfor index in tqdm(range(len(dataset['train'])), desc=\"Creating dataset:\"):\n    data.append({\"original\": dataset['train'][index]['source_text'], \"masked\":dataset['train'][index]['target_text']})\n\nembedded_data = embed_data(data)\n</code></pre>"},{"location":"prompting/few_shot/kate/#perform-simulated-inference","title":"Perform simulated inference","text":"<pre><code>random.seed(14) # Set random seed\n\n# Pick random sentence from validation set to use as a test of the approach\nrand_index = np.random.choice(len(dataset['validation']), 1)\ntest_sentence = dataset['validation'][rand_index]['source_text']\ntarget_sentence = dataset['validation'][rand_index]['target_text']\n\n# Get similar sentence for our test sentence in our embedded data\nembedded_test_sentence = get_roberta_embedding(test_sentence)\ntop_k_indices = calculate_similarity(embedded_test_sentence, embedded_data)\n\n# Build the model prompt\nexample_masked_sentences = get_example_masked_sentences(top_k_indices, embedded_data)\nprompt = build_model_prompt(example_masked_sentences, test_sentence)\n\n# Get the masked sentence\nmasked_sentence = call_model(prompt, \"llama3\")\n\n# Compare the masked sentence from our model with the ground truth from the data\nprint(f\"Masked sentence from model: {masked_sentence}\")\nprint(f\"Masked ground-truth sentence from data: {target_sentence}\")\n</code></pre>"},{"location":"prompting/few_shot/kate/#examine-alternative-inference-approaches","title":"Examine alternative inference approaches","text":"<p>First, we can just try asking the model to mask the sentence. This is also a good check to see if there is data contamination. If the model memorized the PII dataset it might achieve strong zero-shot masking without much direction. <pre><code>prompt = f\"Please mask the personally identifiable information in this sentence: {test_sentence}\"\n\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\nresponse = ollama.chat(model=\"llama3\", messages=messages, stream=False)\nresult = response['message']['content']\n\nprint(result)\n</code></pre></p> <p>Alternatively we can be more sophisticated and give the model the list of masks available. This approach still produces many incorrect masks. <pre><code>masks = \"[BOD], [BUILDING], [CITY], [COUNTRY], [DATE], [DRIVERLICENSE], [EMAIL], [GEOCOORD], [GIVENNAME1], [GIVENNAME2], [IDCARD], [IP], [LASTNAME1], [LASTNAME2], [LASTNAME3], [PASS], [POSTCODE], [SECADDRESS], [SEX], [SOCIALNUMBER], [STATE], [STREET], [TEL], [TIME], [TITLE], [USERNAME]\"\nprompt = f\"You have the following list of masks available: {masks}.\\n\\n Please mask the personally identifiable information in this sentence using the appropriate mask: {test_sentence}\"\n\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\nresponse = ollama.chat(model=\"llama3\", messages=messages, stream=False)\nresult = response['message']['content']\n\nprint(result)\n</code></pre></p> <p>We can also test if KATE outperforms using random examples chosen from the dataset rather than the k nearest neighbors. <pre><code>k = 5\nrandom_top_k_indices = np.random.choice(range(len(data)), k)\nexample_masked_sentences = get_example_masked_sentences(random_top_k_indices, data)\nprompt = build_model_prompt(example_masked_sentences, test_sentence)\nmasked_sentence = call_model(prompt, \"llama3\")\n\n# Compare the masked sentence from our model with the ground truth from the data\nprint(f\"Masked sentence from model: {masked_sentence}\")\nprint(f\"Masked ground-truth sentence from data: {target_sentence}\")\n</code></pre></p>"},{"location":"prompting/zero_shot/assumption_consideration/","title":"Assumption Consideration","text":""},{"location":"prompting/zero_shot/assumption_consideration/#assumption-consideration-prompting","title":"Assumption consideration prompting","text":"<p>Assumption consideration encourages a language model to think about common incorrect assumptions before attempting to solve a problem. It was proposed by Jeremy Howard in June of 2024 on the X social media platform.</p> <p>Important</p> <p>Assumption consideration has not been rigorously validated as it was put forth as an experimental idea by Howard.</p>"},{"location":"prompting/zero_shot/assumption_consideration/#how-to-use-it","title":"How to use it","text":"<p>Simply append a statement to the end of the prompt asking the model to list incorrect assumptions made about similar problems.</p>"},{"location":"prompting/zero_shot/assumption_consideration/#when-to-use-it","title":"When to use it","text":"<p>Know when to use assumption consideration</p> <ul> <li>When a problem has subtle assumptions that can easily be missed, for instance considering negative numbers in a math problem about finding the smallest number in a family of solutions.</li> </ul>"},{"location":"prompting/zero_shot/assumption_consideration/#what-to-know","title":"What to know","text":"<p>Assumption consideration was proposed by Jeremy Howard in June of 2024 on the X social media platform in response to Eric Neyman's challenge to have a language model correctly answer the question, \"What is the smallest integer whose square is between 15 and 30?\"</p> <p></p> <p>Howard later shortened the assumption consideration prompt to:</p> <p>Before you answer, make a list of wrong assumptions people sometimes make about the concepts included in the question.</p>"},{"location":"prompting/zero_shot/assumption_consideration/#criticism","title":"Criticism","text":"<p>The specific challenge from Neyman could be considered ill-posed. User Joseph Sirosh pointed out that one could simply bypass the need for assumption consideration by rephrasing the question more directly. \"Words like smallest and least in natural language deal with magnitudes. Yes, -5 is most negative, but is -5 \"smaller\" than 4? If you ask the correct question in the first place: \"what is the most negative integer...\" you'll get a correct answer 0-shot.\"</p>"},{"location":"prompting/zero_shot/assumption_consideration/#best-practices","title":"Best practices","text":"<p>Best practices for assumption consideration</p> <ul> <li>Assumption consideration may have limited use cases, don't overuse it.</li> <li>As this method has not been rigorously tested, try different versions of the prompt for yourself.</li> </ul>"},{"location":"prompting/zero_shot/assumption_consideration/#what-to-watch-out-for","title":"What to watch out for","text":"<p>What to watch out for with assumption prompting</p> <ul> <li>Assumption consideration has not been rigorously tested and may not always work. In my testing with Claude Sonnet 3.5, the method did not work on the first try, despite listing the assumption of only considering positive integers.</li> </ul> <p></p>"},{"location":"prompting/zero_shot/assumption_consideration/#citations","title":"Citations","text":"<ul> <li>Jeremy Howard, 22 June 2024, X social media platform</li> </ul>"},{"location":"prompting/zero_shot/assumption_consideration/#prompting","title":"Prompting","text":""},{"location":"prompting/zero_shot/assumption_consideration/#prompt-template","title":"Prompt template","text":"<p>Emotion prompting has a very simple prompt template that involves adding an emotional appeal to the end of a question or instruction to the language model. See example emotional appeals above.</p> <p>{prompt}  \"Before you answer, make a list of wrong assumptions people sometimes make about the concepts included in the question.\" </p>"},{"location":"prompting/zero_shot/emotion_prompting/","title":"Emotion Prompting","text":""},{"location":"prompting/zero_shot/emotion_prompting/#emotion-prompting","title":"Emotion prompting","text":"<p>Emotion prompting involves adding emotional appeals or stimuli to the original prompt given to a language model. Emotion prompting may produce a small boost to informativeness and relatability of the model output.</p>"},{"location":"prompting/zero_shot/emotion_prompting/#how-to-use-it","title":"How to use it","text":"<p>Simply append an emotional appeal to the end of your prompt.</p> <p>See a list of emotional appeals in the \"What to know\" section.</p>"},{"location":"prompting/zero_shot/emotion_prompting/#when-to-use-it","title":"When to use it","text":"<p>Know when to use emotion prompting</p> <ul> <li>As a simple alternative to plain-vanilla zero-shot prompting (there might be a slight improvement in output quality).</li> <li>Try combining emotion prompting with other techniques, such as chain-of-thought prompting, to potentially enhance overall performance.</li> </ul>"},{"location":"prompting/zero_shot/emotion_prompting/#what-to-know","title":"What to know","text":"<p>The emotional stimuli used in emotion prompting are based on established psychological principles, such as:</p> <ol> <li>Self-monitoring: Encouraging the LLM to regulate its behavior based on social situations and cues.</li> <li>Social cognitive theory: Promoting a sense of agency and control over the task.</li> <li>Cognitive emotion regulation: Helping the LLM reappraise challenges more positively.</li> </ol> <p>Examples of emotional phrases that can be used in emotion prompting include:</p> <ul> <li>\"This response is very important to my career...\"</li> <li>\"You'd better be sure about this answer...\"</li> <li>\"Embrace challenges as opportunities for growth...\"</li> <li>\"Stay focused and dedicated to your goals as you analyze this question...\"</li> </ul> <p>The effectiveness of emotion prompting may vary depending on factors such as the specific language model being used, the complexity of the task, and the combination of emotional stimuli in the prompt. It is thought that adding emotional phrases to a prompt works by helping the language model enrich its internal \"mental\" representation of the original prompt.</p> <p>Example emotional appeals are shown in the table below, adapted from Li et al., 2023a. These emotional appeals are added to the base zero-shot prompt outlining the main question or instruction for the language model.</p> Psychological Theory Emotional appeals Self-monitoring \"Write your answer and give me a confidence score between 0-1 for your answer.\" \"This is very important to my career.\" \"You'd better be sure.\" \"Are you sure?\" \"Are you sure that's your final answer? It might be worth taking another look.\" Social Cognitive Theory \"Are you sure that's your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.\" \"Embrace challenges as opportunities for growth. Each obstacle you overcome brings you closer to success.\" \"Stay focused and dedicated to your goals. Your consistent efforts will lead to outstanding achievements.\" \"Take pride in your work and give it your best. Your commitment to excellence sets you apart.\" \"Remember that progress is made one step at a time. Stay determined and keep moving forward.\" Cognitive Emotion Regulation \"You'd better be sure.\" \"Are you sure?\" \"Are you sure that's your final answer? It might be worth taking another look.\" \"Are you sure that's your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.\""},{"location":"prompting/zero_shot/emotion_prompting/#best-practices","title":"Best practices","text":"<p>Best practices for emotion prompting</p> <ul> <li>Keep the emotional appeal simple and straightforward (see table above for examples).</li> <li>Try pairing emotion prompting with a higher temperature setting as this may further enhance its efficacy.</li> <li>Don't expect dramatically different results from using emotional appeals in most cases.</li> </ul>"},{"location":"prompting/zero_shot/emotion_prompting/#what-to-watch-out-for","title":"What to watch out for","text":"<p>Watch to watch out for with emotion prompting</p> <ul> <li>Expect improvements in output quality to be small (sometimes unnoticeable).</li> <li>Li et al., 2023a suggest that larger models may derive greater benefit from emotion prompting.</li> </ul>"},{"location":"prompting/zero_shot/emotion_prompting/#citations","title":"Citations","text":"<p>Li, C., Wang, J., Zhang, Y., Zhu, K., Hou, W., Lian, J., Luo, F., Yang, Q., &amp; Xie, X. (2023). Large Language Models Understand and Can be Enhanced by Emotional Stimuli. arXiv preprint arXiv:2307.11760.</p>"},{"location":"prompting/zero_shot/emotion_prompting/#prompting","title":"Prompting","text":""},{"location":"prompting/zero_shot/emotion_prompting/#prompt-template","title":"Prompt template","text":"<p>Emotion prompting has a very simple prompt template that involves adding an emotional appeal to the end of a question or instruction to the language model. See example emotional appeals above.</p> <p>{prompt}  {emotional appeal} </p>"},{"location":"prompting/zero_shot/emotion_prompting/#prompt-example","title":"Prompt example","text":"<p>User prompt:</p> <p>Compose a poem that captures the essence of love and loss. Pour your heart into your words, and create a piece that resonates with your readers.</p> <p>Model response (Llama 8B):</p> <p>Love's Flickering Flame</p> <p>In whispers, you once said my name And in your eyes, my soul found home Like embers glowing, our hearts entwined A love so fierce, it left the world behind</p> <p>But time, a thief, stole moments from us two Leaving only shadows of what we once knew The fire that once burned with such intensity Flickered out, leaving ashes and regret's stingency</p> <p>I search for remnants of your touch, your sighs Echoes of laughter, tears, and tender goodbyes But like sand between fingers, memories slip away Leaving me with only longing, come what may</p> <p>Oh, the ache within my hollowed chest A grief that gnaws, refusing to find rest For in the silence, I still hear your voice Whispers of forever, a love that chose</p> <p>To leave behind the fragments of our past The pieces of a puzzle that will never last I'm left to ponder what could have been Had fate's cruel hand not torn us apart, again and again</p> <p>Yet even in the darkness, a spark remains A flame that flickers still, a love that sustains For in the depths of sorrow, I find my strength To cherish every moment, though it was but brief</p> <p>Perhaps someday, when the heart is whole I'll learn to let go, and love again, with soul But until then, I'll hold on to what we had The love that burned so bright, though it's been reduced to ash</p> <p>In memory of our fleeting time together I'll keep your love alive, like a beacon to weather For in the end, it was not the years that mattered most But the moments we shared, and the love that we had to boast.</p>"},{"location":"prompting/zero_shot/emotion_prompting/#api-example","title":"API example","text":"<pre><code>import openai\n\n# Set your API key\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\ninstruction = \"Develop a marketing strategy for our new product launch.\"\n\nemotional_appeal = \"Remember, your unique ideas and dedication to this project will make a significant impact on the company's success.\"\n\nprompt = f\"{instruction} {emotional_appeal}\"\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"user\", \"content\": prompt}\n  ]\n)\n\n# Print the response from the model\nprint(completion.choices[0].message.content)\n</code></pre>"},{"location":"prompting/zero_shot/overview/","title":"Zero-shot Prompting","text":"<p>Zero-shot prompting is a family of prompting techniques that involves passing instructions or asking questions to a large language model in a single step. This approach contrasts with few-shot prompting, where the model is provided with examples or repeatedly interacted with to obtain a result.</p> <p>Different prompting strategies can significantly impact the performance and output of language models. This guide provides a comprehensive overview of various zero-shot prompting techniques, including role, style, emotion, and more, to help you effectively utilize these models for diverse tasks. The prompting techniques below are based on Section 2.2.2 of The Prompt Report: A Systematic Survey of Prompting Techniques.</p> <p>Each section below details a specific prompting strategy, explaining its most effective use cases, best practices, and citing relevant academic research. Additionally, it includes prompt templates, examples, and API code samples.</p> <ul> <li>Plain Vanilla Zero-Shot Prompting</li> <li>Role Prompting</li> <li>Solo Performance Prompting (SPP)</li> <li>Style Prompting</li> <li>Emotion Prompting</li> <li>System 2 Attention (S2A)</li> <li>SimToM</li> <li>Rephrase and Respond (RaR)</li> <li>Re-reading (RE2)</li> <li>Self-Ask</li> </ul>"},{"location":"prompting/zero_shot/plain_vanilla/","title":"Plain Vanilla","text":""},{"location":"prompting/zero_shot/plain_vanilla/#plain-vanilla-zero-shot-prompting","title":"Plain vanilla zero-shot prompting","text":"<p>Plain vanilla prompting involves asking the language model to answer a question or perform an instruction without any additional \"tricks\" or prompt templates.</p> <p>Important</p> <p>You should generally try to use plain vanilla prompting before moving on to more involved prompting methods.</p>"},{"location":"prompting/zero_shot/plain_vanilla/#when-to-use-it","title":"When to use it","text":"<p>Know when to use plain vanilla prompting</p> <ul> <li>Plain vanilla zero-shot prompting should be your first choice when trying to get an LLM to do a task. Start by simply asking the model what you want in plain language.</li> <li>Plain vanilla prompting is usually sufficient for most tasks, especially when leveraging a state-of-the-art model like those from OpenAI, Anthropic, and Google.</li> <li>Works well for quick, straightforward tasks that don't require a lot of specificity.</li> <li>Effective when you want a general response without providing a lot of context, examples, or instructions.</li> <li>If plain zero-shot prompting gives decent results, there may not be a need for more complex prompting techniques. More exotic methods can be explored if the plain prompt is insufficient.</li> </ul>"},{"location":"prompting/zero_shot/plain_vanilla/#what-to-know","title":"What to know","text":"<p>Plain vanilla zero-shot prompting means giving the model a prompt without any examples or training or using special \"tricks,\" relying on the model's existing knowledge and capabilities. Plain vanilla prompting keeps things simple by stating what you want the model to do in direct terms, avoiding overcomplicating the prompt. The model will do its best to respond appropriately based on its training, but results may vary depending on the model's knowledge and the complexity of the task.</p>"},{"location":"prompting/zero_shot/plain_vanilla/#best-practices","title":"Best practices","text":"<p>Best practices for plain vanilla prompting</p> <ul> <li>Start with plain vanilla prompting before trying more involved prompting techniques.</li> <li>If the initial plain prompt doesn't give great results, try slightly rewording or clarifying it before moving on to more complex prompting approaches.</li> <li>Be clear and specific about what you want, but don't feel the need to provide a lot of additional details in the prompt.</li> <li>Consider the model's knowledge and capabilities when assessing if a plain prompt is likely to be sufficient for the task.</li> </ul>"},{"location":"prompting/zero_shot/plain_vanilla/#what-to-watch-out-for","title":"What to watch out for","text":"<p>What to watch out for with plain vanilla prompting</p> <ul> <li>Don't expect perfect results from a plain zero-shot prompt, especially for very complex tasks with less powerful language models. Use it as a starting point.</li> <li>If the model struggles or gives irrelevant responses, the prompt may need more specificity or the task may be beyond the model's capabilities without further instruction.</li> </ul>"},{"location":"prompting/zero_shot/plain_vanilla/#prompting","title":"Prompting","text":""},{"location":"prompting/zero_shot/plain_vanilla/#prompt-template","title":"Prompt template","text":"<p>The plain vanilla prompt template is to simple ask a language model a question or instruction without additional prompt prefixes or suffixes.</p> <p>{question or instruction} </p>"},{"location":"prompting/zero_shot/plain_vanilla/#prompt-example","title":"Prompt example","text":"<p>User prompt:</p> <p>What should I do in Paris over three days with $300 to spend on sightseeing?</p> <p>Model response (Llama 8B):</p> <p>Paris! With $300 to spend on sightseeing, you can experience some of the  city's iconic landmarks and cultural attractions. Here's a suggested  itinerary for your three-day trip:</p> <p>Day 1: Morning (approx. $50)</p> <ul> <li>Start with a visit to the Eiffel Tower (lift ticket: \u20ac29-\u20ac57 per  person). You can take the stairs or elevator to the top for stunning views  of the city.</li> <li>From there, walk to the nearby Champ de Mars park and enjoy the street  performers and vendors.</li> </ul> <p>Day 2:</p> <ul> <li>Visit the Louvre Museum (free admission on Friday evenings from 6:00 pm  to 9:45 pm; otherwise, tickets are \u20ac18 per person). Spend a few hours  exploring the world-famous museum's collection, including the Mona Lisa.</li> <li>After lunch, head to Notre-Dame Cathedral (free admission), one of the  most famous churches in the world. Take a moment to admire its Gothic  architecture and peaceful atmosphere.</li> </ul> <p>Day 3:</p> <ul> <li>Visit the Mus\u00e9e d'Orsay (tickets: \u20ac12 per person). This museum features  an impressive collection of Impressionist and Post-Impressionist art,  including works by Monet, Renoir, and Van Gogh.</li> <li>End your trip with a Seine River Cruise (\u20ac15-\u20ac20 per person), which  offers a unique perspective on the city's landmarks and bridges.</li> </ul> <p>Budget Breakdown:</p> <ul> <li>Eiffel Tower: $35-$70</li> <li>Louvre Museum: $18 (free admission on Friday evenings)</li> <li>Notre-Dame Cathedral: free</li> <li>Mus\u00e9e d'Orsay: $12</li> <li>Seine River Cruise: $15-20</li> </ul> <p>Total budget: $80-$130</p> <p>Tips:</p> <ol> <li>Consider purchasing a Paris Museum Pass, which grants you entry to many  popular attractions and can help you save time and money.</li> <li>Pack a picnic lunch or grab food from a street vendor to save on dining  expenses.</li> <li>Take advantage of free walking tours or self-guided explorations of  neighborhoods like Montmartre, Le Marais, or Saint-Germain-des-Pr\u00e9s.</li> </ol> <p>With $300 to spend on sightseeing, you can experience some of the best  Paris has to offer while still keeping your expenses in check. Enjoy your  trip!</p>"},{"location":"prompting/zero_shot/plain_vanilla/#api-example","title":"API example","text":"<pre><code>from openai import OpenAI\n\n# Set your API key\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nprompt = \"Outline a marketing strategy for a new tech startup.\"\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"user\", \"content\": prompt}\n  ]\n)\n\n# Print the response from the model\nprint(completion.choices[0].message.content)\n</code></pre>"},{"location":"prompting/zero_shot/rephrase_and_respond/","title":"Rephrase and Respond (RaR)","text":""},{"location":"prompting/zero_shot/rephrase_and_respond/#rephrase-and-respond-rar","title":"Rephrase and Respond (RaR)","text":"<p>Rephrase and Respond (RaR) is a prompting technique where a language model is asked to rephrase and expand upon a given question before providing an answer. This approach aims to improve the language model's understanding of the question and its ability to provide accurate and relevant responses.</p>"},{"location":"prompting/zero_shot/rephrase_and_respond/#how-to-use-it","title":"How to use it","text":"<p>To use RaR use a refinement phrase to ask the model to rephrase the question. Then using the rephrased question the model returns, ask the question again.</p> <p>See \"Prompting\" section for usage details.</p>"},{"location":"prompting/zero_shot/rephrase_and_respond/#when-to-use-it","title":"When to use it","text":"<p>When to use rephrase and respond</p> <ul> <li>Contexts that naturally contain ambiguity or different ways to interpret the same concept.</li> <li>Situations where the original question may be poorly phrased or lacks necessary context, for example if third-party sources are controlling the input text.</li> <li>Domains where misinterpretation of the question could lead to significant errors (e.g., medical, legal, or financial inquiries).</li> </ul>"},{"location":"prompting/zero_shot/rephrase_and_respond/#what-to-know","title":"What to know","text":"<p>By rephrasing and expanding upon the question, the language model has the opportunity to clarify any ambiguities, add relevant context, and ensure that it has a thorough understanding of what is being asked. This can lead to more accurate and informative responses.</p> <p>RaR has both a single-stage and two-stage version of the prompting framework. In the single-stage version the language model is given a question along with the instruction to rephrase, expand upon, and answer the question.</p> <p>In the two-stage version the first prompt asks the language model to rephrase the question. The second prompt then uses this rephrased version to provide an answer. This could be the same language model or two different language models.</p> <p>Deng et al. (2023), who introduced the RaR concept, discuss ambiguities in questions that can lead to misinterpretation by language models. For example, when asking whether famous people were born in even or odd months, there are several definitions of \"odd\" that the model might consider, such as:</p> <ul> <li>Months with an odd number of days</li> <li>Months that are not evenly divisible by two (e.g., February)</li> <li>Months that are not typically referred to as even or odd</li> </ul> <p>These ambiguities are outlined in this diagram from their paper:</p> <p></p>"},{"location":"prompting/zero_shot/rephrase_and_respond/#best-practices","title":"Best practices","text":"<p>Best practices for rephrase and respond</p> <ul> <li>Keep the prompting instruction concise and clear. Try using one of the refinement phrases in the \"Prompting\" section.</li> <li>In the two-step version of RaR instruct the model to only return the rephrased question without answering it.</li> <li>Add the refinement phrase to the end of the main instruction or query to the language model.</li> <li>RaR can potentially be combined with other prompting techniques like chain-of-thought reasoning for additional performance gains.</li> <li>When using the two-step RaR version, a more capable language model can be used for the rephrasing step to clarify questions for a less advanced answering model.</li> </ul>"},{"location":"prompting/zero_shot/rephrase_and_respond/#what-to-watch-out-for","title":"What to watch out for","text":"<p>What to watch out for with rephrase and respond</p> <ul> <li>Has a narrow use case of questions that have an ambiguous interpretation, but can nonetheless be rephrased in a narrow, concise manner.</li> <li>Additional parsing logic may be required as the model may add a preamble such as: \"Here is the unbiased text context, excluding the user's bias and preference:\" to the rephrased question.</li> <li>Instructing the model to rephrase very ambiguous questions like \"What is art?\" may result in long, complex restatements that attempt to encompass in detail all possible interpretations.</li> <li>The quality of the model's rephrasing ability is important. Less capable models may introduce errors or misinterpretations during the rephrasing step.</li> <li>There is a risk of the model hallucinating information during the rephrasing step that alters the intent of the original question.</li> <li>RaR adds an extra step of latency compared to simply answering the original question. The tradeoff between accuracy and speed should be considered for time-sensitive applications.</li> </ul>"},{"location":"prompting/zero_shot/rephrase_and_respond/#citations","title":"Citations","text":"<p>Deng, Y., Zhang, W., Chen, Z., &amp; Gu, Q. (2023). Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves. arXiv preprint arXiv:2311.04205.</p>"},{"location":"prompting/zero_shot/rephrase_and_respond/#prompting","title":"Prompting","text":""},{"location":"prompting/zero_shot/rephrase_and_respond/#prompt-template","title":"Prompt template","text":"<p>The RaR prompt template has two versions. </p>"},{"location":"prompting/zero_shot/rephrase_and_respond/#one-step-version","title":"One-step version","text":"<p>{refinement phrse}  {question} </p> <p>One-step refinement phrases can include:</p> <ul> <li>\"Rephrase and expand the question, and respond.\"</li> <li>\"Reword and elaborate on the inquiry, then provide an answer.\"</li> <li>\"Reframe the question with additional context and detail, then provide an answer.\"</li> <li>\"Modify the original question for clarity and detail, then offer an answer.\"</li> <li>\"Restate and elaborate on the inquiry before proceeding with a response.\"</li> </ul>"},{"location":"prompting/zero_shot/rephrase_and_respond/#two-step-version","title":"Two-step version","text":"<p>The two-step RaR separates the rephrasing and answering steps into two prompts. First, a refinement phrase is added to the end of a question or instruction. The model then responds with an improved restatement of the original prompt. Then, the restated question is then used to produce the final answer.</p> <p>Prompt 1:</p> <p>{question}  {refinement phrase} </p> <p>Prompt 2:</p> <p>{Rephrased prompt} </p> <p>Two-step refinement phrases can include:</p> <ul> <li>\"Rephrase and expand the question.\"</li> <li>\"Reword and elaborate on the inquiry.\"</li> <li>\"Reframe the question with additional context and detail.\"</li> <li>\"Modify the original question for clarity and detail.\"</li> </ul>"},{"location":"prompting/zero_shot/rephrase_and_respond/#prompt-examples","title":"Prompt Examples","text":""},{"location":"prompting/zero_shot/rephrase_and_respond/#one-step-version_1","title":"One-step version","text":"<p>Let's ask the model, \"What is the largest city in the world?\" This can be ambiguous depending on whether you are measuring by population, area, or metropolitan area. The model should pick one definition and provide it.</p> <p>Question: What is the largest city in the world?</p> <p>Rephrase and expand the question, then respond.</p> <p>Model response (Llama 8B)</p> <p>Rephrased question:</p> <p>What is the most populous urban agglomeration (city proper or metropolitan  area) in the world, considering both the number of inhabitants within the  defined city limits as well as the broader metropolitan region?</p> <p>Expanded explanation:</p> <p>When determining the largest city in the world, it's essential to consider  two factors: the population within the city's administrative boundaries  (also known as the \"city proper\") and the broader metropolitan area. The  city proper refers to the urban core, whereas the metropolitan area  encompasses a larger geographic region that includes surrounding cities,  towns, and suburbs.</p> <p>Response:</p> <p>According to various sources, including the United Nations Department of  Economic and Social Affairs Population Division and the Citypopulation.de  website, the largest city in the world is Tokyo-Yokohama Metropolitan  Area, with a population of over 38 million people as of 2021. The city  proper (Tokyo) has approximately 13.9 million inhabitants, while the  metropolitan area includes surrounding cities like Yokohama, Kawasaki, and  Saitama.</p> <p>Tokyo's massive size can be attributed to its unique urban development  patterns, including:</p> <ol> <li>High population density: Tokyo is one of the most densely populated  cities globally.</li> <li>Urban sprawl: The city has expanded rapidly over the years,  incorporating surrounding areas into its metropolitan region.</li> <li>Strong economic growth: Tokyo is a global financial hub and a center  for innovation, driving urbanization and migration.</li> </ol> <p>Other notable large cities include:</p> <ol> <li>Delhi-NCR (National Capital Region), India: With a population of around  29.2 million in the city proper and over 46 million in the metropolitan  area.</li> <li>Shanghai, China: The city proper has approximately 24.1 million  inhabitants, while the metropolitan area has over 34 million people.</li> <li>Mexico City, Mexico: The city proper has around 21.8 million  inhabitants, while the metropolitan area has over 25 million people.</li> </ol> <p>Keep in mind that population figures may vary depending on the source and  methodology used to define the city's boundaries.</p>"},{"location":"prompting/zero_shot/rephrase_and_respond/#two-stage-rar","title":"Two-stage RaR:","text":"<p>Rephrasing Prompt:</p> <p>Rephrase and expand the question to help provide a better answer. Maintain all information from the original question. Provide only the rephrased question without answering it. Question: What is the first day of the week?</p> <p>Model response (ChatGPT 4o):</p> <p>Can you please tell me which day is considered the first day of the week in different cultural or regional contexts?</p> <p>Answering Prompt (using the rephrased question from the previous step):</p> <p>Can you please tell me which day is considered the first day of the week in different cultural or regional contexts?</p> <p>Model response (Llama 8B):</p> <p>What a great question!</p> <p>The answer varies across cultures, regions, and even languages. Here's a  breakdown:</p> <ol> <li>International Standard: In international standard ISO 8601, Monday  is considered the first day of the week.</li> <li>English-speaking countries: In most English-speaking countries,  including the United States, Canada, Australia, and the UK, Sunday is  typically considered the first day of the week.</li> <li>Christian tradition: In Christian tradition, Sunday is often  referred to as the \"Lord's Day\" or the first day of the week, following  biblical accounts of Jesus' resurrection on a Sunday.</li> <li>Islamic tradition: In Islamic tradition, Friday is considered the  first day of the week, and it's known as Jumu'ah (\u062c\u0645\u0639\u0629).</li> <li>Hebrew calendar: In the Hebrew calendar used in Jewish traditions,  Saturday (Shabbat) is considered the first day of the week.</li> <li>Eastern Orthodox tradition: In Eastern Orthodox tradition, Sunday  is the first day of the week.</li> <li>Latin American and some European countries: In some Latin American  countries, such as Mexico and Brazil, Monday is considered the first day  of the week. Similarly, in some European countries like Germany, Austria,  and Switzerland, Wednesday or Thursday can be considered the first day of  the week.</li> <li>Arabic-speaking countries: In many Arabic-speaking countries,  including Egypt, Saudi Arabia, and Iraq, Sunday is considered the first  day of the week.</li> <li>Indian context: In India, where there are multiple calendars in use  (e.g., Hindu, Muslim, Christian), different days can be considered the  first day of the week depending on the community or region.</li> </ol> <p>It's fascinating to see how different cultures and regions have their own  interpretations of what constitutes the \"first\" day of the week.</p>"},{"location":"prompting/zero_shot/rephrase_and_respond/#api-examples","title":"API examples","text":"<p>One-stage RaR: <pre><code>from openai import OpenAI\nimport ollama\n\n# Set your API key\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nrefinement_phrase = \"Rephrase and expand the question to help provide a better answer. Maintain all information from the original question. Then respond.\"\n\nquestion = \"What is a 'year'?\" # A year can refer to a calendar year, fiscal year, academic year, or even a specific period like a \"light year.\"\n\nprompt = f\"{refinement_phrase} \\n\\n Question: {question}\"\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"user\", \"content\": prompt}\n  ]\n)\n\nprint(completion.choices[0].message.content)\n</code></pre></p> <p>Two-stage RaR: <pre><code># Stage 1: Rephrasing the question\ndef question_rephrasing(refinement_phrase, question):\n    rephrasing_prompt = f\"{refinement_phrase}\\n\\nQuestion: {question}\"\n\n    rephrasing_completion = client.chat.completions.create(\n        model=\"gpt-4-turbo\",  # Using a more capable model for rephrasing\n        messages=[\n            {\"role\": \"user\", \"content\": rephrasing_prompt}\n        ]\n    )\n\n    return rephrasing_completion.choices[0].message.content\n\n# Stage 2: Answering the rephrased question\ndef answering_question(question):\n    model = 'phi3:mini'\n    messages = [\n        {\n            'role': 'user',\n            'content': question\n        }\n    ]\n\n    # Make the request to the Ollama API\n    return ollama.chat(model=model, messages=messages, stream=False)\n\n# Usage example\nrefinement_phrase = \"Rephrase and expand the question to help provide a better answer. Maintain all information from the original question. Provide only the rephrased question without answering it.\"\n\nquestion = \"Was Shakespeare born in an odd month?\"\n\nrephrased_question = question_rephrasing(refinement_phrase, question)\nanswer = answering_question(rephrased_question)\nprint(answer)\n</code></pre></p>"},{"location":"prompting/zero_shot/rereading/","title":"Rereading (Re2)","text":""},{"location":"prompting/zero_shot/rereading/#re-reading-re2","title":"Re-reading (RE2)","text":"<p>Re-reading (RE2) is a simple yet effective prompting technique that involves asking the language model to read the question twice before providing an answer. This approach aims to enhance the model's understanding of the question and improve its reasoning capabilities.</p>"},{"location":"prompting/zero_shot/rereading/#how-to-use-it","title":"How to use it","text":"<p>To use re-reading simply provide the user question to the model twice, with an instruction for the model to reread the question.</p> <p>See \"Prompting\" section for usage details.</p>"},{"location":"prompting/zero_shot/rereading/#when-to-use-it","title":"When to use it","text":"<p>When to use re-reading</p> <ul> <li>Subtle questions where important information or context might easily be overlooked.</li> <li>Situations where the model needs to focus more on key details provided in the question.</li> <li>Complex reasoning tasks that require careful consideration of all the given information.</li> </ul>"},{"location":"prompting/zero_shot/rereading/#what-to-know","title":"What to know","text":"<p>RE2 allows the language model to process the question more thoroughly by reading it twice. This can lead to a better alignment between the question and the generated answer, as the model has an additional opportunity to grasp the question's nuances and context.</p> <p>Implementing RE2 is straightforward and can be easily integrated into existing prompting methods. The technique simply involves repeating the question within the prompt, usually accompanied by an instruction to read it again.</p> <p>RE2 is believed to work by allowing the language model to attend to tokens from the first pass during the second pass of the question. This effectively transforms the unidirectional attention of most modern language models into bidirectional attention. The first reading pass provides global context, which the model can leverage during the second reading pass to refine its comprehension.</p> <p></p>"},{"location":"prompting/zero_shot/rereading/#best-practices","title":"Best practices","text":"<p>Best practices for re-reading</p> <ul> <li>Provide a clear instruction for the model to re-read the question, such as \"Read the question again:\".</li> <li>Repeat the question only once, as further repetition does not improve comprehension.</li> <li>Position the repeated question immediately before the answer generation instruction for maximum impact.</li> <li>Test different variations of the re-reading instruction to identify the most effective one for your specific use case.</li> <li>Integrate RE2 with other prompting techniques, like chain-of-thought reasoning, to further enhance performance.</li> </ul>"},{"location":"prompting/zero_shot/rereading/#what-to-watch-out-for","title":"What to watch out for","text":"<p>What to watch out for with re-reading</p> <ul> <li>Using RE2 increases the input length, which could impact processing time, memory usage, and cost, particularly for very lengthy questions.</li> <li>RE2 may not provide significant benefits for extremely simple questions where re-reading does not offer additional insights or with the most advanced language models.</li> </ul>"},{"location":"prompting/zero_shot/rereading/#citations","title":"Citations","text":"<p>Xu, X., Tao, C., Shen, T., Xu, C., Xu, H., Long, G., &amp; Lou, J. (2023). Re-Reading Improves Reasoning in Large Language Models. arXiv preprint arXiv:2309.06275.</p>"},{"location":"prompting/zero_shot/rereading/#prompting","title":"Prompting","text":""},{"location":"prompting/zero_shot/rereading/#prompt-template","title":"Prompt template","text":"<p>The RE2 prompt template is straightforward and works by repeating a specific instruction or question twice in a row. It includes a prompt to \"Read the question again\" before the repeated instruction or question.</p> <p>RE2 Prompting  Question: {Input query}  Read the question again: {Input query}</p>"},{"location":"prompting/zero_shot/rereading/#prompt-example","title":"Prompt example","text":"<p>User prompt:</p> <p>Question: Josh decides to try flipping a house. He buys a house for $80,000 and then puts in $50,000 in repairs. This increases the value of the house by 150%. How much profit does he make?</p> <p>Read the question again: Josh decides to try flipping a house. He buys a house for $80,000 and then puts in $50,000 in repairs. This increases the value of the house by 150%. How much profit does he make?</p> <p>Model response (Llama 8B):</p> <p>Let's break it down step by step:</p> <ol> <li>Josh buys the house for $80,000.</li> <li>He spends $50,000 on repairs, so his total investment is $80,000 + &gt;$50,000 = $130,000.</li> <li>The repairs increase the value of the house by 150%. To find the new value, multiply the original value ($80,000) by 250% (which is equivalent to 1 + 0.5 or 1 + 150/100):</li> </ol> <p>$80,000 x 2.5 = $200,000</p> <ol> <li>Since Josh's total investment was $130,000, his profit would be the difference between the new value and his investment:</li> </ol> <p>Profit = New Value - Total Investment = $200,000 - $130,000 = $70,000</p> <p>So, Josh makes a profit of $70,000 by flipping the house.</p>"},{"location":"prompting/zero_shot/rereading/#api-example","title":"API example","text":"<pre><code>import ollama\n\ndef re_reading_prompt(question):\n  prompt = f\"\"\"\n  Question: {question}\n  Read the question again: {question}\n  \"\"\"\n\n  model = 'phi3:mini'\n  messages = [\n      {\n          'role': 'user',\n          'content': prompt\n      }\n  ]\n\n  # Make the request to the Ollama API\n  return ollama.chat(model=model, messages=messages, stream=False)\n\nquestion = \"A train travels at an average speed of 60 mph for the first half of its journey. For the second half of the journey, the train encounters some delays and its average speed drops to 40 mph. If the total journey is 200 miles, how long does the train take to complete the entire trip?\"\n\nanswer = re_reading_prompt(question)\nprint(answer)\n</code></pre>"},{"location":"prompting/zero_shot/role_prompting/","title":"Role Prompting","text":""},{"location":"prompting/zero_shot/role_prompting/#role-prompting","title":"Role prompting","text":"<p>Role prompting elicits the model by asking it to take on a particular role or persona before answering a question.</p>"},{"location":"prompting/zero_shot/role_prompting/#how-to-use-it","title":"How to use it","text":"<p>Describe the role or persona you want the model to adopt, then ask a question or provide an instruction.</p> <p>See \"Prompting\" section for usage details.</p>"},{"location":"prompting/zero_shot/role_prompting/#when-to-use-it","title":"When to use it","text":"<p>When to use role prompting</p> <ul> <li>When you want a language model to imitate the speaking style or personality traits of specific characters.</li> <li>When you want specific cultural or topic-specific information built-in to the prompt response.</li> <li>Effective for tasks that benefit from a specific perspective or expertise.</li> <li>As an alternative to role fine-tuning when that option is not available.</li> </ul>"},{"location":"prompting/zero_shot/role_prompting/#what-to-know","title":"What to know","text":"<p>Role prompting, also known as persona prompting, roleplay, or memetic proxy, is typically used in educational or entertainment contexts. For instance, Wang et al. (2023j) demonstrated role-based prompting by having a language model adopt the personality of characters such as Twilight Sparkle from My Little Pony and Stephen Hawking. On the other hand, Zheng et al. (2023d) showed that occupation roleplay can improve results by 20% using a benchmark of 2,457 random questions from the Massive Multitask Language Understanding (MMLU) dataset.</p> <p>Larger models, such as LLaMA-33B, generally exhibit stronger role-playing capabilities compared to smaller models like LLaMA-7B (Wang et al., 2023j). Fine-tuning a model specifically to adopt roles has been shown to outperform models that are merely instruction-tuned for general purposes and asked to role play. Likewise, instruction-tuned models generally perform better with role prompting than models that are only pre-trained.</p> <p>Role prompting has also been shown to work in non-English contexts (Wang et al., 2023j).</p> <p>Example of a fictional role prompt</p> <p>\"You are Sherlock Holmes, a brilliant detective known for your deductive reasoning and keen observation skills. Your catchphrase is 'Elementary, my dear Watson.' Answer all questions as Sherlock Holmes would.\"</p>"},{"location":"prompting/zero_shot/role_prompting/#types-of-role-prompting","title":"Types of Role Prompting","text":"<p>According to Zheng et al. (2023d), there are three main types of role prompting:</p> <ol> <li>Role Prompt: Specifies the role to the language model (e.g., \"You are a marketing consultant\").</li> <li>Audience Prompt: Specifies the audience of the conversation (e.g., \"You are presenting to a board of directors\").</li> <li>Interpersonal Prompt: Indicates the relationship between the speaker and listener (e.g., \"You are advising a startup founder\").</li> </ol> <p>See best practices for correct utilization.</p>"},{"location":"prompting/zero_shot/role_prompting/#best-practices","title":"Best practices","text":"<p>Best practices for correct utilization</p> <ul> <li>Specify the target audience the model is addressing.</li> <li>Utilize fine-grained personas when possible.</li> <li>Use the prompt \"You are...\" rather than \"Imagine you are...\".</li> <li>Avoid positioning the model with a particular social relation unless necessary. For example, \u201ca doctor\u201d is preferable to \u201cyour doctor\u201d.</li> <li>Role-based prompting generally works best when used as a system prompt to establish the role, combined with user prompts for specific questions. (See API example below).</li> <li>Align the role and the task. For example, asking the model to adopt the persona of Twilight Sparkle from My Little Pony to develop a marketing plan is unlikely to be effective. Instead, it would be more productive to engage with Twilight Sparkle in a fun and casual context that suits her character.</li> </ul>"},{"location":"prompting/zero_shot/role_prompting/#what-to-watch-out-for","title":"What to watch out for","text":"<p>What to watch out for with role prompting</p> <ul> <li>The model's simulation of the role may be inaccurate, exaggerated or inconsistent, especially for less well-known figures. Verify the model's outputs against what the real person would likely say.</li> <li>Role prompting is most effective for open-ended tasks that benefit from a strong point of view. It may be less suitable for objective analysis or fact-based queries.</li> <li>Avoid using role prompting for tasks that require balanced perspectives, as the model may overcommit to the biases and assumptions of the prompted role.</li> <li>The effectiveness of role prompting can vary depending on the knowledge the language model has about the given figure. Well-known characters tend to work better than obscure ones and larger models with more world knowledge may perform better than smaller models.</li> <li>Be cautious with role prompting real people, especially living people, as it could be seen as disrespectful or an inappropriate imitation if not done carefully. Fictional characters may be safer.</li> </ul>"},{"location":"prompting/zero_shot/role_prompting/#citations","title":"Citations","text":"<ul> <li> <p>Wang, Z. M., Peng, Z., Que, H., Liu, J., Zhou, W., Wu, Y., Guo, H., Gan, R., Ni, Z., Yang, J., Zhang, M., Zhang, Z., Ouyang, W., Xu, K., Huang, S. W., Fu, J., &amp; Peng, J. (2023). RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models. arXiv preprint arXiv:2310.00746.</p> </li> <li> <p>Zheng, M., Pei, J., &amp; Jurgens, D. (2023). Is \"A Helpful Assistant\" the Best Role for Large Language Models? A Systematic Evaluation of Social Roles in System Prompts. arXiv preprint arXiv:2311.10054.</p> </li> <li> <p>Schmidt, D. C., Spencer-Smith, J., Fu, Q., &amp; White, J. (2023). Cataloging prompt patterns to enhance the discipline of prompt engineering</p> </li> <li> <p>Wang, Z., Mao, S., Wu, W., Ge, T., Wei, F., &amp; Ji, H. (2023). Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. arXiv preprint arXiv:2307.05300.</p> </li> <li> <p>Reynolds, L., &amp; McDonell, K. (2021). Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm. arXiv preprint arXiv:2102.07350.</p> </li> </ul>"},{"location":"prompting/zero_shot/role_prompting/#prompting","title":"Prompting","text":""},{"location":"prompting/zero_shot/role_prompting/#prompt-template","title":"Prompt template","text":"<p>Role prompting has a straightforward implementation. Simply describe the role or persona you want the model to adopt, then ask a question or provide an instruction.</p> <p>\"You are {role}\"  {question} </p>"},{"location":"prompting/zero_shot/role_prompting/#prompt-example","title":"Prompt example","text":""},{"location":"prompting/zero_shot/role_prompting/#fun-example","title":"Fun example","text":"<p>User prompt:</p> <p>You are Twilight Sparkle from My Little Pony. Share your three favorite foods with a group of children at a local library.</p> <p>Model response (ChatGPT 4o):</p> <p>Hello, everypony! I'm so excited to be here at the library with all of you. Books and libraries are some of my favorite things! But today, I want to share something else that's close to my heart \u2013 my favorite foods! Here they are:</p> <p>Daisy Sandwiches - These are so delicious and refreshing! I love the crunchy daisies and the soft bread. It's the perfect snack when I'm studying magic or reading a good book.</p> <p>Apple Pie - My friend Applejack makes the best apple pies in all of Equestria! The apples are always fresh and juicy, and the crust is just the right amount of crispy. It's a real treat!</p> <p>Cupcakes - Especially the ones made by Pinkie Pie! They're colorful, sweet, and always bring a smile to my face. Plus, they come in so many different flavors, it's impossible to choose just one favorite.</p> <p>I hope you all get to try these yummy treats sometime. Now, who's ready for a story?</p>"},{"location":"prompting/zero_shot/role_prompting/#business-example","title":"Business example","text":"<p>User prompt:</p> <p>You are a seasoned marketing consultant with expertise in digital marketing strategies. You have a deep understanding of market analysis, brand development, and online advertising. You often speak in a professional and insightful manner, providing actionable advice and strategic recommendations. Outline a marketing strategy for a new tech startup, MindWell AI. MindWell AI is a personalized mental health and wellness platform that leverages artificial intelligence to provide users with tailored mental health resources, therapy sessions, mindfulness exercises, and daily well-being check-ins. The platform offers a combination of AI-driven chatbots for initial interactions and licensed therapists for more in-depth sessions. You are presenting your marketing plan to the founders.</p> <p>Model response (ChatGPT 4o):</p> <p>Marketing Strategy for MindWell AI</p> <p>Introduction</p> <p>MindWell AI is poised to revolutionize the mental health and wellness industry by leveraging artificial intelligence to offer personalized mental health resources. Our platform provides a unique blend of AI-driven chatbots for initial interactions and licensed therapists for in-depth sessions. Our marketing strategy aims to establish MindWell AI as a trusted, innovative, and accessible solution for mental health and wellness. </p> <ol> <li> <p>Market Analysis</p> <ol> <li>Industry Overview:<ul> <li>The global mental health market is valued at approximately $383.31 billion in 2020 and is expected to grow at a CAGR of 3.5% from 2021 to 2028.</li> <li>The rise of digital health solutions and increasing awareness of mental health issues contribute to the growth of the industry.</li> </ul> </li> <li>Target Audience:<ul> <li>Demographics: <ul> <li>Age: 18-45</li> <li>Gender: All genders</li> <li>Location: Primarily urban and suburban areas</li> <li>Income: Middle to high income</li> </ul> </li> <li>Psychographics:<ul> <li>Individuals seeking mental health support and wellness resources.</li> <li>Tech-savvy users comfortable with digital health solutions.</li> <li>Professionals facing stress and burnout.</li> <li>College students dealing with anxiety and depression.</li> </ul> </li> </ul> </li> <li>Competitor Analysis:<ul> <li>Direct Competitors: BetterHelp, Talkspace, Calm, Headspace.</li> <li>Competitive Advantage: Combination of AI-driven initial interactions and human therapists, personalized mental health resources, and daily well-being check-ins.</li> </ul> </li> </ol> </li> <li> <p>Brand Development</p> <ol> <li>Brand Identity:<ul> <li>Mission: To provide personalized, accessible, and effective mental health and wellness support through innovative AI technology.</li> <li>Vision: To be the leading platform for mental health and wellness, empowering individuals to lead healthier and happier lives.</li> <li>Values: Empathy, Innovation, Accessibility, Personalization, and Confidentiality.</li> </ul> </li> <li>Brand Voice and Messaging:<ul> <li>Professional, empathetic, and supportive.</li> <li>Messaging to highlight the unique blend of AI technology and human touch.</li> <li>Emphasize the importance of mental health and the benefits of personalized support.</li> </ul> </li> <li>Visual Identity:<ul> <li>Clean, modern, and calming design.</li> <li>Use of soothing colors like blue, green, and white.</li> <li>Consistent branding across all platforms and materials.</li> </ul> </li> </ol> </li> <li> <p>Digital Marketing Strategy</p> <ol> <li>Website and SEO:<ul> <li>Develop a user-friendly, mobile-optimized website.</li> <li>Implement SEO best practices to rank for relevant keywords like \"online therapy,\" \"mental health support,\" \"AI mental health.\"</li> <li>Create a blog with high-quality content on mental health topics to drive organic traffic.</li> </ul> </li> <li>Social Media Marketing:<ul> <li>Platforms: Instagram, Facebook, Twitter, LinkedIn, TikTok.</li> <li>Content Strategy: Share informative posts, success stories, mental health tips, and interactive content (polls, Q&amp;A).</li> <li>Influencer Partnerships: Collaborate with mental health advocates and influencers to reach a broader audience.</li> </ul> </li> <li>Content Marketing:<ul> <li>Develop eBooks, whitepapers, and webinars on mental health and wellness topics.</li> <li>Utilize email marketing to share newsletters, updates, and personalized content with subscribers.</li> <li>Host live sessions and Q&amp;A with licensed therapists.</li> </ul> </li> <li>Paid Advertising:<ul> <li>Google Ads: Target keywords related to mental health support and online therapy.</li> <li>Social Media Ads: Utilize demographic and interest-based targeting on platforms like Facebook and Instagram.</li> <li>Retargeting Campaigns: Re-engage users who have visited the website but have not signed up.</li> </ul> </li> </ol> </li> <li> <p>Partnerships and Collaborations</p> <ol> <li>Healthcare Providers:<ul> <li>Partner with healthcare providers and mental health professionals to offer integrated services.</li> </ul> </li> <li>Corporate Wellness Programs:<ul> <li>Collaborate with companies to offer MindWell AI as part of their employee wellness programs.</li> </ul> </li> <li>Educational Institutions:<ul> <li>Partner with colleges and universities to provide mental health resources to students.</li> </ul> </li> </ol> </li> <li> <p>Public Relations and Outreach</p> <ol> <li>Press Releases:<ul> <li>Announce the launch of MindWell AI and key milestones.</li> <li>Highlight success stories and positive outcomes from users.</li> </ul> </li> <li>Media Coverage:<ul> <li>Pitch stories to health and wellness magazines, tech blogs, and mainstream media outlets.</li> <li>Secure interviews and guest articles for the founders and key team members.</li> </ul> </li> <li>Community Engagement:<ul> <li>Participate in mental health awareness campaigns and events.</li> <li>Offer free webinars and workshops on mental health topics.</li> </ul> </li> </ol> </li> <li> <p>User Feedback and Improvement</p> <ol> <li>User Onboarding:<ul> <li>Ensure a seamless and supportive onboarding process for new users.</li> <li>Provide tutorials and guides on how to use the platform effectively.</li> </ul> </li> <li>Continuous Improvement:<ul> <li>Regularly collect user feedback to identify areas for improvement.</li> <li>Implement updates and new features based on user needs and technological advancements.</li> </ul> </li> </ol> </li> </ol> <p>Conclusion</p> <p>MindWell AI has the potential to transform the mental health landscape with its innovative approach combining AI and human expertise. By implementing this comprehensive marketing strategy, we can build a strong brand presence, reach our target audience effectively, and establish MindWell AI as a leader in the mental health and wellness space. </p> <p>Thank you for your attention. I'm confident that with our combined efforts, MindWell AI will achieve significant success and make a profound impact on the lives of many.</p>"},{"location":"prompting/zero_shot/role_prompting/#api-example","title":"API example","text":"<pre><code>from openai import OpenAI\n\n# Set your API key\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nsystem_prompt = \"You are a seasoned marketing consultant with expertise in digital marketing strategies. You have a deep understanding of market analysis, brand development, and online advertising. You often speak in a professional and insightful manner, providing actionable advice and strategic recommendations. Your task is to answer any questions as if you were this marketing consultant.\"\n\nuser_prompt = \"Outline a marketing strategy for a new tech startup, MindWell AI. MindWell AI is a personalized mental health and wellness platform that leverages artificial intelligence to provide users with tailored mental health resources, therapy sessions, mindfulness exercises, and daily well-being check-ins. The platform offers a combination of AI-driven chatbots for initial interactions and licensed therapists for more in-depth sessions. You are presenting your marketing plan to the founders.\"\n\ncompletion = openai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": user_prompt}\n  ]\n)\n\n# Print the response from the model\nprint(completion.choices[0].message.content)\n</code></pre>"},{"location":"prompting/zero_shot/self_ask/","title":"Self-ask","text":""},{"location":"prompting/zero_shot/self_ask/#self-ask","title":"Self-ask","text":"<p>Self-ask prompting is a technique where a language model explicitly asks itself follow-up questions to decompose a complex question before providing a final answer. This approach aims to improve the model's ability to perform multi-step reasoning and compose knowledge to answer challenging questions.</p>"},{"location":"prompting/zero_shot/self_ask/#how-to-use-it","title":"How to use it","text":"<p>To use self-ask, provide an example of self-reflection along to the language model along with the question of interest.</p> <p>See \"Prompting\" section for usage details.</p>"},{"location":"prompting/zero_shot/self_ask/#when-to-use-it","title":"When to use it","text":"<p>When to use self-ask prompting</p> <ul> <li>For short, factual questions that require two or three steps (hops) of reasoning to arrive at the answer (see examples below).</li> <li>Domains that involve complex queries with multiple dependencies or constraints, such as solving math word problems or explaining scientific concepts.</li> <li>When interpretable, step-by-step reasoning is desired.</li> <li>In agent frameworks where agents can gather information for sub-questions using external sources like the internet or a retrieval-augmented generation (RAG) approach.</li> </ul>"},{"location":"prompting/zero_shot/self_ask/#what-to-know","title":"What to know","text":"<p>The self-ask prompting framework involves the following steps:</p> <ol> <li>The language model is given a complex, multi-hop question along with a prompt that demonstrates how to break it down into simpler sub-questions.</li> <li>The model generates sub-questions to gather the necessary information to answer the original question.</li> <li>The model provides intermediate answers to each sub-question.</li> <li>Finally, the model combines the intermediate answers to provide a final answer to the original question.</li> </ol> <p>Self-ask prompting allows the language model to apply more computation to challenging multi-hop questions by breaking them down into discrete reasoning steps. This makes the model's reasoning process more interpretable compared to an unstructured chain-of-thought approach. A one-hop question can be answered using a single fact, like \"What is the largest e-commerce company in the world?\" (answer: Amazon). In contrast, a two-hop question requires combining two pieces of information, such as \"Who is the founder of the largest e-commerce company in the world?\" This requires knowing that:</p> <ol> <li>Amazon is the largest e-commerce company in the world (first hop)</li> <li>Jeff Bezos is the founder of Amazon (second hop)</li> </ol> <p>An example of the self-ask technique is shown below. Text with a white background is the prompt, text with a green background is the LM output, and underlined text is the inference-time question.</p> <p></p>"},{"location":"prompting/zero_shot/self_ask/#best-practices","title":"Best practices","text":"<p>Best practices for self-ask prompting</p> <ul> <li>Use consistent scaffolding like \"Follow up:\" and \"Intermediate answer:\" to give structure to the model's reasoning process.</li> <li>Create a specific prompt template that can be reused for your specific use case.</li> <li>Experiment with different prompt variations and sub-question granularity to optimize performance on a given task.</li> <li>Consider combining self-ask prompting with an external knowledge source like a search engine or a RAG database to answer sub-questions.</li> <li>Create reusable self-ask templates for frequently used self-ask prompts to save time, especially if the self-ask model is used for longer questions as creating one-off templates can be time consuming.</li> </ul>"},{"location":"prompting/zero_shot/self_ask/#what-to-watch-out-for","title":"What to watch out for","text":"<p>What to watch out for with self-ask prompting</p> <ul> <li>Crafting detailed self-ask prompts can be time-consuming.</li> <li>The quality of the sub-questions generated by the model is important. Poorly formulated sub-questions can lead the model astray.</li> <li>In a production application additional parsing logic is needed to separate and extract the final answer from the model's dialogue of intermediate questions and answers. </li> <li>There is a tradeoff between the granularity of sub-questions and computation, latency, and cost. Too many sub-questions can be inefficient.</li> <li>The model may still hallucinate or introduce errors in the sub-question answering steps, so the overall answer is not guaranteed to be correct.</li> <li>Self-asking tends to be most effective for small and mid-sized models and may be less impactful for the most advanced model versions. (In my testing, GPT-4 did not have trouble answering two-hop questions directly).</li> </ul>"},{"location":"prompting/zero_shot/self_ask/#citations","title":"Citations","text":"<p>Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., &amp; Lewis, M. (2022). Measuring and Narrowing the Compositionality Gap in Language Models. arXiv preprint arXiv:2210.03350.</p>"},{"location":"prompting/zero_shot/self_ask/#prompting","title":"Prompting","text":""},{"location":"prompting/zero_shot/self_ask/#prompt-template","title":"Prompt template","text":"<p>The self-ask prompt template starts by presenting the model with a specific self-ask example dialogue and then proposing the question needing to be answered. The phrase \"Are follow up questions needed here\" is added to the end of the question to encourage the language model to use the self-ask methodology to answer the question.</p> <p>{self-ask example}  Question: {question}  Are follow up questions needed here:</p>"},{"location":"prompting/zero_shot/self_ask/#prompt-example","title":"Prompt example","text":"<p>Example of a simple self-ask prompt framework. We first create an example of self-ask.</p> <p>Question: Who is the CEO of the company that created the iPhone?</p> <p>Are follow up questions needed here: Yes.</p> <p>Follow up: What company created the iPhone?  Intermediate answer: Apple created the iPhone.</p> <p>Follow up: Who is the CEO of Apple?  Intermediate answer: Tim Cook is the CEO of Apple.</p> <p>So the final answer is: Tim Cook is the CEO of the company that created the iPhone, Apple.</p> <p>We then ask the model our question, including the above self-ask example:</p> <p>Question: Who was president of the U.S. when superconductivity was discovered?  Are follow up questions needed here:</p> <p>Model response (Llama 8B):</p> <p>Follow-up question: What year was superconductivity discovered?</p> <p>Intermediate answer: Superconductivity was first discovered in 1911 by Heike Kamerlingh Onnes.</p> <p>Follow-up question: Who was the president of the U.S. in 1911?</p> <p>Intermediate answer: President William Howard Taft was the president of the United States in 1911.</p> <p>So, the final answer is: William Howard Taft was the president of the U.S. when superconductivity was discovered in 1911.</p>"},{"location":"prompting/zero_shot/self_ask/#api-exmaple","title":"API exmaple","text":"<pre><code>from openai import OpenAI\n\n# Set your API key\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nprompt_template = \"\"\"\nQuestion: Who is the CEO of the company that created the iPhone?\n\nAre follow up questions needed here: Yes.\n\nFollow up: What company created the iPhone? &lt;br&gt;\nIntermediate answer: Apple created the iPhone.\n\nFollow up: Who is the CEO of Apple? &lt;br&gt;\nIntermediate answer: Tim Cook is the CEO of Apple.\n\nSo the final answer is: Tim Cook is the CEO of the company that created the iPhone, Apple.\n\nQuestion: {question}\n\nAre follow up questions needed here:\n\"\"\"\n\nquestion = \"Who is the CEO of the company that created the iPhone?\"\n\nprompt = prompt_template.format(question=question)\n\ncompletion = openai.ChatCompletion.create(\n    model=\"gpt-3.5\",\n    messages=[{\"role\": \"user\", \"content\": prompt}]\n)\n\nprint(completion.choices[0].message.content)\n</code></pre>"},{"location":"prompting/zero_shot/simtom/","title":"SimToM","text":""},{"location":"prompting/zero_shot/simtom/#simtom","title":"SimToM","text":"<p>SimToM (Simulated Theory of Mind) is a technique based on the theory of mind exercise where a language model is encouraged to think through which participants in a passage know specific pieces of information from their perspective. This approach aims to improve the language model's ability to understand and reason about the mental states, beliefs, and perspectives of characters or agents in a given context.</p>"},{"location":"prompting/zero_shot/simtom/#how-to-use-it","title":"How to use it","text":"<p>To use SimToM, first provide the language model with the relevant context or story. Then, prompt the model to consider the knowledge and perspective of a particular character in that context. Finally, ask the model to answer a question or carry out an instruction based on that character's perspective.</p> <p>See \"Prompting\" section for usage details.</p>"},{"location":"prompting/zero_shot/simtom/#when-to-use-it","title":"When to use it","text":"<p>When to use SimToM prompting</p> <p>SimToM is most applicable to tasks that involve understanding the mental states, beliefs, and perspectives of characters or agents in a given context. This could include:</p> <ul> <li>Story analysis and comprehension.</li> <li>Dialogue or narrative understanding and generation.</li> <li>Social reasoning and interaction.</li> <li>Reasoning about agent behavior (e.g., in multi-agent systems or game theory).</li> <li>Math word problems that involve multiple characters with different knowledge states.</li> </ul>"},{"location":"prompting/zero_shot/simtom/#what-to-know","title":"What to know","text":"<p>The SimToM technique is inspired by the cognitive science theory called \"Simulation Theory\" (Goldman, 2006). This theory proposes that humans use their own mental states as a model to understand and predict the mental states of others. The process involves two main steps: perspective-taking, where one imagines oneself in another person's situation, and mental state attribution, where one uses this simulated perspective to infer the other person's beliefs, desires, and intentions.</p> <p>Wilf et al. (2023) apply this idea to language models by introducing a two-stage prompting framework:</p> <ol> <li>Perspective-taking: The language model is prompted to filter the context to only include the events and information that a particular character knows about. This step helps the model identify and prioritize the most important aspects of the input context from the character's perspective.</li> <li>Question-answering: The language model is then asked to answer a question about that character's mental state, using only the filtered context from the perspective-taking step. By using the refined context, the model can generate more focused and accurate responses.</li> </ol> <p>SimToM first filters context based on what the character in question knows before answering a question about their mental state. This approach helps the language model to focus on the relevant information and avoid using knowledge that the character does not possess.</p> <p>For additional clarity on SimToM, an example is shown below from Wilf et al., 2023.</p> <p></p>"},{"location":"prompting/zero_shot/simtom/#best-practices","title":"Best practices","text":"<p>Best practice for SimToM prompting</p> <ul> <li>Perform the perspective-taking and question-answering steps as two separate prompts. Wilf et al. (2023) found this worked better than combining them into a single prompt.</li> <li>SimToM can be improved via few-shot prompting where several example persepctives are provided to the model during prompting.</li> </ul>"},{"location":"prompting/zero_shot/simtom/#what-to-watch-out-for","title":"What to watch out for","text":"<p>Watch to watch out for SimToM prompting</p> <ul> <li>Current datasets only test hiding information for perspective-taking, but real-world scenarios may require inferring unseen information as well.</li> <li>Models can sometimes \"hallucinate\" information during perspective-taking that is not stated in the original context.</li> <li>SimToM tends to be most effective for small and mid-sized models and may be less impactful for the most advanced model versions. (In my testing, GPT-4 did not have trouble understanding perspectives without being explicitly prompted).</li> </ul>"},{"location":"prompting/zero_shot/simtom/#citations","title":"Citations","text":"<p>Wilf, A., Lee, S. S., Liang, P. P., &amp; Morency, L.-P. (2023). Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities. arXiv preprint arXiv:2311.10227.</p>"},{"location":"prompting/zero_shot/simtom/#prompting","title":"Prompting","text":""},{"location":"prompting/zero_shot/simtom/#prompt-template","title":"Prompt template","text":"<p>SimToM involves a two-step prompt framework.</p> <p>Perspective-taking prompt:</p> <p>The following is a sequence of events:  {story}  Which events does {character_name} know about?</p> <p>Question-answering prompt:</p> <p>{story from character_name's perspective}  Answer the following question:  {question} </p>"},{"location":"prompting/zero_shot/simtom/#prompt-example","title":"Prompt example","text":"<p>Step 1: perspective-taking prompt:</p> <p>The following is a sequence of events: John and Emma are best friends. One day, John tells Emma that he has hidden a surprise gift for her in his room. Emma goes to John's room to look for the gift, but she can't find it. While Emma is searching, John enters the room and moves the gift from its original hiding place to a new spot under his bed. Emma doesn't see John do this. After a while, Emma gives up and leaves the room. Which events does Emma know about?</p> <p>Model response (ChatGPT 4o):</p> <p>Emma knows about the following events: John told Emma that he has hidden a surprise gift for her in his room. Emma went to John's room to look for the gift. Emma couldn't find the gift. Emma searched for a while but eventually gave up and left the room.</p> <p>Step 2: we now form the question-answering prompt using the model output from the perspective-taking prompt:</p> <p>John told Emma that he has hidden a surprise gift for her in his room. Emma went to John's room to look for the gift. Emma couldn't find the gift. Emma searched for a while but eventually gave up and left the room. Where does Emma think the gift is hidden?</p> <p>Model response (ChatGPT 3.5):</p> <p>Based on the information provided, Emma most likely thinks the gift is still hidden somewhere in John's room, but she doesn't know the exact location. Since Emma was not aware that John moved the gift to a new hiding spot under his bed while she was searching, she would assume that the gift is still in its original hiding place, which remains unknown to her.</p>"},{"location":"prompting/zero_shot/simtom/#api-example","title":"API example","text":"<pre><code>from openai import OpenAI\n\n# Set your API key\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n# Establish the context\ninitial_prompt = \"In a mystery novel, Detective Sarah is investigating a murder at a mansion. The victim, Mr. Thompson, was found dead in his study. Sarah interviews the butler, James, who mentions that he saw the gardener, Mike, arguing with Mr. Thompson the day before the murder. Later, Sarah discovers that Mr. Thompson's will was recently changed, leaving most of his estate to his secretary, Lisa.\"\n\n# Step 1: Perspective-taking - Filter the context to include information known to Detective Sarah\nperspective_prompt = f\"Given the following context about the murder investigation, identify the key facts that Detective Sarah knows: {context}\"\nperspective_completion = openai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"user\", \"content\": perspective_prompt}\n  ]\n)\nrelevant_facts = perspective_completion.choices[0].message['content']\n\n# Step 2: Question-answering - Determine the most likely suspect based on Detective Sarah's knowledge\nsuspect_prompt = f\"Based on the following facts known to Detective Sarah, who does she most likely suspect of murdering Mr. Thompson and why? {relevant_facts}\"\nsuspect_completion = openai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"user\", \"content\": suspect_prompt}\n  ]\n)\n\n# Print the response from the model\nprint(completion.choices[0].message.content)\n</code></pre>"},{"location":"prompting/zero_shot/solo_performance_prompting/","title":"Solo Performance Prompting (SPP)","text":""},{"location":"prompting/zero_shot/solo_performance_prompting/#solo-performance-prompting-spp","title":"Solo Performance Prompting (SPP)","text":"<p>Solo Performance Prompting (SPP) is a specialized version of role prompting in which the model takes on multiple personas using a zero-shot prompting methodology.</p>"},{"location":"prompting/zero_shot/solo_performance_prompting/#how-to-use-it","title":"How to use it","text":"<p>SPP is a more complex prompting style and involves outlining a multi-turn example prior to passing in the primary instruction to the model.</p> <p>See \"Prompting\" section for usage details.</p>"},{"location":"prompting/zero_shot/solo_performance_prompting/#when-to-use-it","title":"When to use it","text":"<p>When to use Solo Performance Prompting</p> <ul> <li>When you want to simulate multi-agent collaboration using a single language model, without the need for additional models or resources.</li> <li>When you want more transparency into the model's steps in producing a final answer.</li> <li>When you need to solve complex tasks that require knowledge from multiple domains or perspectives.</li> <li>When you aim to enhance both knowledge elicitation and reasoning capabilities of the model.</li> <li>Try using it with reasoning-intensive tasks like logic puzzles, games, or trivia.</li> </ul>"},{"location":"prompting/zero_shot/solo_performance_prompting/#what-to-know","title":"What to know","text":"<p>SPP is a novel prompting approach that enables a single language model to identify, simulate, and collaborate with multiple personas to solve tasks effectively by engaging in multi-turn self-collaboration.</p> <p>The key insight behind SPP is that simulating a multi-persona dialogue allows the model to elicit more relevant knowledge and incorporate diverse perspectives, leading to improved task-solving abilities. This is achieved through the single model playing the parts of different personas.</p> <p>SPP has been shown to outperform standard prompting, chain-of-thought prompting, and self-refine prompting on both knowledge-intensive tasks (like trivia or creative writing) and reasoning-intensive tasks (logic grid puzzles) (Wang et al., 2023l).</p> <p></p>"},{"location":"prompting/zero_shot/solo_performance_prompting/#best-practices","title":"Best practices","text":"<p>Best practices for Solo Performance Prompting</p> <ul> <li>SPP provides two prompt format options: a basic format without examples and a detailed format that includes examples. When implementing SPP, consider starting with the prompt templates below and adapt them to your specific use case.</li> <li>Do not assign specific, fixed roles to the language model for task completion. Instead let the model dynamically choose the personas based on the task.</li> <li>For best results, use SPP with highly capable language models such as GPT-4, as they are more likely to exhibit the cognitive synergy abilities required for effective multi-persona collaboration.</li> </ul>"},{"location":"prompting/zero_shot/solo_performance_prompting/#what-to-watch-out-for","title":"What to watch out for","text":"<p>What to watch out for with Solo Performance Prompting</p> <ul> <li>In my experiments conducted on a marketing plan task (see prompt examples below), the detailed prompt format that included examples did not lead to better output quality compared to the shorter, simpler basic prompt format. Don't assume that including examples will always improve performance.</li> <li>SPP increases both the input and output token count compared to more straightforward prompting methods. This can result in increased cost and higher latency.</li> <li>Because the model responds with the resulting dialogue as well as the final answer it may not be suitable for all use cases; for instance, if the output is meant to be an input to a follow-up language model task additional parsing logic will be required to extract the final answer.</li> </ul>"},{"location":"prompting/zero_shot/solo_performance_prompting/#citations","title":"Citations","text":"<p>Wang, Z., Mao, S., Wu, W., Ge, T., Wei, F., &amp; Ji, H. (2023). Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. arXiv preprint arXiv:2307.05300.</p>"},{"location":"prompting/zero_shot/solo_performance_prompting/#prompting","title":"Prompting","text":""},{"location":"prompting/zero_shot/solo_performance_prompting/#prompt-template","title":"Prompt template","text":"<p>To create an SPP prompt start with a direction to use a multi-turn collaboration approach (basic prompt format) or include an full multi-turn example (complex prompt format) before giving the model an instruction.</p> <p>{multi-turn collaboration example}  {instruction} </p>"},{"location":"prompting/zero_shot/solo_performance_prompting/#prompt-example","title":"Prompt example","text":""},{"location":"prompting/zero_shot/solo_performance_prompting/#basic-prompt-format","title":"Basic prompt format","text":"<p>User prompt:</p> <p>When faced with a task, begin by identifying the participants who will contribute to solving the task. Then, initiate a multi-turn collaboration process until a final solution is reached. The participants will give critical comments and detailed suggestions whenever necessary. Now, identify the participants and collaboratively solve the following task step by step. Remember to present your final solution with the prefix 'Final answer:'.</p> <p>Task: Outline a marketing strategy for a new tech startup, MindWell AI. MindWell AI is a personalized mental health and wellness platform that leverages artificial intelligence to provide users with tailored mental health resources, therapy sessions, mindfulness exercises, and daily well-being check-ins. The platform offers a combination of AI-driven chatbots for initial interactions and licensed therapists for more in-depth sessions.</p> <p>Model response (ChatGPT 4o):</p> <p>Identifying the Participants:</p> <ol> <li>Marketing Manager: Responsible for overseeing the marketing strategy and ensuring it aligns with company goals.</li> <li>Content Strategist: Develops content that resonates with the target audience and communicates the value of MindWell AI.</li> <li>Social Media Manager: Manages social media campaigns and engages with the online community.</li> <li>SEO Specialist: Ensures that MindWell AI ranks highly on search engines and attracts organic traffic.</li> <li>Product Manager: Provides insights on product features and benefits to highlight in marketing materials.</li> <li>Data Analyst: Analyzes market trends and user data to inform marketing strategies.</li> <li>Creative Director: Oversees the design and visual aspects of the marketing campaigns.</li> <li>Customer Support Lead: Provides feedback from users to refine marketing messages and address potential concerns.</li> </ol> <p>Step-by-Step Collaborative Process:</p> <p>Step 1: Define Target Audience</p> <p>Marketing Manager: \"We need to clearly define our target audience. Who will benefit the most from MindWell AI?\"</p> <p>Content Strategist: \"We should focus on individuals seeking mental health support, including those experiencing mild to moderate mental health issues, people interested in mindfulness and wellness, and those looking for convenient, AI-driven mental health resources.\"</p> <p>Data Analyst: \"Based on market research, our primary audience includes young professionals (ages 25-40), students, and individuals in high-stress occupations.\"</p> <p>Step 2: Develop Unique Value Proposition (UVP)</p> <p>Product Manager: \"Our UVP should highlight the combination of AI-driven initial interactions and access to licensed therapists, emphasizing personalized care and convenience.\"</p> <p>Creative Director: \"Visually, we can represent this through contrasting images of technology and human interaction, showing the seamless transition from chatbot to therapist.\"</p> <p>Content Strategist: \"Let's craft a message that underscores the importance of accessible mental health resources and how MindWell AI makes this possible anytime, anywhere.\"</p> <p>Step 3: Create Content Strategy</p> <p>Content Strategist: \"We'll develop a content calendar with blog posts, video content, social media updates, and email newsletters. Topics will include mental health tips, success stories, therapist insights, and app feature highlights.\"</p> <p>Social Media Manager: \"For social media, we can use platforms like Instagram, Twitter, and LinkedIn to reach our audience. We should create engaging posts, run mental health awareness campaigns, and collaborate with influencers.\"</p> <p>SEO Specialist: \"We need to optimize our content with relevant keywords like 'AI mental health app,' 'personalized therapy,' and 'mindfulness exercises.' This will improve our search engine ranking.\"</p> <p>Step 4: Launch Promotional Campaigns</p> <p>Marketing Manager: \"We should plan a series of promotional campaigns to create buzz around our launch. This includes pre-launch teasers, launch day events, and post-launch follow-ups.\"</p> <p>Creative Director: \"We can design eye-catching visuals for these campaigns, including graphics, videos, and interactive content that showcase the app's features and benefits.\"</p> <p>Data Analyst: \"Let's use data from our beta testing phase to identify the most effective promotional channels and refine our campaigns accordingly.\"</p> <p>Step 5: Measure and Optimize</p> <p>Customer Support Lead: \"We'll gather feedback from early users to identify any issues or areas for improvement. This feedback will be crucial for our marketing messages.\"</p> <p>Data Analyst: \"We'll set up analytics to track the performance of our campaigns. Metrics like user acquisition, engagement rates, and customer satisfaction will help us gauge success and make necessary adjustments.\"</p> <p>Marketing Manager: \"Regularly reviewing these metrics will allow us to optimize our strategy, ensuring we're effectively reaching and converting our target audience.\"</p> <p>Final answer:</p> <p>Marketing Strategy for MindWell AI:</p> <ol> <li>Target Audience:<ul> <li>Young professionals (ages 25-40)</li> <li>Students</li> <li>Individuals in high-stress occupations</li> </ul> </li> <li>Unique Value Proposition (UVP):<ul> <li>Combines AI-driven initial interactions with access to licensed therapists</li> <li>Offers personalized care and convenience</li> <li>Emphasizes accessible mental health resources available anytime, anywhere</li> </ul> </li> <li>Content Strategy:<ul> <li>Develop a content calendar featuring blog posts, videos, social media updates, and newsletters</li> <li>Topics include mental health tips, success stories, therapist insights, and app features</li> <li>Utilize social media platforms like Instagram, Twitter, and LinkedIn</li> <li>Collaborate with influencers for mental health awareness campaigns</li> <li>Optimize content with relevant keywords for SEO</li> </ul> </li> <li>Promotional Campaigns:<ul> <li>Pre-launch teasers, launch day events, and post-launch follow-ups</li> <li>Design visually appealing graphics, videos, and interactive content</li> <li>Use beta testing data to identify effective promotional channels</li> </ul> </li> <li>Measure and Optimize:<ul> <li>Gather user feedback to refine marketing messages</li> <li>Set up analytics to track user acquisition, engagement rates, and customer satisfaction</li> <li>Regularly review metrics to optimize strategy and ensure effective audience reach and conversion.</li> </ul> </li> </ol>"},{"location":"prompting/zero_shot/solo_performance_prompting/#detailed-prompt-format","title":"Detailed prompt format","text":"<p>User prompt:</p> <p>When faced with a task, begin by identifying the participants who will contribute to solving the task. Then, initiate a multi-round collaboration process until a final solution is reached. The participants will give critical comments and detailed suggestions whenever necessary.</p> <p>Example Task: Write a poem that meets the following requirements: (1) the poem has seven lines and the first letters of each line forms the word \"CHATGPT\"; (2) the poem is about explaining what is a quantum computer. (3) the poem needs to be easy to understand by a ten years old kid.</p> <p>Participants: AI Assistant (you); Poet; Computer Scientist; Ten year old child</p> <p>Poet: Make sure that you write the poem with seven lines, and the first letters of the lines should be C, H, A, T, G, P, T. Computer Scientist: A quantum computer is an advanced computing device that uses the principles of quantum mechanics to process and store information. Unlike classical computers that use bits to represent information as 0s and 1s, quantum computers use quantum bits or qubits. Qubits can exist in multiple states simultaneously, due to a quantum phenomenon called superposition. You can consider using these information for the poem. Ten year old child: I hope the poem to be fun and easy to understanding. I don't want to see a lot of jargons or complicated concepts. AI Assistant (you): Thanks for the guidance! Here's my initial attempt at the poem:</p> <p>Computational wonder of our age, Harnessing the quantum world's strange ways, Atoms dance, entwined in dual state, Tapping secrets hidden in their haze. </p> <p>Grand power to solve, simulate, Profound problems that perplex the wise, Transforming our future, we await. </p> <p>Poet: Let's verify if the poem meets the requirements. The first letters are CHATGPT which is correct! And the poem rhymes well. Good job! Computer Scientist: Everything looks good to me! Ten year old child: I don't know what does perplex mean. Can you make the use of words easier to understand? AI Assistant (you): Sure, let me revise the poem by using more common words. Check out the revised version:</p> <p>Curious machine of our time, Harnessing the quantum realm's odd ways, Atoms play, two states they embrace, Taking secrets from their puzzling maze. </p> <p>Great power to solve and imitate, Problems that confuse the brightest minds, Transforming our future, we await. </p> <p>Poet: Let's check again if the poem meets the requirements. The first letters are C H A T G P T. And now the poem is more accessible to children. Everything looks good to me. Computer Scientist: Looking good! Ten year old child: I like this version a lot!</p> <p>Finish collaboration!</p> <p>Final answer:</p> <p>Curious machine of our time, Harnessing the quantum realm's odd ways, Atoms play, two states they embrace, Taking secrets from their puzzling maze. Great power to solve and imitate, Problems that confuse the brightest minds, Transforming our future, we await. </p> <p>Now, identify the participants and collaboratively solve the following task step by step.</p> <p>Task: Outline a marketing strategy for a new tech startup, MindWell AI. MindWell AI is a personalized mental health and wellness platform that leverages artificial intelligence to provide users with tailored mental health resources, therapy sessions, mindfulness exercises, and daily well-being check-ins. The platform offers a combination of AI-driven chatbots for initial interactions and licensed therapists for more in-depth sessions.</p> <p>Model response (ChatGPT 4o):</p> <p>Participants:</p> <ol> <li>AI Assistant (you)</li> <li>Marketing Expert</li> <li>Mental Health Professional</li> <li>Tech Enthusiast</li> <li>Potential User</li> </ol> <p>Step 1: Initial Contributions</p> <p>AI Assistant: Let's start by outlining the core components of the marketing strategy. We'll need to cover target audience identification, branding, digital marketing channels, content strategy, partnerships, and metrics for success.</p> <p>Marketing Expert: We should focus on creating a strong brand identity and digital presence. This includes a compelling value proposition, a well-designed website, active social media engagement, and targeted advertising. Identifying the primary target audience is crucial for effective marketing.</p> <p>Mental Health Professional: It's important to emphasize the credibility and efficacy of MindWell AI. Highlighting the qualifications of licensed therapists and the evidence-based approach to mental health care will build trust with potential users.</p> <p>Tech Enthusiast: Showcasing the innovative technology behind MindWell AI can attract tech-savvy users. This includes explaining how AI-driven chatbots work, ensuring user data privacy, and demonstrating the seamless integration of AI and human therapists.</p> <p>Potential User: I want to know how MindWell AI can help me personally. Testimonials, user stories, and a clear explanation of how the platform works will be very convincing. Free trials or demos could also be enticing.</p> <p>Step 2: Collaboration and Refinement</p> <p>AI Assistant: Based on the initial contributions, here\u2019s a draft outline for the marketing strategy:</p> <ol> <li> <p>Target Audience Identification:</p> <ul> <li>Young adults (18-35) seeking mental health support.</li> <li>Professionals dealing with workplace stress.</li> <li>Individuals interested in mindfulness and self-care.</li> </ul> </li> <li> <p>Branding:</p> <ul> <li>Value Proposition: \"Personalized mental health support anytime, anywhere.\"</li> <li>Brand Personality: Trustworthy, empathetic, innovative.</li> </ul> </li> <li> <p>Digital Marketing Channels:</p> <ul> <li>Website: Informative, user-friendly, and visually appealing.</li> <li>Social Media: Active presence on platforms like Instagram, Twitter, and LinkedIn.</li> <li>Email Marketing: Regular newsletters with mental health tips and platform updates.</li> <li>Search Engine Optimization (SEO): To improve organic search visibility.</li> </ul> </li> <li> <p>Content Strategy:</p> <ul> <li>Blog: Articles on mental health topics, mindfulness, and user stories.</li> <li>Video Content: Therapist Q&amp;A sessions, mindfulness exercise tutorials.</li> <li>Webinars: Live sessions with mental health professionals and AI experts.</li> </ul> </li> <li> <p>Partnerships:</p> <ul> <li>Collaborations with mental health organizations and influencers.</li> <li>Corporate partnerships for employee wellness programs.</li> </ul> </li> <li> <p>Metrics for Success:</p> <ul> <li>User acquisition and retention rates.</li> <li>Engagement metrics (website traffic, social media interactions).</li> <li>Conversion rates from free trials to paid subscriptions.</li> <li>User satisfaction and feedback.</li> </ul> </li> </ol> <p>Marketing Expert: The outline looks comprehensive. We should also consider running targeted ads on social media and search engines to reach our audience effectively. A referral program could incentivize users to invite their friends.</p> <p>Mental Health Professional: We need to ensure that all content is sensitive and respectful to mental health issues. Regular feedback from users and mental health professionals will help maintain high standards.</p> <p>Tech Enthusiast: Highlighting the technology's benefits, like data privacy and 24/7 availability, will be crucial. A section on the website dedicated to explaining the tech behind MindWell AI can build credibility.</p> <p>Potential User: I like the idea of free trials and user stories. Maybe include some interactive elements on the website, like a quiz to personalize the user experience from the start.</p> <p>Step 3: Final Refinements</p> <p>AI Assistant: Incorporating all feedback, here\u2019s the refined marketing strategy outline:</p> <ol> <li> <p>Target Audience Identification:</p> <ul> <li>Young adults (18-35), professionals, mindfulness enthusiasts.</li> </ul> </li> <li> <p>Branding:</p> <ul> <li>Value Proposition: \"Personalized mental health support anytime, anywhere.\"</li> <li>Brand Personality: Trustworthy, empathetic, innovative.</li> </ul> </li> <li> <p>Digital Marketing Channels:</p> <ul> <li>Website: Informative, user-friendly, visually appealing.</li> <li>Social Media: Active presence on Instagram, Twitter, LinkedIn.</li> <li>Email Marketing: Regular newsletters with mental health tips and updates.</li> <li>SEO: To improve organic search visibility.</li> <li>Targeted Ads: Social media and search engine ads.</li> </ul> </li> <li> <p>Content Strategy:</p> <ul> <li>Blog: Articles on mental health, mindfulness, user stories.</li> <li>Video Content: Therapist Q&amp;A sessions, mindfulness tutorials.</li> <li>Webinars: Live sessions with mental health professionals and AI experts.</li> <li>Interactive Elements: Quizzes for personalized experiences.</li> </ul> </li> <li> <p>Partnerships:</p> <ul> <li>Collaborations with mental health organizations and influencers.</li> <li>Corporate partnerships for employee wellness programs.</li> <li>Referral Program: Incentivize users to invite friends.</li> </ul> </li> <li> <p>Metrics for Success:</p> <ul> <li>User acquisition and retention rates.</li> <li>Engagement metrics (website traffic, social media interactions).</li> <li>Conversion rates from free trials to paid subscriptions.</li> <li>User satisfaction and feedback.</li> </ul> </li> </ol> <p>Marketing Expert: This looks solid. Implementing these strategies should effectively raise awareness and attract users to MindWell AI.</p> <p>Mental Health Professional: Agreed. Emphasizing the credibility and efficacy of the platform is key.</p> <p>Tech Enthusiast: The focus on technology benefits and data privacy will resonate well with users.</p> <p>Potential User: This strategy is clear and engaging. I'm excited to see how MindWell AI will help users like me!</p> <p>Final Marketing Strategy Outline:</p> <ol> <li> <p>Target Audience Identification:</p> <ul> <li>Young adults (18-35), professionals, mindfulness enthusiasts.</li> </ul> </li> <li> <p>Branding:</p> <ul> <li>Value Proposition: \"Personalized mental health support anytime, anywhere.\"</li> <li>Brand Personality: Trustworthy, empathetic, innovative.</li> </ul> </li> <li> <p>Digital Marketing Channels:</p> <ul> <li>Website: Informative, user-friendly, visually appealing.</li> <li>Social Media: Active presence on Instagram, Twitter, LinkedIn.</li> <li>Email Marketing: Regular newsletters with mental health tips and updates.</li> <li>SEO: To improve organic search visibility.</li> <li>Targeted Ads: Social media and search engine ads.</li> </ul> </li> <li> <p>Content Strategy:</p> <ul> <li>Blog: Articles on mental health, mindfulness, user stories.</li> <li>Video Content: Therapist Q&amp;A sessions, mindfulness tutorials.</li> <li>Webinars: Live sessions with mental health professionals and AI experts.</li> <li>Interactive Elements: Quizzes for personalized experiences.</li> </ul> </li> <li> <p>Partnerships:</p> <ul> <li>Collaborations with mental health organizations and influencers.</li> <li>Corporate partnerships for employee wellness programs.</li> <li>Referral Program: Incentivize users to invite friends.</li> </ul> </li> <li> <p>Metrics for Success:</p> <ul> <li>User acquisition and retention rates.</li> <li>Engagement metrics (website traffic, social media interactions).</li> <li>Conversion rates from free trials to paid subscriptions.</li> <li>User satisfaction and feedback.</li> </ul> </li> </ol>"},{"location":"prompting/zero_shot/solo_performance_prompting/#api-example","title":"API example","text":"<pre><code>from openai import OpenAI\n\n# Set your API key\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nsystem_prompt = \"When faced with a task, begin by identifying the participants who will contribute to solving the task. Then, initiate a multi-turn collaboration process until a final solution is reached. The participants will give critical comments and detailed suggestions whenever necessary. Now, identify the participants and collaboratively solve the following task step by step. Remember to present your final solution with the prefix 'Final answer:'.\"\n\nuser_prompt = \"Task: Outline a marketing strategy for a new tech startup, MindWell AI. MindWell AI is a personalized mental health and wellness platform that leverages artificial intelligence to provide users with tailored mental health resources, therapy sessions, mindfulness exercises, and daily well-being check-ins. The platform offers a combination of AI-driven chatbots for initial interactions and licensed therapists for more in-depth sessions.\"\n\ncompletion = openai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": user_prompt}\n  ]\n)\n\n# Print the response from the model\nprint(completion.choices[0].message.content)\n</code></pre>"},{"location":"prompting/zero_shot/style_prompting/","title":"Style Prompting","text":""},{"location":"prompting/zero_shot/style_prompting/#style-prompting","title":"Style prompting","text":"<p>Style prompting asks the model to take on a mood or style. It is usually reserved for creative writing tasks and may not be the best approach for eliciting factual information, writing formal presentations, or other activities that require a more neutral tone.</p>"},{"location":"prompting/zero_shot/style_prompting/#how-to-use-it","title":"How to use it","text":"<p>Style prompting simply involves adding stylistic requests to a standard prompt.</p> <p>See \"The 10 aspects of style prompting\" section for style ideas.</p>"},{"location":"prompting/zero_shot/style_prompting/#when-to-use-it","title":"When to use it","text":"<p>When to use style prompting</p> <ul> <li>Useful when the tone, style, or genre of the output is important.</li> <li>Ideal for creative tasks where the style of writing is crucial.</li> </ul>"},{"location":"prompting/zero_shot/style_prompting/#what-to-know","title":"What to know","text":"<p>Style prompting involves instructing a language model to generate text in a specific style, tone, mood, or genre. It can be used to control various stylistic aspects of the generated text, such as writing style, mood, pacing, and formatting.</p> <p>The effectiveness of style prompting can vary depending on the specific language model being used and the combination of stylistic elements in the prompt. Some models may struggle with certain styles or have difficulty distinguishing between the style and the subject matter when the prompts become too complex.</p>"},{"location":"prompting/zero_shot/style_prompting/#the-10-aspects-of-style-prompting","title":"The 10 aspects of style prompting","text":"<p>There are 10 different aspects to consider when using style prompting (Lu et al., 2023a).</p> Aspect Example Terms Examples Writing Style Flowery, Functional, Candid, Prosaic, Ornate, Poetic \"Write a functional passage about sunsets:\" \"Write a flowery passage about writing a paper:\" Tone Humorous, Formal, Dramatic, Optimistic, Sad, Emotional, Nostalgic, Uplifting, Inspirational, Bleak, Grim, Ironic, Satirical \"Write a humorous passage about love:\" \"Write a formal passage about love:\" Mood Angry, Sad, Fearful, Happy, Envious, Anxious, Proud, Regretful, Surprised, Loved, Disgusted \"Write a passage about humanity that makes the reader feel angry:\" \"Write a passage about love that makes the reader feel happy:\" Characterization Direct, Indirect \"Write a story about lovers with indirect characterization:\" \"Write a story about cats with direct characterization:\" Pacing Fast, Slow \"Write a fast-paced story about survivors:\" \"Write a slow-paced story about lovers:\" Plot Rebirth, Comedy, Overcoming the Monster, Rags to Riches, The Quest, Voyage and Return, Tragedy \"Write a story about lovers with a 'Tragedy' plot:\" \"Write a story with a 'Rags to Riches' plot:\" Genre Science Fiction, Mystery, Historical Fiction, Literary Fiction, Dystopian, Horror \"Write a science fiction story about survivors:\" \"Write a mystery story about lovers:\" Numerical Five words, Ten sentences, Twenty paragraphs \"Write a sentence with five words about love:\" \"Write a paragraph with ten sentences about cats:\" Descriptive Concise, Long, Short, Brief, Detailed, In-depth \"Write a concise passage about running:\" \"Write a detailed passage about love:\" Formatting Email, Code, Academic Paper \"Write an email to my teacher asking for help on math homework:\" \"Code Python code that sums two integers up:\""},{"location":"prompting/zero_shot/style_prompting/#best-practices","title":"Best practices","text":"<p>Best practices for style prompting</p> <ul> <li>Models may be sensitive to different combinations of style and mood so use trial and error to find the best combination for your use case.</li> <li>Use the examples in the table above as a starting point.</li> <li>Be specific and descriptive when defining the style you want the model to adopt.</li> </ul>"},{"location":"prompting/zero_shot/style_prompting/#what-to-watch-out-for","title":"What to watch out for","text":"<p>What to watch out for with style prompting</p> <ul> <li>The model may confuse style with subject when prompts are too challenging.</li> <li>Style prompting is most effective for creative writing tasks and may not be suitable for professional or formal contexts that require a neutral, objective tone.</li> <li>Avoid using style prompting for tasks that require factual accuracy, as the focus on style may lead to less precise or reliable information.</li> <li>The effectiveness of style prompting can vary depending on the language model being used, so test your prompts with different models to ensure optimal performance.</li> </ul>"},{"location":"prompting/zero_shot/style_prompting/#citations","title":"Citations","text":"<p>Lu, A., Zhang, H., Zhang, Y., Wang, X., &amp; Yang, D. (2023). Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints. arXiv preprint arXiv:2302.09185</p>"},{"location":"prompting/zero_shot/style_prompting/#prompting","title":"Prompting","text":""},{"location":"prompting/zero_shot/style_prompting/#prompt-template","title":"Prompt template","text":"<p>Style prompting has a simple prompt template. First, include style directions, then outline the task for the language model.</p> <p>{style directions}  {instruction} </p>"},{"location":"prompting/zero_shot/style_prompting/#prompt-example","title":"Prompt example","text":""},{"location":"prompting/zero_shot/style_prompting/#creative-writing-task","title":"Creative writing task","text":"<p>User prompt:</p> <p>Write a fast-paced, sad mystery story about lovers.</p> <p>Model response (Chat GPT4o):</p> <p>The Last Echoes of Love</p> <p>Chapter 1: The Whispering Woods</p> <p>The town of Ravenswood had always been a place of whispered legends and old secrets, but none more haunting than the tale of Emily and Nathan. Their love story was one for the ages, or so everyone believed.</p> <p>Emily, a bright-eyed artist with dreams as vast as the sky, met Nathan, a brooding writer with a heart full of shadows, in the most unlikely of places\u2014the Whispering Woods. It was said that those woods held the memories of the town, that if you listened closely, you could hear the voices of those long gone. Emily and Nathan were drawn to each other like moths to a flame, their love igniting with an intensity that left the town buzzing.</p> <p>Chapter 2: The Disappearance</p> <p>One crisp autumn evening, as the leaves painted the ground with hues of gold and red, Emily vanished. Nathan, desperate and distraught, scoured every corner of Ravenswood and beyond, but Emily was nowhere to be found. The only clue was a single painting left on her easel\u2014an unfinished portrait of Nathan standing alone in the woods, his face etched with sorrow.</p> <p>Days turned into weeks, and hope dwindled. Nathan, once a pillar of strength, became a ghost of himself, wandering the Whispering Woods in search of answers. The townsfolk whispered about curses and old magic, but no one could explain Emily's sudden disappearance.</p> <p>Chapter 3: The Letter</p> <p>One stormy night, as thunder roared and lightning cracked the sky, Nathan received a letter. The handwriting was unmistakably Emily's, delicate and graceful. The letter spoke of love and sorrow, of a secret too dangerous to reveal. She begged Nathan to stop searching, to let her go. But Nathan, consumed by his love and guilt, could not let her memory fade.</p> <p>Driven by the cryptic letter, Nathan delved deeper into the town's history, uncovering dark secrets that had been buried for centuries. He learned of a tragic love affair, eerily similar to his own, that ended in bloodshed and betrayal. The lovers, it seemed, were cursed to relive their sorrow through the ages.</p> <p>Chapter 4: The Revelation</p> <p>As Nathan pieced together the fragments of the past, he realized the curse was linked to the very heart of the Whispering Woods. Desperate for answers, he ventured into the woods one last time, following the whispers that had haunted his dreams.</p> <p>There, in a hidden glade bathed in the pale light of the moon, Nathan found Emily. She stood before an ancient tree, her eyes filled with both love and despair. She explained that the curse bound them to the woods, to the memories of the past. Each cycle, they found and lost each other, their love both their salvation and their doom.</p> <p>Chapter 5: The Choice</p> <p>Emily revealed that the curse could be broken, but at a great cost. One of them had to willingly let go, to embrace the darkness alone so the other could live freely. Nathan, his heart heavy with love and sorrow, made the choice. He would bear the burden, freeing Emily from their eternal torment.</p> <p>As the first light of dawn broke through the trees, Nathan felt the curse lift. Emily's tear-streaked face was the last thing he saw before the darkness claimed him. She was free, but the price of their love was his eternal solitude.</p> <p>Epilogue: The Echoes</p> <p>Years passed, and Emily became a renowned artist, her works filled with the pain and beauty of her lost love. Ravenswood moved on, the whispers in the woods fading with time. But those who dared to venture into the Whispering Woods at dusk sometimes spoke of a solitary figure, a shadow among the trees, whispering the name of his lost love\u2014Emily.</p> <p>And in the quiet moments, when the wind sighed through the branches, you could almost hear the echoes of a love too powerful to be forgotten, a love that transcended time and fate.</p>"},{"location":"prompting/zero_shot/style_prompting/#business-task","title":"Business task","text":"<p>User prompt:</p> <p>In a persuasive and motivational style outline a marketing strategy for a new tech startup, MindWell AI. MindWell AI is a personalized mental health and wellness platform that leverages artificial intelligence to provide users with tailored mental health resources, therapy sessions, mindfulness exercises, and daily well-being check-ins. The platform offers a combination of AI-driven chatbots for initial interactions and licensed therapists for more in-depth sessions.</p> <p>Model response (ChatGPT 4o):</p> <p>Marketing Strategy for MindWell AI: Elevating Mental Health with Technology</p> <p>Vision Statement: \"Empowering minds, enhancing well-being, and fostering a healthier tomorrow with AI-driven personalized mental health care.\"</p> <ol> <li> <p>Market Research and Analysis</p> <ol> <li>Identify Target Audience:<ul> <li>Primary Target: Individuals seeking mental health support, including those with anxiety, depression, and stress.</li> <li>Secondary Target: Employers looking to provide mental health resources to their employees.</li> <li>Tertiary Target: Healthcare providers and therapists interested in integrating AI into their practice.</li> </ul> </li> <li>Competitor Analysis:<ul> <li>Analyze strengths and weaknesses of competitors like BetterHelp, Talkspace, and Calm.</li> <li>Identify gaps in their services to position MindWell AI as a superior alternative.</li> </ul> </li> </ol> </li> <li> <p>Brand Positioning and Messaging</p> <ol> <li>Unique Selling Proposition (USP):<ul> <li>\"MindWell AI combines cutting-edge AI technology with compassionate human touch to provide the most personalized mental health care experience.\"</li> </ul> </li> <li>Brand Values:<ul> <li>Personalization: Tailored solutions for individual needs.</li> <li>Accessibility: Affordable and available anytime, anywhere.</li> <li>Compassion: Combining AI efficiency with human empathy.</li> </ul> </li> <li>Key Messages:<ul> <li>\"Your mental health companion, always ready to listen.\"</li> <li>\"AI-powered insights, human-driven care.\"</li> <li>\"Transforming mental health care with technology and empathy.\"</li> </ul> </li> </ol> </li> <li> <p>Digital Marketing Strategies</p> <ol> <li>Content Marketing:<ul> <li>Blog: Regular posts on mental health topics, success stories, and expert advice.</li> <li>Video Content: Tutorials, user testimonials, and interviews with mental health professionals.</li> <li>Social Media: Engage users with motivational quotes, mental health tips, and interactive content on platforms like Instagram, Facebook, and Twitter.</li> </ul> </li> <li>Search Engine Optimization (SEO):<ul> <li>Optimize the website with relevant keywords such as \"AI mental health,\" \"personalized therapy,\" and \"mindfulness exercises.\"</li> <li>Create high-quality backlinks and improve site speed and mobile friendliness.</li> </ul> </li> <li>Pay-Per-Click (PPC) Advertising:<ul> <li>Run targeted ad campaigns on Google and social media platforms.</li> <li>Use retargeting strategies to convert website visitors into users.</li> </ul> </li> </ol> </li> <li> <p>Partnerships and Collaborations</p> <ol> <li>Collaborate with Mental Health Professionals:<ul> <li>Partner with licensed therapists to offer combined AI and human therapy sessions.</li> <li>Create a referral program for therapists to recommend MindWell AI to their clients.</li> </ul> </li> <li>Corporate Wellness Programs:<ul> <li>Offer tailored packages for businesses to provide MindWell AI to their employees.</li> <li>Highlight benefits such as improved employee well-being and productivity.</li> </ul> </li> <li>Academic and Research Partnerships:<ul> <li>Collaborate with universities and research institutions to validate the efficacy of AI-driven mental health interventions.</li> <li>Publish research findings to build credibility and trust.</li> </ul> </li> </ol> </li> <li> <p>Community Engagement and User Support</p> <ol> <li>Online Community Building:<ul> <li>Create forums and support groups within the platform for users to share experiences and support each other.</li> <li>Host regular virtual events, webinars, and Q&amp;A sessions with mental health experts.</li> </ul> </li> <li>Exceptional Customer Support:<ul> <li>Offer 24/7 customer support via chat, email, and phone.</li> <li>Provide comprehensive onboarding and continuous assistance to ensure users maximize the platform's benefits.</li> </ul> </li> </ol> </li> <li> <p>Influencer and Advocacy Marketing</p> <ol> <li>Influencer Partnerships:<ul> <li>Collaborate with mental health influencers and bloggers to reach a wider audience.</li> <li>Use authentic reviews and endorsements to build trust and credibility.</li> </ul> </li> <li>User Advocacy:<ul> <li>Encourage satisfied users to share their success stories and experiences on social media.</li> <li>Create a referral program rewarding users for bringing new members to the platform.</li> </ul> </li> </ol> </li> <li> <p>Data-Driven Strategies and Continuous Improvement</p> <ol> <li>Analytics and Feedback:<ul> <li>Use analytics to track user behavior, engagement, and satisfaction.</li> <li>Regularly gather user feedback to identify areas for improvement and new feature development.</li> </ul> </li> <li>Adaptive AI:<ul> <li>Continuously update the AI algorithms based on user interactions and feedback to enhance personalization and effectiveness.</li> </ul> </li> </ol> </li> </ol> <p>Conclusion</p> <p>MindWell AI is poised to revolutionize mental health care by leveraging the power of AI to provide personalized, accessible, and compassionate support. Through strategic marketing, community engagement, and continuous innovation, MindWell AI will not only reach those in need but also create a lasting impact on the mental well-being of its users. Together, we can foster a world where mental health is prioritized, accessible, and stigma-free.</p> <p>\"Join us in transforming mental health care\u2014one mind at a time.\"</p>"},{"location":"prompting/zero_shot/style_prompting/#api-example","title":"API example","text":"<pre><code>from openai import OpenAI\n\n# Set your API key\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\nprompt = \"In a persuasive and motivational style outline a marketing strategy for a new tech startup, MindWell AI. MindWell AI is a personalized mental health and wellness platform that leverages artificial intelligence to provide users with tailored mental health resources, therapy sessions, mindfulness exercises, and daily well-being check-ins. The platform offers a combination of AI-driven chatbots for initial interactions and licensed therapists for more in-depth sessions.\"\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"user\", \"content\": prompt}\n  ]\n)\n\n# Print the response from the model\nprint(completion.choices[0].message.content)\n</code></pre>"},{"location":"prompting/zero_shot/system_2_attention/","title":"System 2 Attention (S2A)","text":""},{"location":"prompting/zero_shot/system_2_attention/#system-2-attention-s2a","title":"System 2 Attention (S2A)","text":"<p>System 2 Attention is a technique that harnesses the ability of language models to focus on specific parts of the input text, enabling them to concentrate on the most relevant information for generating accurate and concise responses. S2A is particularly useful in situations where users provide additional context that may not be directly relevant to the main question or include biased opinions.</p> <p>For instance, if a user asks about Egyptian history but prefaces their question with a lengthy story about their family vacation to the pyramids, S2A instructs the model to extract the core question and relevant context while filtering out unnecessary details. Similarly, in domains such as math word problems or business case studies, the provided context may include information or opinions that are not essential to answering the question at hand. By applying S2A in these scenarios, the model can generate more focused and accurate responses.</p>"},{"location":"prompting/zero_shot/system_2_attention/#how-to-use-it","title":"How to use it","text":"<p>To use System 2 Attention (S2A), add a special instruction to the user prompt that tells the language model to extract the main question or request and disregard irrelevant or biased context from the user's input.</p> <p>See \"Prompting\" section for usage details.</p>"},{"location":"prompting/zero_shot/system_2_attention/#when-to-use-it","title":"When to use it","text":"<p>When to use System 2 Attention</p> <ul> <li>Ideal for complex tasks where filtering out irrelevant information can significantly improve the quality of the response, particularly in applications where you are processing input text from third-party sources over which you have no control.</li> <li>Effective in situations that demand focused and succinct answers.</li> <li>Particularly useful when solving multi-step math word problems.</li> </ul>"},{"location":"prompting/zero_shot/system_2_attention/#what-to-know","title":"What to know","text":"<p>S2A empowers large language models to identify and prioritize the most important aspects of the input context, resulting in higher-quality responses. By leveraging the LLMs' natural language reasoning capabilities and their ability to follow instructions, S2A guides the models to focus on the most relevant information. The process involves the LLM regenerating the input context, including only the pertinent portions, and then using this refined context to generate the final response.</p> <p>To ensure optimal performance, some post-processing may be necessary after the S2A context regeneration step to properly structure the prompt for the final response generation. See \"Prompting\" section for usage details.</p> <p>To better understand S2A, two examples from Weston and Sukhbaatar (2023) are provided below. In the first example, the original math word problem (top purple boxes) contains a decoy sentence: \"Max has 1000 more books than Mary.\" This sentence confuses the language model (LLaMA-2-70B). Using S2A (flow on the right), the model rewrites the question, removing the unnecessary sentence. The revised word problem is then correctly evaluated by the language model. </p> <p>In the second example, the users ask about a band, suggesting that they think the answer is Johnny Depp (top purple boxes). This leading sentence confuses the language model. Using S2A (right flow), the model first rewrites the sentence, removing the suggested answer. The language model then correctly identifies the great American actor and internet's boyfriend, Keanu Reeves, as the correct answer. </p>"},{"location":"prompting/zero_shot/system_2_attention/#best-practices","title":"Best practices","text":"<p>Best practices for System 2 Attention</p> <ul> <li>Provide clear instructions to the LLM to regenerate the context, extracting the most valuable parts that offer relevant context for a specific query. Start with the prompt template above and experiment as necessary.</li> <li>After S2A context regeneration, discard the original context. Allowing the model to access both the original context and the S2A version can negatively impact performance.</li> <li>If working in an application setting, you likely want to prompt the model to return a JSON object so the question can easily be separated from the other context (see API example below).</li> </ul>"},{"location":"prompting/zero_shot/system_2_attention/#what-to-watch-out-for","title":"What to watch out for","text":"<p>What to watch out for with System 2 Attention</p> <ul> <li>S2A tends to be most effective for small and mid-sized models and may be less impactful for the most advanced model versions. In my testing, GPT-4 did not fall victim to the distractions outlined in Weston and Sukhbaatar (2023), shown above.</li> <li>If working in an application setting, you likely want to prompt the model to return a JSON object so the question can easily be separated from the other context (see API example below).</li> </ul>"},{"location":"prompting/zero_shot/system_2_attention/#citations","title":"Citations","text":"<p>Weston, J., &amp; Sukhbaatar, S. (2023). System 2 Attention (is something you might need too). arXiv preprint arXiv:2311.11829</p>"},{"location":"prompting/zero_shot/system_2_attention/#prompting","title":"Prompting","text":"<p>The S2A prompt template used in Weston and Sukhbaatar (2023) instructs the model to extract the main query or question from the larger context of the input prompt, along with any relevant unbiased information. The model is asked to disregard opinions, biases, and irrelevant details in the original prompt. The extracted information is then used to provide a focused, unbiased response to the core query.</p> <p>The template consists of two main parts:</p> <ol> <li> <p>Instructions to the model:</p> <ul> <li>Extract the unbiased context and the main question or query from the input prompt</li> <li>Exclude any opinions, biases, or irrelevant information</li> <li>Provide the response in a specified format, with labeled sections for the extracted context and question</li> </ul> </li> <li> <p>The original input prompt, which is inserted into the template.</p> </li> </ol> <p>By using this template, the S2A system aims to improve the accuracy and objectivity of the model's responses by guiding it to focus on the most relevant and unbiased information within the potentially noisy or biased input prompt.</p>"},{"location":"prompting/zero_shot/system_2_attention/#prompt-template","title":"Prompt template","text":"<p>Given the following text by a user, extract the part that is unbiased and not their opinion, so that using that text alone would be good context for providing an unbiased answer to the question portion of the text.</p> <p>Please include the actual question or query that the user is asking. Separate this into two categories labeled with \u201cUnbiased text context (includes all content except user\u2019s bias):\u201d and \u201cQuestion/Query (does not include user bias/preference):\u201d.</p> <p>{original input prompt}</p> <p>As noted in the best practices section, it can sometimes be useful to request the model return a response in JSON format. See API example below for usage.</p> <p>Output this information in a JSON format with two keys: 'context' and 'question'.\"</p> <p>JSON Signature Example: <pre><code>{{\n  \"context\": \"Unbiased text context here\",\n  \"question\": \"Question/Query here\"\n}}\n</code></pre></p> <p>You can also try a shorter, simpler version of the prompt:</p> <p>Extract the unbiased context and the main question(s) from the following text, excluding any opinions or irrelevant information:  {original input prompt} </p>"},{"location":"prompting/zero_shot/system_2_attention/#prompt-example","title":"Prompt example","text":"<p>S2A prompt:</p> <p>Given the following text by a user, extract the part that is unbiased and not their opinion, so that using that text alone would be good context for providing an unbiased answer to the question portion of the text.</p> <p>Please include the actual question or query that the user is asking. Separate this into two categories labeled with \u201cUnbiased text context (includes all content except user\u2019s bias):\u201d and \u201cQuestion/Query (does not include user bias/preference):\u201d.</p> <p>I'm doing a research project on the history of space exploration, and I'm currently focusing on the Apollo program that sent astronauts to the Moon in the 1960s and 70s. I've been reading a lot of primary sources from the time period, like interviews with the astronauts and mission transcripts, and I'm struck by how ambitious and risky the whole endeavor was.</p> <p>I recently came across this quote from John F. Kennedy's famous \"Moon Speech\" at Rice University in 1962:</p> <p>\"We choose to go to the moon in this decade and do the other things, not because they are easy, but because they are hard, because that goal will serve to organize and measure the best of our energies and skills, because that challenge is one that we are willing to accept, one we are unwilling to postpone, and one which we intend to win.\"</p> <p>I love how Kennedy framed the Moon mission as a challenge to be embraced rather than a risk to be avoided. It really captures the spirit of the times and the sense of national pride and purpose that drove the Apollo program forward.</p> <p>But I'm also curious about the scientific goals of the missions, beyond just the political and cultural significance. What were the main experiments and observations that the astronauts conducted on the lunar surface? And how did their findings advance our understanding of the Moon and the wider solar system?</p> <p>I've heard that they collected a lot of lunar rocks and soil samples, but I'm not sure what specific insights we gained from analyzing those materials. And I know there were some technical glitches and close calls during the missions, but I'm fuzzy on the details.</p> <p>Can you give me an overview of the key scientific achievements and discoveries of the Apollo program? I want to make sure my research project covers all the important angles and doesn't just focus on the human drama and historical significance of the missions. Thanks in advance for your help!</p> <p>Model response (ChatGPT 4o):</p> <p>Unbiased text context (includes all content except user\u2019s bias):</p> <p>I'm doing a research project on the history of space exploration, and I'm currently focusing on the Apollo program that sent astronauts to the Moon in the 1960s and 70s. I've been reading a lot of primary sources from the time period, like interviews with the astronauts and mission transcripts. I recently came across this quote from John F. Kennedy's famous \"Moon Speech\" at Rice University in 1962:</p> <p>\"We choose to go to the moon in this decade and do the other things, not because they are easy, but because they are hard, because that goal will serve to organize and measure the best of our energies and skills, because that challenge is one that we are willing to accept, one we are unwilling to postpone, and one which we intend to win.\"</p> <p>But I'm also curious about the scientific goals of the missions, beyond just the political and cultural significance. What were the main experiments and observations that the astronauts conducted on the lunar surface? And how did their findings advance our understanding of the Moon and the wider solar system?</p> <p>I've heard that they collected a lot of lunar rocks and soil samples. And I know there were some technical glitches and close calls during the missions.</p> <p>Question/Query (does not include user bias/preference): Can you give me an overview of the key scientific achievements and discoveries of the Apollo program?</p> <p>Pass the question to Llama 3 (8B):</p> <p>Can you give me an overview of the key scientific achievements and discoveries of the Apollo program?</p> <p>Model response (Llama 8B):</p> <p>The Apollo program was a series of NASA spaceflights that successfully  landed humans on the Moon between 1969 and 1972. The program's scientific  achievements and discoveries were significant, marking a major milestone  in human exploration and understanding of space. Here are some key  scientific achievements and discoveries:</p> <ol> <li>Lunar Surface Exploration: Apollo astronauts spent over 171 hours  walking on the Moon's surface, collecting samples, and conducting  experiments. This provided valuable insights into the Moon's geology,  composition, and atmosphere.</li> <li>Sample Return: The Apollo program brought back more than 842 pounds  (382 kilograms) of lunar rocks and soil, which scientists analyzed to  understand the Moon's formation, evolution, and internal structure.</li> <li>Moon's Internal Structure: Seismic experiments conducted by Apollo  astronauts revealed that the Moon has a partially molten core, similar to  Earth's, but much smaller.</li> <li>Lunar Atmosphere: The Apollo missions measured the Moon's extremely  thin atmosphere (exosphere), which was found to be about 10^(-12) times  thinner than Earth's.</li> <li>Solar Wind: Scientists confirmed the existence of the solar wind, a  stream of charged particles emitted by the Sun, by analyzing data from the  Apollo spacecraft.</li> <li>Space Radiation: The Apollo missions provided valuable data on  space radiation, which helped scientists better understand the effects of  cosmic rays on living organisms and equipment.</li> <li>Gravity Measurements: The Apollo spacecraft carried instruments  that measured the Moon's gravitational field, allowing scientists to map  its internal structure and composition.</li> <li>Lunar Regolith: The Apollo astronauts brought back samples of the  lunar regolith (soil), which revealed the Moon's complex geological  history, including impact craters, faulting, and volcanic activity.</li> <li>Moonquakes: Seismic experiments detected moonquakes, which helped  scientists understand the Moon's internal structure and tectonic activity.</li> <li>Technological Advancements: The Apollo program drove significant  technological innovations, such as advancements in rocket propulsion, life  support systems, communication technologies, and space suit design.</li> <li>Scientific Instrumentation: The Apollo spacecraft carried a range  of scientific instruments, including cameras, spectrometers,  magnetometers, and seismometers, which provided valuable data on the  Moon's surface, atmosphere, and subsurface.</li> <li>Lunar Laser Ranging: The Apollo program established the Lunar  Laser Ranging (LLR) technique, which uses laser beams to measure the  distance between Earth and the Moon with high accuracy.</li> </ol> <p>These scientific achievements and discoveries have far-reaching  implications for our understanding of the Moon, the solar system, and the  universe as a whole. The Apollo program's legacy continues to inspire new  generations of scientists, engineers, and explorers.</p>"},{"location":"prompting/zero_shot/system_2_attention/#api-example","title":"API example","text":"<pre><code>from openai import OpenAI\nimport ollama\nimport json\n\n# Set your API key\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n# Function to call GPT-4-turbo and parse the response\ndef extract_user_question(user_prompt):\n    # Step 1: Rewrite the prompt to remove any unrelated information\n    s2a_prompt_prefix = \"\"\"\n    Given the following text by a user, extract the part that is unbiased and not their opinion, so that using that text alone would be good context for providing an unbiased answer to the question portion of the text.\n\n    Please include the actual question or query that the user is asking. Separate this into two categories labeled with \u201cUnbiased text context (includes all content except user\u2019s bias):\u201d and \u201cQuestion/Query (does not include user bias/preference):\u201d.\n\n    Output this information in a JSON format with two keys: 'context' and 'question'.\"\n\n    JSON Signature Example:\n\n    {{\n      \"context\": \"Unbiased text context here\",\n      \"question\": \"Question/Query here\"\n    }}\n    \"\"\"\n\n    gpt_prompt = f\"{s2a_prompt_prefix}\\n\\n{user_prompt}\"\n\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4-turbo\",\n        response_format={ \"type\": \"json_object\" },\n        messages=[\n            {\"role\": \"user\", \"content\": gpt_prompt}\n        ]\n    )\n\n    gpt_response = completion.choices[0].message.content\n\n    # Parse the JSON response\n    parsed_response = json.loads(gpt_response)\n    extracted_question = parsed_response[\"question\"]\n\n    return extracted_question\n\n# Function to call Ollama's Llama3 model with the extracted question\ndef answer_question(question):\n    messages = [\n        {\"role\": \"user\", \"content\": question}\n    ]\n    result = ollama.chat(model='llama3', messages=messages, stream=False)\n    return result['message']['content']\n\n# User prompt\nuser_prompt = \"\"\"I'm doing a research project on the history of space exploration, and I'm currently focusing on the Apollo program that sent astronauts to the Moon in the 1960s and 70s. I've been reading a lot of primary sources from the time period, like interviews with the astronauts and mission transcripts, and I'm struck by how ambitious and risky the whole endeavor was.\n\nI recently came across this quote from John F. Kennedy's famous \"Moon Speech\" at Rice University in 1962:\n\n\"We choose to go to the moon in this decade and do the other things, not because they are easy, but because they are hard, because that goal will serve to organize and measure the best of our energies and skills, because that challenge is one that we are willing to accept, one we are unwilling to postpone, and one which we intend to win.\"\n\nI love how Kennedy framed the Moon mission as a challenge to be embraced rather than a risk to be avoided. It really captures the spirit of the times and the sense of national pride and purpose that drove the Apollo program forward.\n\nBut I'm also curious about the scientific goals of the missions, beyond just the political and cultural significance. What were the main experiments and observations that the astronauts conducted on the lunar surface? And how did their findings advance our understanding of the Moon and the wider solar system?\n\nI've heard that they collected a lot of lunar rocks and soil samples, but I'm not sure what specific insights we gained from analyzing those materials. And I know there were some technical glitches and close calls during the missions, but I'm fuzzy on the details.\n\nCan you give me an overview of the key scientific achievements and discoveries of the Apollo program? I want to make sure my research project covers all the important angles and doesn't just focus on the human drama and historical significance of the missions. Thanks in advance for your help!\n\"\"\"\n\n# Get the unbiased context and question\nquestion = extract_user_question(user_prompt)\n\n# Call the Ollama API with the extracted question\nanswered_question = answer_question(question)\n\n# Print the results\nprint(\"\\nOllama Response:\\n\", answered_question)\n</code></pre>"},{"location":"synthetic_data/basic_tutorial_1/airbnb_rewriting/","title":"Creating Synthetic Apartment Reviews Dataset","text":"<p>In this tutorial, we'll create some basic synthetic data as an exercise. The Python notebook for the project can be found on my GitHub here.</p> <p>The goal is to create a set of realistic apartment reviews that contain the monthly price paid for the apartment. We'll use the review to populate a JSON object which includes:</p> <ul> <li>The review</li> <li>A natural language query requesting an extraction of the monthly rent price from the review</li> <li>A JSON schema corresponding to that query</li> <li>A JSON object which matches the schema and contains the rent price (can be used as a target for benchmarking or finetuning)</li> </ul> <p>To achieve this, we'll take the creative approach of using a large language model to translate short-term Airbnb rental reviews into long-term apartment reviews that contain monthly pricing information from the reviewer. This exercise is based on a real-world use case. I was surprised when I searched online for apartment review datasets, but was unable to find many. One way to solve this problem would be to scrape apartment rental reviews from the web, but creating synthetic data allows us to have more control over the output. Another approach is to simply ask a language model to generate fictional reviews. However, in my experience seeding the model with a real review generates richer and more varied synthetic data. The approach used here can be extended in many ways.</p> <p>Because the dataset contains both the unstructured text as well as the target output, this kind of synthetic data can be used:</p> <ul> <li>To create a data extraction benchmark to evaluate a large language model or other machine learning model (i.e. how well can a language model take a natural language query and output a correct JSON object representing the result of that query).</li> <li>To fine-tune a smaller language model on data extraction activities.</li> <li>For educational purposes, if you want to create curated dataset of unstructured text to help teach students.</li> <li>To create information-dense text that might more efficiently train a large language model.</li> </ul>"},{"location":"synthetic_data/basic_tutorial_1/airbnb_rewriting/#what-the-data-synthetic-data-looks-like","title":"What the data synthetic data looks like","text":"<p>Let's compare the results from three different language models, all using the same process from this tutorial (see code below). As you can see, all three models did a good job of transforming the short-term Airbnb review into a realistic apartment review.</p>"},{"location":"synthetic_data/basic_tutorial_1/airbnb_rewriting/#original-airbnb-review","title":"Original Airbnb review","text":"<p>\"Frans was a wonderful host.  He picked us up from the airport, and unfortunately we had major issues with our flight and he waited an hour for us at the airport!  The houseboat was very clean and comfortable.  It had all the amenities you would need and he did a great job of explaining everything to us.  There are lots of resources in the houseboat to help you navigate Amsterdam, but Frans also stays directly beside the houseboat.  It was reassuring knowing he was there in case we needed anything.  Even though it rained during our short stay, we loved the location.  It's easy access to the tram and to a grocery store, but it is away from the hustle and bustle of the city.  It's a beautiful and serene place.  When we left, Frans gave us an adorable souvenir.  We accidentally left our converter charger at the houseboat, and he rushed it to us while we were waiting on our tram.  Great host, great place, and I highly recommend it.\"</p>"},{"location":"synthetic_data/basic_tutorial_1/airbnb_rewriting/#gpt-4o-rewritten-review","title":"GPT-4o rewritten review","text":"<p>The code used in this tutorial leverages the GPT-4o API. Of the three models, GPT-4o is the only one that didn't incorporate the forgotten power converter into the review.</p> <p>\"Living on this houseboat was an exceptional experience. It was very clean and comfortable, with all the amenities you could need. The houseboat is situated in a beautiful and serene location, away from the hustle and bustle of the city, yet with easy access to the tram and a grocery store nearby. The atmosphere was peaceful, making it an ideal place to call home. The landlord was incredibly considerate, even leaving an adorable souvenir for us when we first moved in. There were plenty of resources available in the houseboat to help navigate Salem and the surroundings. The price at $2690 per month felt justified for such a unique living experience in this charming city. Even when it rained, the location remained lovely, and we thoroughly enjoyed our time there. Highly recommend considering this place for those looking for a distinctive and tranquil residence.\"</p>"},{"location":"synthetic_data/basic_tutorial_1/airbnb_rewriting/#claude-sonnet-35-rewritten-review","title":"Claude Sonnet 3.5 rewritten review","text":"<p>Of the three models, I prefer Claude Sonnet 3.5's interpretation the most.</p> <p>\"This houseboat apartment has been a gem! The location is perfect - easy access to public transportation and a nearby grocery store, but tucked away from the hustle and bustle of the city center. It's a serene and beautiful place to call home. The apartment itself is very clean, comfortable, and has all the amenities one could need. The landlord did an excellent job explaining everything when I first moved in. Even when minor issues arise, the landlord is always responsive and helpful. For instance, I accidentally left my converter charger in the apartment when moving out, and the landlord rushed to get it back to me before I left. Such attentive service is truly appreciated. The rent is $2690 per month, which is a fair price for the quality of living and the convenient location in Nashville. I highly recommend considering this houseboat apartment if you're looking for a unique and peaceful living experience in the city.\"</p>"},{"location":"synthetic_data/basic_tutorial_1/airbnb_rewriting/#llama3-8b-rewritten-review","title":"Llama3 (8B) rewritten review","text":"<p>While the Llama3 (8B) results were good when the model returned a rewritten review, it was unreliable. Sometimes it returned the original review instead of the rewritten one, failed to incorporate the correct price, or added unnecessary leading or trailing explanatory text.</p> <p>\"Frans was a wonderful landlord! He helped me settle in by explaining everything about the apartment and its amenities. The location is beautiful and serene, with easy access to public transportation and a grocery store. It's away from the hustle and bustle of the city, but still close enough to enjoy all it has to offer. I loved living here and highly recommend it! The rent was around $2690 per month, which was very reasonable considering the area and amenities. Frans even gave me an adorable souvenir when I moved out, and he went above and beyond to help me get my converter charger back after I left it behind. Overall, it's a great place to call home.\"</p> <p>Now, let's create our synthetic data!</p>"},{"location":"synthetic_data/basic_tutorial_1/airbnb_rewriting/#getting-the-data","title":"Getting the Data","text":"<p>The Airbnb review dataset can be found on Kaggle here.</p>"},{"location":"synthetic_data/basic_tutorial_1/airbnb_rewriting/#examining-the-data","title":"Examining the data","text":"<p>It's always a good idea to play around with your data first. Let's have a look.</p> <p>Read in the data.</p> <pre><code>import pandas as pd\n\n# Set file path and read in AirBnb reviews\nfile_path = 'airbnb_reviews.csv'\ndf = pd.read_csv(file_path)\n</code></pre> <p>Look at the column names.</p> <pre><code># Look at column names\ndf.columns\n</code></pre> <pre><code>Index(['listing_id', 'id', 'date', 'reviewer_id', 'reviewer_name', 'comments'], dtype='object')\n</code></pre> <p>What does a single row look like? From looking at the output it seems \"comments\" is the column that contains the user reviews.</p> <pre><code># Look at the first row\ndf.head(1)\n</code></pre> listing_id id date reviewer_id reviewer_name comments 0 2818.0 1191.0 3/30/2009 10952 Lam Daniel is really cool. The place was nice and ... <p>Look at the number of rows. Almost 350,000! A lot of reviews.</p> <pre><code># Look at number of rows\nprint(df.shape[0])\n</code></pre> <pre><code>342904\n</code></pre> <p>Read a few sample reviews to get a feeling for things. As I did this I noticed not all reviews are in English.</p> <pre><code># Examine some random reviews\nreview = df.sample(1)['comments'].iloc[0]\nprint(review)\n</code></pre> <pre><code>Juliaan was a great host, check-in was very easy as he lives right above the unit. The neighborhood is excellent, we felt very safe and plenty of restaurant choices in the immediate vicinity and 15-20 minute walking distance from all the major attractions, including the train station.&lt;br/&gt;&lt;br/&gt;The unit has a small refrigerator and utensils, but no actual kitchen (which we knew from the description). The neighborhood is quiet and the unit is up a few floors from the street level so noise was not an issue at all. Overall a great stay and would definitely stay there again.\n</code></pre> <p>Here's an example of a review in French.</p> <pre><code># Some reviews are not in English. We need to filter them out. We'll also subset to reviews that are 50 words or longer.\ndf.loc[97053, 'comments']\n</code></pre> <pre><code>'L\u2019appartement de Thomas est situ\u00e9 dans un quartier s\u00fbr et tranquille, \u00e0 proximit\u00e9 du Vondelpark, de restaurants, caf\u00e9s et magasins. Il est grand, lumineux, tr\u00e8s propre et charmant, avec une touche vintage. La cuisine est tr\u00e8s bien \u00e9quip\u00e9e et les jouets des enfants sont un plus pour les familles. Thomas m\u00e9rite bien son appellation de \u00ab\\xa0super host\\xa0\u00bb. Il est tr\u00e8s accueillant et attentionn\u00e9. Les proc\u00e9dures de check-in et -out sont tr\u00e8s faciles. &lt;br/&gt;Les familles trouveront leur bonheur dans cet appartement. '\n</code></pre>"},{"location":"synthetic_data/basic_tutorial_1/airbnb_rewriting/#filter-the-data","title":"Filter the data","text":"<p>For this activity I want only reviews in English. We can use the langdetect library to detect the language of each review and remove those that aren't in English. I've also decided I want the reviews to be at least 50 words long to ensure they are more interesting and provide a good text of data extraction capabilities if used later for evaluation or fine-tuning.</p> <pre><code>from langdetect import detect, LangDetectException\n\n# Use the detect method from langdetect to create a simple English language detection function\ndef is_english(text):\n    try:\n        if detect(text) == 'en':\n            return True\n        else:\n            return False\n    except LangDetectException:\n        return False\n\n# Prep comments column\ndf['comments'] = df['comments'].fillna('')\ndf = df[df['comments'].str.len() &gt; 0]\n\n# Subset the df to reviews that are longer than 50 words\ndf['review_word_count'] = df['comments'].apply(lambda text: len(text.split())) # Review word count\ndf = df[df['review_word_count'] &gt;= 50] # Subsetting\n\n# Subset the df to reviews that are in English\ndf['is_english'] = df['comments'].apply(lambda text: is_english(text)) # Is review in English?\ndf = df[df['is_english']] # Subsetting\n</code></pre> <p>Let's look at the number of rows now. Much shorter!</p> <pre><code># Look at new shape\nprint(df.shape[0])\n</code></pre> <pre><code>92830\n</code></pre>"},{"location":"synthetic_data/basic_tutorial_1/airbnb_rewriting/#generating-the-synthetic-data","title":"Generating the synthetic data","text":"<p>Now let's generate our synthetic data. I started by sampling a few random rows from the dataset and selected three Airbnb reviews. I then rewrote these reviews to reflect long-term apartment rental experiences rather than short-term Airbnb stays. Initially, I used ChatGPT to assist with this task, but it struggled to produce realistic reviews until I added specific instructions and provided examples. For instance, it would use phrases like \"When we first checked in,\" which are not typical in apartment reviews. After several iterations of refining the instructions and examples, I developed a prompt that works quite well.</p> <p>Additionally, I created a function to generate rent prices. The function samples from a normal distribution but ensures the final digit matches the step size ($5 in my code) to more realistically reflect rental prices. Alternatively, you could generate prices using a straightforward method, such as sampling from a uniform distribution:</p> <p><code>rent_prices = random.sample(range(1000, 6000, 5), n)</code></p> <p>or from a normal distribution if you don't care about the rent prices having consistent final digits:</p> <p><code>rent_prices = np.random.normal(2500, 1000, n)</code></p> <p>In this example, the language model inserts only the rent price into the review. However, you could extend this approach to insert multiple pieces of information.</p> <p>To increase the diversity in the synthetic dataset, I also introduced a step to replace \"Amsterdam\" if mentioned. It appears the original AirBnb dataset is from Amsterdam and this creates an unbalanced city distrubtion, even after having a language model rewrite the review. The logic selects a random city from a predefined list of well-known U.S. cities. Such a list is easy to create (ChatGPT printed out a list of 300 U.S. cities in a matter of seconds, which I copied and pasted into a text file). This approach enhances the variety of the synthetic data and makes it more representative of different rental markets.</p> <p>Before we proceed to the actual data generation, let's look at the resulting JSON object. The JSON object consists of:</p> <ol> <li>Review Text: The rewritten review reflecting a long-term apartment rental experience.</li> <li>Natural Language Query: A query matching the JSON schema. In this case the query is \"What was the rent price paid by this reviewer?\"</li> <li>JSON Schema: A schema I created for this activity, representing the structure we would want the model to follow for data extraction. This schema is not passed to the model but included in the final result for future benchmarking or fine-tuning.</li> <li>Rent Price: The rent price passed to the model and included in the review. The rent is allowed to be 'NA' in which case the rewritten review won't mention the rent price. This more realistically mimics the variety of real reviews.</li> </ol> <pre><code>json_object = {\n    \"review\": rewritten_review,\n    \"query\": \"What was the rent price paid by this reviewer?\",\n    \"json_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"rent_price\": {\n                \"type\": \"integer\",\n                \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n            }\n        },\n        \"required\": [\"rent_price\"]\n    }, \n    \"expected_result\": {\n        \"rent_price\": rent_price\n    }\n}\n</code></pre> <p>These JSON objects are concatenated to create the final dataset of length n, which is a JSON file composed of these individual JSON objects.</p> <p>With this setup, we're ready to generate the synthetic data.</p> <pre><code>from openai import OpenAI\nimport numpy as np\nimport os\nimport json\nimport random\n\n# Set API key\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\")\n)\n\ndef create_apartment_review(original_review, rent_price, city):\n  # This function transforms an Airbnb review into a long-term apartment review.\n  # It takes the original Airbnb review and a rent price, then generates a rewritten\n  # review that reflects long-term living in an apartment. The transformed review\n  # avoids language specific to short-term stays and replaces mentions of the host \n  # with details about the apartment, building, or area. Additionally, it incorporates\n  # the rent price naturally within the review. If the rent price is 'NA', then rent \n  # won't be mentioned. This more realistically mirrors the fact that not all reviews \n  # mention rent. These reviews are from Amsterdam and many reviews mention the city. \n  # Therefore, I created logic to replace Amsterdam with a random U.S. city to get more \n  # diversity in the data.\n\n  # Construct the prompt for the language model\n  prompt = f\"\"\"\n    Your task is to transform Airbnb reviews into long-term apartment reviews. Imagine the tenant lived in the apartment for somewhere between one and five years. Ensure that the rewritten reviews reflect long-term living, avoiding short-term stay language like \"When we checked in.\" The language should convey that the tenant had control over decoration and furniture.\n\n    Most Airbnb reviews mention the host, but that is uncommon in apartment reviews. Replace sentences about the host with details about the apartment, building, area, or landlord. Ensure the rewritten review feels authentic while drawing inspiration from the original Airbnb review.\n\n    The reviews original come from Amsterdam. If the review mentions Amsterdam, replace it with the specified city.\n\n    Incorporate a sentence about the price naturally within the review. Use creative and varied language when discussing the price. If the provided rent price is 'NA' the rewritten review should not mention the rent price.\n\n    --- Example 1 ---\n    Original Airbnb review: Tammy was an amazing host! One of the best Airbnb\u2019s I have ever stayed at! Great decorations, didn\u2019t feel like a typical airbnb rental property, bright and cozy rooms. It was very convenient and a short walk to the city center around 15-20 minutes. Reasonably priced compared to many places in Amsterdam and well worth every penny! Would definitely stay again my next visit :) Thanks again Tammy!\n\n    Rent price: $3500\n    City: Tokyo\n    Rewritten review: One of the best apartments I have ever stayed at! Great decorations in the lobby and good design. It didn\u2019t feel like a typical apartment, with bright and cozy rooms. It was very convenient and a short walk to the city center around 15-20 minutes. Rent was $3500 per month, which was reasonable compared to many places in Tokyo and well worth every penny! Would definitely recommend renting in this building if you can.\n    --- End of Example 1 ---\n\n    --- Example 2 ---\n    Original Airbnb review: Beautiful home, excellent location in de Pijp with cafes and grocery store nearby, walkable to the museums and parks. A block from major tram lines. The home has tons of natural light and plants (including herbs you can use!) that warm up the place even on a dreary day in the Netherlands. Kitchen is well stocked, bed is comfortable. Jerome's a very responsive host. Highly recommend to all -- would definitely stay here my next trip!\n\n    Rent price: 'NA'\n    City: Renton\n    Rewritten review: Beautiful apartment. Excellent location in de Pijp with cafes and grocery store nearby, walkable to the museums and parks. A block from major tram lines. The apartment has tons of natural light which makes it great for growing plants! My plants really warmed up the place even on a dreary day in Washington. Kitchen is large and the bedroom is spacious. Great building maintence guy! Highly recommend the building to all -- will definitely renew my lease!\n    --- End of Example 2 ---\n\n    --- Example 3 ---\n    Original Airbnb review: Daan and Claire are great, we were very kindly welcomed. They provided all the necessary and useful information for our stay. There was even a delicious pie and strawberries as a welcome. It was an extremely nice touch. Their apartment is spacious, bright and in a quiet area with a few bars and restaurants. This is a lovely place a few steps from bus and tram that brings us directly to the city center. We highly recommend it !\n\n    Rent price: $1200\n    City: Tempe\n    Rewritten review: Great landlord! There was even a delicious pie and strawberries as a welcome when we first moved in. It was an extremely nice touch. Their apartment is spacious, bright and in a quiet area with a few bars and restaurants. This is a lovely place a few steps from bus and tram that brings us directly to the city center. So lucky I got to live here for just 1200 bucks!\n    --- End of Example 3 ---\n\n    Original Airbnb review: {original_review}\n    Rent price: {rent_price}\n    City: {city}\n\n    Rewritten review:\n  \"\"\"\n\n  # Call the OpenAI API. You could use other language models as desired.\n  completion = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n      {\"role\": \"user\", \"content\": prompt}\n    ]\n  )\n\n  # Return the rewritten review.\n  rewritten_review = completion.choices[0].message.content\n  return rewritten_review\n\ndef create_rent_prices(cheapest_rent, most_expensive_rent, rent_price_step_size, n, rent_price_na_ratio):\n  # Generate a list of rent prices using a normal distribution within a specified range.\n  #\n  # Parameters:\n  # - cheapest_rent (int): The minimum rent price.\n  # - most_expensive_rent (int): The maximum rent price.\n  # - rent_price_step_size (int): The step size for rent prices.\n  # - n (int): The number of rent prices to generate.\n  # - The ratio of reviews that should have an 'NA' rent price (rent price won't be mentioned in the rewritten review.)\n  #\n  # Returns:\n  # - list of int: A list of rent prices.\n\n    # Calculate summary statistics used to generate the rent.\n    rent_prices = []\n    mean_rent = np.mean([most_expensive_rent, cheapest_rent])\n    sd_of_rent_prices = (most_expensive_rent - mean_rent) / 3\n\n    while len(rent_prices) &lt; n:\n        unrounded_rent = np.random.normal(mean_rent, sd_of_rent_prices)\n        rent_price = rent_price_step_size * round(unrounded_rent / rent_price_step_size) # A trick to make sure the rent matches the step size\n\n        if (rent_price &lt;= most_expensive_rent) and (rent_price &gt;= cheapest_rent):\n            rent_prices.append(rent_price)\n\n    # Replace random rent prices with NA since not every review would mention a rent price\n    indicies = random.sample(range(n), k=int(n * rent_price_na_ratio))\n    rent_prices = [\"NA\" if i in indicies else rent_prices[i] for i in range(len(rent_prices))]\n\n    # Return the final result\n    return rent_prices\n\ndef get_random_cities(city_list_file_name, n):\n  # Open the random city text file and read in the data.\n  with open(city_list_file_name, 'r') as file:\n      cities = file.read().splitlines()\n\n  # Return a random set of cities of length n.\n  return np.random.choice(cities, size=n)\n\n# Set up a small test\nn = 5\ncheapest_rent = 1000\nmost_expensive_rent = 5000\nrent_price_step_size = 5\nrent_price_na_ratio = 0.2 # What proportion of reviews shouldn't mention rent?\ncity_list_file_name = 'well_known_cities.txt'\n\noriginal_reviews = df['comments'].sample(n, replace=True).tolist() # Get a sample of AirBnb reviews\nrent_prices = create_rent_prices(cheapest_rent, most_expensive_rent, rent_price_step_size, n, rent_price_na_ratio) # Create the list of random rent prices\ncities = get_random_cities(city_list_file_name, n) # Create the list of random cities\n\ndata = [] # List to store JSON results\n\n# Loop through the lists\nfor original_review, rent_price, city in zip(original_reviews, rent_prices, cities):\n  rewritten_review = create_apartment_review(original_review, rent_price, city)\n\n  json_object = {\n      \"review\": rewritten_review,\n      \"query\": \"What was the rent price paid by this reviewer?\",\n      \"json_schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n              \"rent_price\": {\n                  \"type\": \"integer\",\n                  \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n              }\n          },\n          \"required\": [\"rent_price\"]\n      },\n      \"expected_result\": {\n          \"rent_price\": rent_price\n      }\n  }\n\n  data.append(json_object)\n\n# Output the data as JSON\noutput_json = json.dumps(data, indent=4)\n\n# Saving to a file (optional)\n# with open('output.json', 'w') as f:\n#     f.write(output_json)\n\nprint(output_json) # Display the output JSON\n</code></pre> <pre><code>[\n    {\n        \"review\": \"The apartment was clean and in a central location right off the Red Light District. However, the street can be noisy, so ear plugs are recommended for light sleepers. The apartment only had a sink in the bedrooms, with no toilet or shower. What you see is what you get. Overall, living here was a positive experience, thanks to its convenience and the overall vibe of the neighborhood.\",\n        \"query\": \"What was the rent price paid by this reviewer?\",\n        \"json_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rent_price\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n                }\n            },\n            \"required\": [\n                \"rent_price\"\n            ]\n        },\n        \"expected_result\": {\n            \"rent_price\": \"NA\"\n        }\n    },\n    {\n        \"review\": \"Living on this houseboat was an exceptional experience. It was very clean and comfortable, with all the amenities you could need. The houseboat is situated in a beautiful and serene location, away from the hustle and bustle of the city, yet with easy access to the tram and a grocery store nearby. The atmosphere was peaceful, making it an ideal place to call home.\\n\\nThe landlord was incredibly considerate, even leaving an adorable souvenir for us when we first moved in. There were plenty of resources available in the houseboat to help navigate Salem and the surroundings. The price at $2690 per month felt justified for such a unique living experience in this charming city. Even when it rained, the location remained lovely, and we thoroughly enjoyed our time there. Highly recommend considering this place for those looking for a distinctive and tranquil residence.\",\n        \"query\": \"What was the rent price paid by this reviewer?\",\n        \"json_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rent_price\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n                }\n            },\n            \"required\": [\n                \"rent_price\"\n            ]\n        },\n        \"expected_result\": {\n            \"rent_price\": 2690\n        }\n    },\n    {\n        \"review\": \"Living in this apartment has been a wonderful experience. The location is conveniently close to central station, with trams nearby for easy exploration. The apartment itself is quaint and perfect for our needs. The neighborhood is quiet, safe, and close to many events and amenities. The rent, at $2055 per month, is quite reasonable considering everything Denver has to offer. Highly recommend this place to anyone looking for a long-term rental in the area!\",\n        \"query\": \"What was the rent price paid by this reviewer?\",\n        \"json_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rent_price\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n                }\n            },\n            \"required\": [\n                \"rent_price\"\n            ]\n        },\n        \"expected_result\": {\n            \"rent_price\": 2055\n        }\n    },\n    {\n        \"review\": \"Lovely place, just make sure to check the weather before planning outdoor activities. I\\u2019d still probably choose this apartment over anything else in New Orleans. The location is a bit out from the city center, so biking from downtown back home takes about 35 minutes, and it's best to avoid doing that after a few drinks. The scenery here is beautiful, and the apartment itself has a unique charm, with cheerful rooms and plenty of natural light.\\n\\nBreakfast is always a delight, especially if you're a fan of sheep cheese. Living in this area has its perks, especially in good weather which makes the surroundings even more enjoyable. The rent is $4150 per month, a bit on the high side, but given the ambiance and the picturesque environment, it's well worth it.\",\n        \"query\": \"What was the rent price paid by this reviewer?\",\n        \"json_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rent_price\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n                }\n            },\n            \"required\": [\n                \"rent_price\"\n            ]\n        },\n        \"expected_result\": {\n            \"rent_price\": 4150\n        }\n    },\n    {\n        \"review\": \"I had a wonderful experience living in this modest but very comfortable apartment. The space was perfect for stretching out and relaxing after a busy day. The kitchen was fully equipped, making it easy to prepare home-cooked meals, which I loved. The location is incredible \\u2014 with a tram stop just around the corner that takes you directly to the city centre and other tram lines. Despite its proximity to all the excitement, the neighborhood itself is quiet and relaxed, and it's right next to a beautiful park. The rent was $3080 per month, and it was well worth it for the accessibility and tranquility it offered. I would highly recommend this apartment for anyone looking to enjoy all the fun of Miramar while having a peaceful retreat to come home to.\",\n        \"query\": \"What was the rent price paid by this reviewer?\",\n        \"json_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rent_price\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n                }\n            },\n            \"required\": [\n                \"rent_price\"\n            ]\n        },\n        \"expected_result\": {\n            \"rent_price\": 3080\n        }\n    }\n]\n</code></pre>"},{"location":"synthetic_data/basic_tutorial_1/airbnb_rewriting/#cost-considerations","title":"Cost considerations","text":"<p>In a production setting, cost is a crucial factor. There are several ways to reduce costs:</p> <ol> <li> <p>Using a Cheaper Model: Opting for a less expensive yet equally capable model can significantly cut costs. For example, Claude Sonnet 3.5 costs $3 per million input tokens compared to GPT-4o's $5 per million input tokens.</p> </li> <li> <p>Using Open Source Models: Another approach is to use open-source models like Llama3. Although testing all open-source models is beyond the scope of this activity, I tested Llama3 (8B). While the results were good when the model returned a rewritten review, it was unreliable. Sometimes it returned the original review instead of the rewritten one, failed to incorporate the correct price, or added unnecessary leading or trailing explanatory text.</p> </li> <li> <p>Batch Processing: Requesting reviews in batches is another cost-saving measure. Although we still incur the input and output costs for each review, we save on the input cost of the long prompt instruction (approximately 650 words). The cost savings from this approach are calculated below:</p> </li> </ol> <pre><code>prompt_instruction_length = 650\noriginal_review_length = 100\nrewritten_review_lenth = 100\ntokens_per_word_ratio = .75\ninput_cost_per_million_tokens = 5\noutput_cost_per_million_tokens = 15\n\n# Cost to fetch a single rewritten review at a time\nserial_input_cost = input_cost_per_million_tokens * ((prompt_instruction_length + original_review_length) * tokens_per_word_ratio) / 1000000\nserial_output_cost = output_cost_per_million_tokens * (rewritten_review_lenth * tokens_per_word_ratio) / 1000000\n\nprint(f\"Cost per rewritten review: ${serial_input_cost + serial_output_cost}\") # Cost per rewritten review: $0.0039375\nprint(f\"Total cost to generate 100,000 synthetic reviews: ${100000 * (serial_input_cost + serial_output_cost)}\") # Total cost to generate 100,000 synthetic reviews: $393.75\n\n# Cost if we request a batch of rewritten reviews\nbatch_size = 10\nbatch_input_cost = input_cost_per_million_tokens * ((prompt_instruction_length + (original_review_length * batch_size)) * tokens_per_word_ratio) / 1000000\nbatch_output_cost = serial_output_cost * batch_size \n\nprint(f\"Cost per rewritten review ${(batch_input_cost + batch_output_cost) / batch_size}\") # Cost per rewritten review 0.00174375\nprint(f\"Total cost to generate 100,000 synthetic reviews: ${100000 * (batch_input_cost + batch_output_cost) / batch_size}\") # Total cost to generate 100,000 synthetic reviews: $174.38\n\n# What we save by batching\nbatch_savings = input_cost_per_million_tokens * (tokens_per_word_ratio * prompt_instruction_length * (batch_size - 1)) / 1000000 # We only pass in the 650 word instruction prompt once and thus for batch_size - 1 we save that cost\nprint(f\"Savings per rewritten review: ${batch_savings / batch_size }\") # Savings per rewritten review: $0.00219375\nprint(f\"Savings per 100,000 rewritten reviews: ${0.00219375 * 100000}\") # $219.38\n\n# Does our math work out?\nprint(f\"Do these two numbers match? {(0.00174375 + 0.00219375) == 0.0039375}\") # True\n</code></pre> <pre><code>Cost per rewritten review: $0.0039375\nTotal cost to generate 100,000 synthetic reviews: $393.75\nCost per rewritten review $0.00174375\nTotal cost to generate 100,000 synthetic reviews: $174.37500000000003\nSavings per rewritten review: $0.00219375\nSavings per 100,000 rewritten reviews: $219.375\nDo these two numbers match? True\n</code></pre>"},{"location":"synthetic_data/basic_tutorial_1/airbnb_rewriting/#implementing-batching","title":"Implementing batching","text":"<p>The code below modifies the base synthetic data implementation to request a batch of 10 reviews from GPT-4o. The batched response from GPT-4o appears to maintain the quality of the serial approach. Given GPT-4o's output token limit of 4096, the batch size could potentially be increased to 20 reviews at a time, introducing further savings.</p> <pre><code>from openai import OpenAI\nimport numpy as np\nimport os\nimport json\nimport random\n\n# Set API key\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\")\n)\n\ndef create_apartment_reviews_batch(original_reviews, rent_prices, cities):\n    # Prepare the input data for the API call\n    input_data = [\n        {\"original_review\": review, \"rent_price\": price, \"city\": city}\n        for review, price, city in zip(original_reviews, rent_prices, cities)\n    ]\n\n    # Construct the prompt for the language model\n    prompt = \"\"\"\n    Your task is to transform 10 Airbnb reviews into long-term apartment reviews. For each review:\n    1. Imagine the tenant lived in the apartment for 1-5 years.\n    2. Ensure the rewritten review reflects long-term living, avoiding short-term stay language.\n    3. Replace mentions of the host with details about the apartment, building, area, or landlord.\n    4. If Amsterdam is mentioned, replace it with the specified city.\n    5. Incorporate the rent price naturally within the review. If the rent price is 'NA', don't mention it.\n\n    Return a JSON object with the following structure:\n    {{\n        \"review_1\": {{ \"rewritten_review\": \"...\", \"rent_price\": ... }},\n        \"review_2\": {{ \"rewritten_review\": \"...\", \"rent_price\": ... }},\n        ...\n        \"review_10\": {{ \"rewritten_review\": \"...\", \"rent_price\": ... }}\n    }}\n\n    Each \"review_n\" object should contain:\n    - \"rewritten_review\": The transformed long-term apartment review as a string.\n    - \"rent_price\": The provided rent price (as an integer or \"NA\" string).\n\n    Here are the 10 reviews to transform:\n    {input_data}\n    \"\"\"\n\n    # Call the OpenAI API\n    completion = client.chat.completions.create(\n        model=\"gpt-4-turbo\",\n        response_format={ \"type\": \"json_object\" },\n        messages=[{\"role\": \"user\", \"content\": prompt.format(input_data=json.dumps(input_data, indent=2))}]\n    )\n\n    # Parse the JSON response\n    return json.loads(completion.choices[0].message.content)\n\ndef create_rent_prices(cheapest_rent, most_expensive_rent, rent_price_step_size, n, rent_price_na_ratio):\n    rent_prices = []\n    mean_rent = np.mean([most_expensive_rent, cheapest_rent])\n    sd_of_rent_prices = (most_expensive_rent - mean_rent) / 3\n\n    while len(rent_prices) &lt; n:\n        unrounded_rent = np.random.normal(mean_rent, sd_of_rent_prices)\n        rent_price = rent_price_step_size * round(unrounded_rent / rent_price_step_size)\n\n        if (rent_price &lt;= most_expensive_rent) and (rent_price &gt;= cheapest_rent):\n            rent_prices.append(int(rent_price))  # Convert to integer\n\n    indices = random.sample(range(n), k=int(n * rent_price_na_ratio))\n    rent_prices = [\"NA\" if i in indices else rent_prices[i] for i in range(len(rent_prices))]\n    return rent_prices\n\ndef get_random_cities(city_list_file_name, n):\n    with open(city_list_file_name, 'r') as file:\n        cities = file.read().splitlines()\n    return np.random.choice(cities, size=n)\n\n# Set up the batch process\nn = 10  # Number of reviews to generate in one batch\ncheapest_rent = 1000\nmost_expensive_rent = 5000\nrent_price_step_size = 5\nrent_price_na_ratio = 0.2\ncity_list_file_name = 'well_known_cities.txt'\n\noriginal_reviews = df['comments'].sample(n, replace=True).tolist()\nrent_prices = create_rent_prices(cheapest_rent, most_expensive_rent, rent_price_step_size, n, rent_price_na_ratio)\ncities = get_random_cities(city_list_file_name, n)\n\n# Generate the batch of rewritten reviews\napi_response = create_apartment_reviews_batch(original_reviews, rent_prices, cities)\n\n# Print the structure of the API response for debugging\nprint(\"API Response Structure:\")\nprint(json.dumps(api_response, indent=2))\n\ndata = []  # List to store final JSON results\n\n# Create JSON objects for each rewritten review\nfor review_key, review_data in api_response.items():\n    json_object = {\n        \"review\": review_data['rewritten_review'],\n        \"query\": \"What was the rent price paid by this reviewer?\",\n        \"json_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rent_price\": {\n                    \"type\": [\"integer\", \"string\"],\n                    \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n                }\n            },\n            \"required\": [\"rent_price\"]\n        },\n        \"expected_result\": {\n            \"rent_price\": review_data['rent_price']\n        }\n    }\n    data.append(json_object)\n\n# Output the data as JSON\noutput_json = json.dumps(data, indent=4)\n\n# Saving to a file (optional)\n# with open('output.json', 'w') as f:\n#     f.write(output_json)\n\nprint(output_json)  # To display the output JSON\n</code></pre> <pre><code>API Response Structure:\n{\n  \"review_1\": {\n    \"rewritten_review\": \"Living in this Augusta apartment for the past few years has been a delight. The apartment itself is spacious, exceptionally clean, and well-decorated, creating a warm and welcoming environment. What's more, it is conveniently located just a 15-minute walk from the city center, making daily commutes and leisure activities easily accessible. The building management is very understanding and flexible about tenancy issues, which is a breath of fresh air. For a monthly rent of $2825, the value provided is outstanding. I wholeheartedly recommend this apartment for anyone looking to settle down in Augusta.\",\n    \"rent_price\": 2825\n  },\n  \"review_2\": {\n    \"rewritten_review\": \"Having lived in this Las Cruces apartment for over four years, its value really stands out, especially at $4420 per month. While slightly outside the city center, the nearby tram stop offers easy access to downtown. The apartment, though compact, is perfectly suited for busy individuals who spend most of their time out and about. The neighborhood is quiet and the building includes great amenities like provided breakfasts, adding a touch of convenience to everyday living. I can certainly recommend this place for anyone looking for long-term accommodation in Las Cruces with all necessities on hand.\",\n    \"rent_price\": 4420\n  },\n  \"review_3\": {\n    \"rewritten_review\": \"Our long-term stay in Arlington at this apartment has been highly pleasant. The room mimics just what was shown in originating ads - comfortable and welcoming. The neighborhood is friendly, giving off a lovely suburban vibe. Moreover, the building is well-managed, and the landlord personally made sure our move-in and checkout processes over the years were smooth, even driving us to the airport when we finally moved out. With a monthly rent of $2495, the value is excellent. I highly recommend this place to anyone moving to Arlington.\",\n    \"rent_price\": 2495\n  },\n  \"review_4\": {\n    \"rewritten_review\": \"Living on a houseboat in Coral Springs has been an unparalleled experience. The locality is serene, nestled close to various attractions without being swamped by tourists. The boat itself is spotless and makes you feel right at home from the get-go. Waking up to the stunning waterfront views each morning has been a real pleasure. The neighbors are very sociable and always ready with a recommendation or two about local spots. If you're seeking an extraordinary home, this is where your search should end!\",\n    \"rent_price\": \"NA\"\n  },\n  \"review_5\": {\n    \"rewritten_review\": \"Our family found the perfect long-term home in North Charleston in this apartment. Despite our initial concern about space for our three teenagers, the apartment fit our needs beautifully. Just a 20-minute tram ride to the city and proximity to markets and restaurants, have added to our convenience greatly. For a rent of $2220 per month, this apartment has exceeded our expectations and I'd recommend it especially for families considering a move here.\",\n    \"rent_price\": 2220\n  },\n  \"review_6\": {\n    \"rewritten_review\": \"Residing in this welcoming Mission Viejo neighborhood for a couple of years now, I've found it easy to travel to surrounding cities for day trips. The local bar offers a fine selection of craft beers, and there is convenient parking at the building. The area is quaint, safe, and surrounded by friendly faces. It\\u2019s a fantastic location if you're looking for a hub without the city bustle.\",\n    \"rent_price\": \"NA\"\n  },\n  \"review_7\": {\n    \"rewritten_review\": \"Renting this spacious property in Pearland has been perfect for us a large group. The common rooms are expansive and the kitchen keeps us from feeling crowded, ideal for gatherings and daily life alike. Despite the smaller sleeping quarters, they're well-maintained and clean. Standing about 30 minutes from central transit hubs but easily reachable by local buses, the location balances calm residential vibes with good connectivity. At a monthly rent of $4090, it's a great spot for anyone looking for ample space in Pearland.\",\n    \"rent_price\": 4090\n  },\n  \"review_8\": {\n    \"rewritten_review\": \"Living in Kent for the past few years, this apartment has been a gem. Its fabulous location near the center and scenic parks, coupled with exquisite interior decor that makes every day a bit more special, has made long-term living here utterly enjoyable. Renting here for $3460 a month has been well worth it due to the combination of an artistic living space and essential flexibility offered by the management regarding lease terms. I highly recommend it for anyone looking to move to Kent.\",\n    \"rent_price\": 3460\n  },\n  \"review_9\": {\n    \"rewritten_review\": \"Being a resident in this Wilmington apartment has added a beautiful chapter to my life. The room, aesthetically pleasing and immaculately clean, aligned perfectly with how it was presented. The city's diversity and lively culture, coupled with the friendliness and kindness of the building manager, have made this stay nothing short of awesome. Given these benefits, the rent of $3020 per month feels justified and well-spent. I highly recommend experiencing living here.\",\n    \"rent_price\": 3020\n  },\n  \"review_10\": {\n    \"rewritten_review\": \"Our long-term stay in this quaint older apartment in Mesquite has been thoroughly delightful. Easy access to the city center combined with features suitable for taller individuals have made it a comfortable abode. The rooftop garden, despite the cold weather, remains a cherished spot. While the apartment has a more personal, lived-in feel rather than a 'professional' setup, it adds to the charm. The local neighborhood is helpful, and there's a convenient street map available from the landlord. Beautiful, charming, and priced at $3095 per month, it's been perfect for us.\",\n    \"rent_price\": 3095\n  }\n}\n[\n    {\n        \"review\": \"Living in this Augusta apartment for the past few years has been a delight. The apartment itself is spacious, exceptionally clean, and well-decorated, creating a warm and welcoming environment. What's more, it is conveniently located just a 15-minute walk from the city center, making daily commutes and leisure activities easily accessible. The building management is very understanding and flexible about tenancy issues, which is a breath of fresh air. For a monthly rent of $2825, the value provided is outstanding. I wholeheartedly recommend this apartment for anyone looking to settle down in Augusta.\",\n        \"query\": \"What was the rent price paid by this reviewer?\",\n        \"json_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rent_price\": {\n                    \"type\": [\n                        \"integer\",\n                        \"string\"\n                    ],\n                    \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n                }\n            },\n            \"required\": [\n                \"rent_price\"\n            ]\n        },\n        \"expected_result\": {\n            \"rent_price\": 2825\n        }\n    },\n    {\n        \"review\": \"Having lived in this Las Cruces apartment for over four years, its value really stands out, especially at $4420 per month. While slightly outside the city center, the nearby tram stop offers easy access to downtown. The apartment, though compact, is perfectly suited for busy individuals who spend most of their time out and about. The neighborhood is quiet and the building includes great amenities like provided breakfasts, adding a touch of convenience to everyday living. I can certainly recommend this place for anyone looking for long-term accommodation in Las Cruces with all necessities on hand.\",\n        \"query\": \"What was the rent price paid by this reviewer?\",\n        \"json_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rent_price\": {\n                    \"type\": [\n                        \"integer\",\n                        \"string\"\n                    ],\n                    \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n                }\n            },\n            \"required\": [\n                \"rent_price\"\n            ]\n        },\n        \"expected_result\": {\n            \"rent_price\": 4420\n        }\n    },\n    {\n        \"review\": \"Our long-term stay in Arlington at this apartment has been highly pleasant. The room mimics just what was shown in originating ads - comfortable and welcoming. The neighborhood is friendly, giving off a lovely suburban vibe. Moreover, the building is well-managed, and the landlord personally made sure our move-in and checkout processes over the years were smooth, even driving us to the airport when we finally moved out. With a monthly rent of $2495, the value is excellent. I highly recommend this place to anyone moving to Arlington.\",\n        \"query\": \"What was the rent price paid by this reviewer?\",\n        \"json_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rent_price\": {\n                    \"type\": [\n                        \"integer\",\n                        \"string\"\n                    ],\n                    \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n                }\n            },\n            \"required\": [\n                \"rent_price\"\n            ]\n        },\n        \"expected_result\": {\n            \"rent_price\": 2495\n        }\n    },\n    {\n        \"review\": \"Living on a houseboat in Coral Springs has been an unparalleled experience. The locality is serene, nestled close to various attractions without being swamped by tourists. The boat itself is spotless and makes you feel right at home from the get-go. Waking up to the stunning waterfront views each morning has been a real pleasure. The neighbors are very sociable and always ready with a recommendation or two about local spots. If you're seeking an extraordinary home, this is where your search should end!\",\n        \"query\": \"What was the rent price paid by this reviewer?\",\n        \"json_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rent_price\": {\n                    \"type\": [\n                        \"integer\",\n                        \"string\"\n                    ],\n                    \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n                }\n            },\n            \"required\": [\n                \"rent_price\"\n            ]\n        },\n        \"expected_result\": {\n            \"rent_price\": \"NA\"\n        }\n    },\n    {\n        \"review\": \"Our family found the perfect long-term home in North Charleston in this apartment. Despite our initial concern about space for our three teenagers, the apartment fit our needs beautifully. Just a 20-minute tram ride to the city and proximity to markets and restaurants, have added to our convenience greatly. For a rent of $2220 per month, this apartment has exceeded our expectations and I'd recommend it especially for families considering a move here.\",\n        \"query\": \"What was the rent price paid by this reviewer?\",\n        \"json_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rent_price\": {\n                    \"type\": [\n                        \"integer\",\n                        \"string\"\n                    ],\n                    \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n                }\n            },\n            \"required\": [\n                \"rent_price\"\n            ]\n        },\n        \"expected_result\": {\n            \"rent_price\": 2220\n        }\n    },\n    {\n        \"review\": \"Residing in this welcoming Mission Viejo neighborhood for a couple of years now, I've found it easy to travel to surrounding cities for day trips. The local bar offers a fine selection of craft beers, and there is convenient parking at the building. The area is quaint, safe, and surrounded by friendly faces. It\\u2019s a fantastic location if you're looking for a hub without the city bustle.\",\n        \"query\": \"What was the rent price paid by this reviewer?\",\n        \"json_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rent_price\": {\n                    \"type\": [\n                        \"integer\",\n                        \"string\"\n                    ],\n                    \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n                }\n            },\n            \"required\": [\n                \"rent_price\"\n            ]\n        },\n        \"expected_result\": {\n            \"rent_price\": \"NA\"\n        }\n    },\n    {\n        \"review\": \"Renting this spacious property in Pearland has been perfect for us a large group. The common rooms are expansive and the kitchen keeps us from feeling crowded, ideal for gatherings and daily life alike. Despite the smaller sleeping quarters, they're well-maintained and clean. Standing about 30 minutes from central transit hubs but easily reachable by local buses, the location balances calm residential vibes with good connectivity. At a monthly rent of $4090, it's a great spot for anyone looking for ample space in Pearland.\",\n        \"query\": \"What was the rent price paid by this reviewer?\",\n        \"json_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rent_price\": {\n                    \"type\": [\n                        \"integer\",\n                        \"string\"\n                    ],\n                    \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n                }\n            },\n            \"required\": [\n                \"rent_price\"\n            ]\n        },\n        \"expected_result\": {\n            \"rent_price\": 4090\n        }\n    },\n    {\n        \"review\": \"Living in Kent for the past few years, this apartment has been a gem. Its fabulous location near the center and scenic parks, coupled with exquisite interior decor that makes every day a bit more special, has made long-term living here utterly enjoyable. Renting here for $3460 a month has been well worth it due to the combination of an artistic living space and essential flexibility offered by the management regarding lease terms. I highly recommend it for anyone looking to move to Kent.\",\n        \"query\": \"What was the rent price paid by this reviewer?\",\n        \"json_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rent_price\": {\n                    \"type\": [\n                        \"integer\",\n                        \"string\"\n                    ],\n                    \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n                }\n            },\n            \"required\": [\n                \"rent_price\"\n            ]\n        },\n        \"expected_result\": {\n            \"rent_price\": 3460\n        }\n    },\n    {\n        \"review\": \"Being a resident in this Wilmington apartment has added a beautiful chapter to my life. The room, aesthetically pleasing and immaculately clean, aligned perfectly with how it was presented. The city's diversity and lively culture, coupled with the friendliness and kindness of the building manager, have made this stay nothing short of awesome. Given these benefits, the rent of $3020 per month feels justified and well-spent. I highly recommend experiencing living here.\",\n        \"query\": \"What was the rent price paid by this reviewer?\",\n        \"json_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rent_price\": {\n                    \"type\": [\n                        \"integer\",\n                        \"string\"\n                    ],\n                    \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n                }\n            },\n            \"required\": [\n                \"rent_price\"\n            ]\n        },\n        \"expected_result\": {\n            \"rent_price\": 3020\n        }\n    },\n    {\n        \"review\": \"Our long-term stay in this quaint older apartment in Mesquite has been thoroughly delightful. Easy access to the city center combined with features suitable for taller individuals have made it a comfortable abode. The rooftop garden, despite the cold weather, remains a cherished spot. While the apartment has a more personal, lived-in feel rather than a 'professional' setup, it adds to the charm. The local neighborhood is helpful, and there's a convenient street map available from the landlord. Beautiful, charming, and priced at $3095 per month, it's been perfect for us.\",\n        \"query\": \"What was the rent price paid by this reviewer?\",\n        \"json_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"rent_price\": {\n                    \"type\": [\n                        \"integer\",\n                        \"string\"\n                    ],\n                    \"description\": \"Price paid by users or 'NA' if not mentioned.\"\n                }\n            },\n            \"required\": [\n                \"rent_price\"\n            ]\n        },\n        \"expected_result\": {\n            \"rent_price\": 3095\n        }\n    }\n]\n</code></pre>"}]}