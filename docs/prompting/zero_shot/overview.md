# Zero-shot Prompting
Zero-shot prompting is a family of prompting techniques that involves passing instructions or asking questions to a large language model in a single step. This approach contrasts with few-shot prompting, where the model is provided with examples or repeatedly interacted with to obtain a result.

Different prompting strategies can significantly impact the performance and output of language models. This guide provides a comprehensive overview of various zero-shot prompting techniques, including role, style, emotion, and more, to help you effectively utilize these models for diverse tasks. The prompting techniques below are based on Section 2.2.2 of [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608).

Each section below details a specific prompting strategy, explaining its most effective use cases, best practices, and citing relevant academic research. Additionally, it includes prompt templates, examples, and API code samples.

- [Plain Vanilla Zero-Shot Prompting](plain_vanilla.md)
- [Role Prompting](role_prompting.md)
- [Solo Performance Prompting (SPP)](solo_performance_prompting.md)
- [Style Prompting](style_prompting.md)
- [Emotion Prompting](emotion_prompting.md)
- [System 2 Attention (S2A)](system_2_attention.md)
- [SimToM](simtom.md)
- [Rephrase and Respond (RaR)](rephrase_and_respond.md)
- [Re-reading (RE2)](rereading.md)
- [Self-Ask](self_ask.md)
